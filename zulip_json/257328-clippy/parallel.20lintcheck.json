[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"264664\">@flip1995</span> in other news, I managed to parallelize lintcheck so that we can <code>cargo clippy</code> several crates at a time!<br>\nThe biggest problem was that we can't continue sharing our cargo-clippy-on-source target dir because cargo would lock and kill the parallelism <span aria-label=\"big smile\" class=\"emoji emoji-1f604\" role=\"img\" title=\"big smile\">:big_smile:</span> <br>\nI've resorted to using <code>${N_CPUS}</code> target dirs at the same time (one for each cpu-core ideally)  which is not optimal for caching but still better than nothing.</p>",
        "id": 226883905,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613683729
    },
    {
        "content": "<p>I see. Sacrificing memory for speed. The modern software engineering way. <span aria-label=\"smiley\" class=\"emoji emoji-1f603\" role=\"img\" title=\"smiley\">:smiley:</span> But that is indeed great news!</p>",
        "id": 226884408,
        "sender_full_name": "flip1995",
        "timestamp": 1613683970
    },
    {
        "content": "<p>Just finished running on the ~400 crates-io crates.toml that I added to your repo.<br>\nThe resulting log file is 35 megabyte lol</p>",
        "id": 226884489,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613683998
    },
    {
        "content": "<blockquote>\n<p>I see. Sacrificing memory for speed. </p>\n</blockquote>\n<p>also disk space...<br>\n13 gigs in the target dir...<br>\nWe might need to auto-clean that every then and now</p>",
        "id": 226884717,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613684112
    },
    {
        "content": "<p>Oh that is really big :O Yeah, I don't think we can use this for triaging new lints. But it is definitely great to find ICEs and prevent really bad bugs in Clippy.</p>",
        "id": 226885282,
        "sender_full_name": "flip1995",
        "timestamp": 1613684384
    },
    {
        "content": "<p>oh I have a bug where it creates one target dir per crate <span aria-label=\"rolling eyes\" class=\"emoji emoji-1f644\" role=\"img\" title=\"rolling eyes\">:rolling_eyes:</span>  ...</p>",
        "id": 226885535,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613684494
    },
    {
        "content": "<p>So it seems that for the current set of crates, using two target dirs and running two clippys at the same time is all in all slower than using a single target dir and only sequential clippy :(</p>",
        "id": 226895986,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613688818
    },
    {
        "content": "<p>I did not think that the shared target dir would make such a difference</p>",
        "id": 226896075,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613688846
    },
    {
        "content": "<p>I think the big difference is, that cargo already uses all CPUs to compile deps in parallel. And then with a shared target dir it just reuses the deps from the previous crate.</p>",
        "id": 226896362,
        "sender_full_name": "flip1995",
        "timestamp": 1613689017
    },
    {
        "content": "<p>yes</p>",
        "id": 226896600,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613689133
    },
    {
        "content": "<p>so it kind of depends on how many deps the average crate that we check has.<br>\nThe less deps it has, the more of an impact the parallelization will have</p>",
        "id": 226896655,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613689179
    },
    {
        "content": "<p>But I also only have a 2c/4t system, I wonder how it would look on let's say a quad or octacore system</p>",
        "id": 226896790,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613689224
    },
    {
        "content": "<p>I could try that tomorrow. I have access to a 12t machine (and some 40c+ servers at work, I may be able to abuse after working hours <span aria-label=\"smiley\" class=\"emoji emoji-1f603\" role=\"img\" title=\"smiley\">:smiley:</span>).</p>",
        "id": 226897025,
        "sender_full_name": "flip1995",
        "timestamp": 1613689344
    },
    {
        "content": "<p>:o</p>",
        "id": 226897285,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613689477
    },
    {
        "content": "<p>but then again, the more crates we check in total, the more likely we are going to have a random \"cache hit\" ....</p>",
        "id": 226897729,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613689754
    },
    {
        "content": "<p>I can time it with different number of crates, if you send me your patch for parallelization.</p>",
        "id": 226898622,
        "sender_full_name": "flip1995",
        "timestamp": 1613690268
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust-clippy/pull/6764\">https://github.com/rust-lang/rust-clippy/pull/6764</a></p>",
        "id": 226898816,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613690401
    },
    {
        "content": "<p>ran on the 400-crates toml now:<br>\nmaster: <code>1458,48s user 109,25s system 76% cpu 33:58,81 total</code><br>\nrayon par_iter: <code>1873,03s user 137,84s system 144% cpu 23:13,41 total</code></p>",
        "id": 226904412,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613694230
    },
    {
        "content": "<p>personally as a clippy user, I would really hate to rebuild the shared cache over and over again</p>",
        "id": 226912147,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1613700910
    },
    {
        "content": "<p>In particular I would run out of memory really quickly and start thrashing</p>",
        "id": 226912171,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1613700932
    },
    {
        "content": "<p>I encourage you to use <a href=\"http://docs.rs\">docs.rs</a> as a test case, I think you'll find several caches takes <em>much</em> more time than running clippy in sequence regardless of the number of cores</p>",
        "id": 226912264,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1613701026
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"232545\">@Joshua Nelson</span> this is only a dev tool <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span></p>",
        "id": 226923540,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613714238
    },
    {
        "content": "<p>Ah ok, sorry for the noise</p>",
        "id": 226923548,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1613714258
    },
    {
        "content": "<p>it does not change how  <code>cargo clippy</code> works :)</p>",
        "id": 226923621,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613714339
    },
    {
        "content": "<p>See my timing results in the PR. TL;DR: parallelizing  this makes sense if you have access to a machine with <strong>many</strong> cores, so I would just add a flag that makes this possible.</p>",
        "id": 226994005,
        "sender_full_name": "flip1995",
        "timestamp": 1613754055
    },
    {
        "content": "<p>added the flag now (pr also includes a few other minor changes)</p>",
        "id": 227039973,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1613773840
    }
]