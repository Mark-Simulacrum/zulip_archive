[
    {
        "content": "<p>The clippy tests mostly consist of running clippy on a <code>.rs</code> file and checking the output, this is done via compiletest, right?<br>\nI timed running <code>clippy-driver</code> on a  default <code>cargo new</code> <code>main.rs</code> and it took around  0.1 seconds.<br>\nMultiplied by our 464 test files this already makes 46 seconds of only starting up clippy..?<br>\nThis seems like roughly 1/3 of the total time the tests take on my system <code> cargo --color=always test  101,21s user 32,64s system 321% cpu 41,622 total</code><br>\nI wonder how much we could speed up our tests simply by having fewer files to test.</p>",
        "id": 211199018,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1600987684
    },
    {
        "content": "<p>mh, according to <code>-Ztime-passes</code>,  <code>clippy-driver foo.rs</code> also runs llvm <span aria-label=\"face palm\" class=\"emoji emoji-1f926\" role=\"img\" title=\"face palm\">:face_palm:</span></p>",
        "id": 211199963,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1600988289
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"217864\">@matthiaskrgr</span> you should add <code>--emit=metadata</code> to avoid codegen</p>",
        "id": 211200039,
        "sender_full_name": "Eric Huss",
        "timestamp": 1600988369
    },
    {
        "content": "<p>ah!<br>\nDown to ~0.06 seconds with --emit-metadata :)</p>",
        "id": 211200164,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1600988438
    },
    {
        "content": "<p>oh weird, why do we have some clippy-tests annotated with <code>// run-pass</code> ...<br>\nIs that intentional?</p>",
        "id": 211200472,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1600988653
    },
    {
        "content": "<p>It does not look like we ever actually check the output of the running tests</p>",
        "id": 211200889,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1600988949
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"217864\">matthiaskrgr</span> <a href=\"#narrow/stream/257328-clippy/topic/speeding.20up.20tests.20by.20using.20less.20files/near/211200472\">said</a>:</p>\n<blockquote>\n<p>oh weird, why do we have some clippy-tests annotated with <code>// run-pass</code> ...<br>\nIs that intentional?</p>\n</blockquote>\n<p>It seems they were added in <a href=\"https://github.com/rust-lang/rust-clippy/issues/3922\">clippy#3922</a></p>",
        "id": 211201266,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1600989269
    },
    {
        "content": "<p>Apparently the tests would not fail if there's an ICE and the annotation is not present</p>",
        "id": 211201525,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1600989456
    },
    {
        "content": "<p>mmh.<br>\nif I remove the //run-pass from the tests and add <code>--emit=metadata</code>, this seems to shave ~7 seconds off the testsuite, prob not worth it though</p>",
        "id": 211201993,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1600989837
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"279272\">Eduardo Broto</span> <a href=\"#narrow/stream/257328-clippy/topic/speeding.20up.20tests.20by.20using.20less.20files/near/211201525\">said</a>:</p>\n<blockquote>\n<p>Apparently the tests would not fail if there's an ICE and the annotation is not present</p>\n</blockquote>\n<p>this seems like something you could test ;)</p>",
        "id": 211202121,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1600989933
    },
    {
        "content": "<p>if it saves 7 seconds on each run that seems worth trying it out</p>",
        "id": 211202127,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1600989944
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232545\">Joshua Nelson</span> <a href=\"#narrow/stream/257328-clippy/topic/speeding.20up.20tests.20by.20using.20less.20files/near/211202121\">said</a>:</p>\n<blockquote>\n<p>this seems like something you could test <span aria-label=\"wink\" class=\"emoji emoji-1f609\" role=\"img\" title=\"wink\">:wink:</span></p>\n</blockquote>\n<p>Indeed <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span>  I'm thinking that we may need to add some annotations to other crash tests if it's really needed <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span></p>",
        "id": 211202290,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1600990083
    },
    {
        "content": "<blockquote>\n<p>Apparently the tests would not fail if there's an ICE and the annotation is not present</p>\n</blockquote>\n<p>Well it seems to also be missing from a bunch of tests.. ^^</p>",
        "id": 211202337,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1600990144
    },
    {
        "content": "<p>Hm. What if we  have a file that does not compile (so can't use <code>run-pass</code>) but still should fail if clippy ICEs on it?</p>",
        "id": 211202574,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1600990380
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"217864\">matthiaskrgr</span> <a href=\"#narrow/stream/257328-clippy/topic/speeding.20up.20tests.20by.20using.20less.20files/near/211202574\">said</a>:</p>\n<blockquote>\n<p>Hm. What if we  have a file that does not compile (so can't use <code>run-pass</code>) but still should fail if clippy ICEs on it?</p>\n</blockquote>\n<p>compiletest expects that tests fail to compile by default</p>",
        "id": 211202817,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1600990580
    },
    {
        "content": "<p>not sure if that's what clippy uses though</p>",
        "id": 211202827,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1600990585
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232545\">Joshua Nelson</span> <a href=\"#narrow/stream/257328-clippy/topic/speeding.20up.20tests.20by.20using.20less.20files/near/211202827\">said</a>:</p>\n<blockquote>\n<p>not sure if that's what clippy uses though</p>\n</blockquote>\n<p>It uses <a href=\"https://github.com/laumann/compiletest-rs\">https://github.com/laumann/compiletest-rs</a> which should be (mostly) equivalent</p>",
        "id": 211203877,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1600991459
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"217864\">matthiaskrgr</span> <a href=\"#narrow/stream/257328-clippy/topic/speeding.20up.20tests.20by.20using.20less.20files/near/211202574\">said</a>:</p>\n<blockquote>\n<p>Hm. What if we  have a file that does not compile (so can't use <code>run-pass</code>) but still should fail if clippy ICEs on it?</p>\n</blockquote>\n<p>We should definitely open an issue and investigate all of this, good catch</p>",
        "id": 211203972,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1600991529
    },
    {
        "content": "<p>(Yes right now I'm gonna sleep <span aria-label=\"big smile\" class=\"emoji emoji-1f604\" role=\"img\" title=\"big smile\">:big_smile:</span> , will take a look at that tomorrow)</p>",
        "id": 211204562,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1600992048
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"217864\">@matthiaskrgr</span> about the suggestion to reduce the number of test files, I'm not sure if it's worth it. That would make the tests more complex to read, and the stderr files way longer. This would also increase the probability of conflicts between PRs.</p>\n<p>Moreover, UI tests are not the ones that take most of the time. In my machine, UI tests take 17.4 seconds. The dogfood tests are the ones taking longer, 49 secs including clippy and subprojects. That's running clippy on 5 crates, which I consider a reasonable amount of time.</p>",
        "id": 211223282,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1601016140
    },
    {
        "content": "<p>I also think that the UI test time is fine for now. However, if there's low hanging fruit to improve, that's cool!</p>",
        "id": 211237098,
        "sender_full_name": "Phil H",
        "timestamp": 1601027118
    },
    {
        "content": "<blockquote>\n<p>oh weird, why do we have some clippy-tests annotated with // run-pass</p>\n</blockquote>\n<p>iirc, we use <code>run-pass</code>because there was no <code>compile-pass</code> in compiletest-rs at the time. There might be now <span aria-label=\"thinking\" class=\"emoji emoji-1f914\" role=\"img\" title=\"thinking\">:thinking:</span><br>\nI'm not sure if plain UI tests would be enough to catch ICEs?</p>",
        "id": 211237385,
        "sender_full_name": "Phil H",
        "timestamp": 1601027293
    },
    {
        "content": "<p>running all tests for the first time: <code>228,71s user 41,23s system 192% cpu 2:20,58 total</code><br>\nruning tests again afterwards (no changes, no need to recompile everything for dogfood test): <code>100,55s user 32,86s system 245% cpu 54,343 total</code><br>\n:/</p>",
        "id": 211247429,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1601034569
    },
    {
        "content": "<p>Regarding the <code>run-pass</code> annotations, I've tested it and an ICE makes the test fail regardless of the annotation.</p>\n<div class=\"codehilite\"><pre><span></span><code>diff of stderr:\n\n+thread &#39;rustc&#39; panicked at &#39;the disco&#39;, clippy_lints/src/repeat_once.rs:41:9\n+note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n+\n+error: internal compiler error: unexpected panic\n+\n+note: the compiler unexpectedly panicked. this is a bug.\n+\n+note: we would appreciate a bug report: https://github.com/rust-lang/rust-clippy/issues/new\n+\n+note: Clippy version: clippy 0.0.212 (e636b88aa 2020-09-23)\n+\n+\n\nThe actual stderr differed from the expected stderr.\nActual stderr saved to /home/ebroto/src/ebroto-clippy/target/debug/test_build_base/crashes/ice-5944.stderr\nTo update references, run this command from build directory:\ntests/ui/update-references.sh &#39;/home/ebroto/src/ebroto-clippy/target/debug/test_build_base&#39; &#39;crashes/ice-5944.rs&#39;\n\nerror: 1 errors occurred comparing output.\nstatus: exit code: 101\n[...]\ntest [ui] ui/crashes/ice-5944.rs ... FAILED\n\nfailures:\n\nfailures:\n    [ui] ui/crashes/ice-5944.rs\n\ntest result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 519 filtered out\n\ntest compile_test ... FAILED\n</code></pre></div>",
        "id": 211254155,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1601038804
    },
    {
        "content": "<p>I think it makes sense that the tests fail because the output will be different than the expected one (regardless if there's an actual error in the stderr file or if it's empty/not present).</p>\n<p>I'm gonna go ahead and open a PR with the annotations removed.</p>\n<p>UI tests seem to take the same amount of time after the change BTW</p>",
        "id": 211254412,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1601038973
    },
    {
        "content": "<p>Maybe you could set <code>emit=metadata</code> on by default for these tests</p>",
        "id": 211254482,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1601039029
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232545\">Joshua Nelson</span> <a href=\"#narrow/stream/257328-clippy/topic/speeding.20up.20tests.20by.20using.20less.20files/near/211254482\">said</a>:</p>\n<blockquote>\n<p>Maybe you could set <code>emit=metadata</code> on by default for these tests</p>\n</blockquote>\n<p>Will try <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 211254774,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1601039192
    },
    {
        "content": "<p>Wow that was an improvement, from ~17 secs down to ~12 secs in my machine, thanks!</p>",
        "id": 211255686,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1601039693
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust-clippy/issues/6084\">clippy#6084</a></p>",
        "id": 211257592,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1601040656
    },
    {
        "content": "<p>yeah, when I just added <code>emit=metadata</code>I got errors for the <code>// runs-pass</code> tests that's when I noticed them in the first place</p>",
        "id": 211262801,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1601043095
    },
    {
        "content": "<p>Perhaps we can use <code>./tests/ui/custom_ice_message.rs</code> to also check that ices in the tests are reported properly in the <code>tests/ui/crashes</code> dir?</p>",
        "id": 211263346,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1601043320
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"217864\">matthiaskrgr</span> <a href=\"#narrow/stream/257328-clippy/topic/speeding.20up.20tests.20by.20using.20less.20files/near/211263346\">said</a>:</p>\n<blockquote>\n<p>Perhaps we can use <code>./tests/ui/custom_ice_message.rs</code> to also check that ices in the tests are reported properly in the <code>tests/ui/crashes</code> dir?</p>\n</blockquote>\n<p>If I'm not mistaken none of the crash tests currently result in an ICE, we add them after avoiding the ICE, so I think the output files are either empty/non-present or  contain normal lint errors</p>",
        "id": 211265296,
        "sender_full_name": "Eduardo Broto",
        "timestamp": 1601044276
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"279272\">Eduardo Broto</span> <a href=\"#narrow/stream/257328-clippy/topic/speeding.20up.20tests.20by.20using.20less.20files/near/211265296\">said</a>:</p>\n<blockquote>\n<p>If I'm not mistaken none of the crash tests currently result in an ICE, we add them after avoiding the ICE, so I think the output files are either empty/non-present or  contain normal lint errors</p>\n</blockquote>\n<p>correct. all the tests in crashes are regression tests.</p>",
        "id": 211428942,
        "sender_full_name": "flip1995",
        "timestamp": 1601232512
    }
]