[
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/issues/80674\">#80674</a> another case of time going backwards. Possibly due to lying hypervisor + old kernel + not-quite-recent CPU (2012 vintage).<br>\nIs this a strong enough case to extend the mutex hammer to x86_64 linux which so far has been exempt from it?<br>\nOr is advising users to change their clock source or fixing the hypervisor sufficient? At least linux provides options here...</p>",
        "id": 235549710,
        "sender_full_name": "The 8472",
        "timestamp": 1619026045
    },
    {
        "content": "<p>All of the issues with the clock going backward are usually due to hardware bugs.</p>",
        "id": 235550467,
        "sender_full_name": "Amanieu",
        "timestamp": 1619026328
    },
    {
        "content": "<p>Or virtual hardware. But I'm starting to wonder whether the right answer is to panic on hardware bugs, or whether we can be more robust against them...</p>",
        "id": 235550575,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619026374
    },
    {
        "content": "<p>That said, I also wonder if we could fix the <em>kernel</em>'s implementation of MONOTONIC...</p>",
        "id": 235550659,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619026401
    },
    {
        "content": "<p>Rather than having to work around it in Rust.</p>",
        "id": 235550679,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619026409
    },
    {
        "content": "<p>It <em>really</em> should be the kernel's job to fix this. After all, they are the ones promising <em>monotonic</em> time.</p>",
        "id": 235550825,
        "sender_full_name": "Amanieu",
        "timestamp": 1619026457
    },
    {
        "content": "<p>rename the constant to MONOTONICISH</p>",
        "id": 235550919,
        "sender_full_name": "Mara",
        "timestamp": 1619026498
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"143274\">Amanieu</span> <a href=\"#narrow/stream/219381-t-libs/topic/more.20broken.20clocks/near/235550825\">said</a>:</p>\n<blockquote>\n<p>It <em>really</em> should be the kernel's job to fix this. After all, they are the ones promising <em>monotonic</em> time.</p>\n</blockquote>\n<p>Yeah, I agree.</p>",
        "id": 235550955,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619026511
    },
    {
        "content": "<p>The kernel _does_ fix this by default<br>\n<a href=\"https://github.com/torvalds/linux/blob/8bb495e3f02401ee6f76d1b1d77f3ac9f079e376/arch/x86/kernel/pvclock.c#L77-L100\">https://github.com/torvalds/linux/blob/8bb495e3f02401ee6f76d1b1d77f3ac9f079e376/arch/x86/kernel/pvclock.c#L77-L100</a></p>\n<p>Except for cases where the hardware/vm promises that it's really reliable... the issue is implementations that promise this and then break their promise</p>",
        "id": 235551045,
        "sender_full_name": "The 8472",
        "timestamp": 1619026545
    },
    {
        "content": "<p>And we're dealing with some old frankenkernel in this particular case, so even if a current kernel implemented more defenses it wouldn't help.</p>",
        "id": 235551373,
        "sender_full_name": "The 8472",
        "timestamp": 1619026677
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/219381-t-libs/topic/more.20broken.20clocks/near/235550659\">said</a>:</p>\n<blockquote>\n<p>That said, I also wonder if we could fix the <em>kernel</em>'s implementation of MONOTONIC...</p>\n</blockquote>\n<p>this seems like a loosing battle, since presumably there is more than one kernel to worry about</p>",
        "id": 235552291,
        "sender_full_name": "Jane Lusby",
        "timestamp": 1619027048
    },
    {
        "content": "<p>Well, we don't trust all kernels. E.g. linux is trusted conditional on it being x86(64). windows is never trusted. osx is always trusted...</p>",
        "id": 235552798,
        "sender_full_name": "The 8472",
        "timestamp": 1619027240
    },
    {
        "content": "<p>Is there a kernel option to make it not trust the kvm clock?</p>",
        "id": 235553239,
        "sender_full_name": "Amanieu",
        "timestamp": 1619027419
    },
    {
        "content": "<p>Basically to make it ignore <code>PVCLOCK_TSC_STABLE_BIT</code>.</p>",
        "id": 235553300,
        "sender_full_name": "Amanieu",
        "timestamp": 1619027442
    },
    {
        "content": "<p>I'm not sure, but you can tell it to pick a different clock source</p>",
        "id": 235553393,
        "sender_full_name": "The 8472",
        "timestamp": 1619027487
    },
    {
        "content": "<p>From a quick grep, apparently not.</p>",
        "id": 235553551,
        "sender_full_name": "Amanieu",
        "timestamp": 1619027551
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"220273\">Jane Lusby</span> <a href=\"#narrow/stream/219381-t-libs/topic/more.20broken.20clocks/near/235552291\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/219381-t-libs/topic/more.20broken.20clocks/near/235550659\">said</a>:</p>\n<blockquote>\n<p>That said, I also wonder if we could fix the <em>kernel</em>'s implementation of MONOTONIC...</p>\n</blockquote>\n<p>this seems like a loosing battle, since presumably there is more than one kernel to worry about</p>\n</blockquote>\n<p>I think the distinction would be that if a kernel promises \"monotonic\" we could trust the kernel, while if we don't have such a promise we need to synthesize one ourselves.</p>",
        "id": 235553564,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619027556
    },
    {
        "content": "<p>did misunderstand what you meant when you said \"fix the kernel's implementation\"? I assumed you meant patch the kernel upstream somehow</p>",
        "id": 235553678,
        "sender_full_name": "Jane Lusby",
        "timestamp": 1619027613
    },
    {
        "content": "<p>There's a <code>clearcpuid</code> boot flag and KVM flags are communicated via cpuid, of sorts? So maybe it can be blocked.</p>",
        "id": 235554243,
        "sender_full_name": "The 8472",
        "timestamp": 1619027852
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"220273\">Jane Lusby</span> <a href=\"#narrow/stream/219381-t-libs/topic/more.20broken.20clocks/near/235553678\">said</a>:</p>\n<blockquote>\n<p>did misunderstand what you meant when you said \"fix the kernel's implementation\"? I assumed you meant patch the kernel upstream somehow</p>\n</blockquote>\n<p>That's exactly what I meant. If CLOCK_MONOTONIC goes backwards, that's a bug in the Linux kernel.</p>",
        "id": 235554427,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619027925
    },
    {
        "content": "<p>According to the documentation:</p>\n<blockquote>\n<p>All CLOCK_MONOTONIC variants guarantee that the  time  returned  by consecutive calls will not go backwards, but successive calls may—depending on the architecture—return identical (not-increased) time values.</p>\n</blockquote>",
        "id": 235554471,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619027954
    },
    {
        "content": "<p>yeah but it looks like they are just dealing with the same problem as we have, with an upstream clock lying about its properties (hardware or vm or whatever)</p>",
        "id": 235554533,
        "sender_full_name": "Mara",
        "timestamp": 1619028001
    },
    {
        "content": "<p>Sure, but perhaps the kernel should do the same thing we do, and try to detect if the clock goes backwards.</p>",
        "id": 235554639,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619028021
    },
    {
        "content": "<p>But yeah, ultimately, if the hardware or VM says \"I'm monotonic\" and then isn't, that's a hardware/VM bug.</p>",
        "id": 235554677,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619028045
    },
    {
        "content": "<p>The kernel should work around that if it can, but given the performance sensitivity of getting the time, it may or may not be able to.</p>",
        "id": 235554731,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619028067
    },
    {
        "content": "<p>the kernel does have the same workaround as we have</p>",
        "id": 235554798,
        "sender_full_name": "Mara",
        "timestamp": 1619028101
    },
    {
        "content": "<p>would the kernel be able to do anything about that if gettime is going through the VDSO?</p>",
        "id": 235554799,
        "sender_full_name": "Steven Fackler",
        "timestamp": 1619028102
    },
    {
        "content": "<p>and just like us they disable it on some trusted sources</p>",
        "id": 235554830,
        "sender_full_name": "Mara",
        "timestamp": 1619028117
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"310399\">@Mara</span> In some cases, the kernel may have enough information to detect that the sources are lying about being trustable.</p>",
        "id": 235554972,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619028169
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"243558\">@Steven Fackler</span> yeah that's an interesting question. the code in the vdso wouldn't have write access to anything in kernel memory, but it could do it in a per-process way.</p>",
        "id": 235554991,
        "sender_full_name": "Mara",
        "timestamp": 1619028175
    },
    {
        "content": "<p>the kernel knows more than we do though. it can look at cpuids, choose different clock sources and can afford to spend a few microseconds on a calibration loop during boot time.</p>",
        "id": 235554993,
        "sender_full_name": "The 8472",
        "timestamp": 1619028175
    },
    {
        "content": "<p>For instance, \"hmmm, the bit is set to say it's really monotonic, but the hardware ID says it's this buggy VM, so, no\".</p>",
        "id": 235555054,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619028192
    },
    {
        "content": "<p>yeah. computers are hard :(</p>",
        "id": 235555152,
        "sender_full_name": "Mara",
        "timestamp": 1619028239
    },
    {
        "content": "<p>If we can get more information about the buggy environment people are using that lets monotonic go backwards, we might be able to get the Linux kernel to denylist that environment's \"I'm monotonic!\" flag and say \"no, you're not\".</p>",
        "id": 235555175,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619028243
    },
    {
        "content": "<p>checking whether the time goes backwards on every tsc call is exactly as expensive/reliable as forcing it to be monotonic. either you do it thread/core-local in which case it's not 100% reliable or you do it globally and then you suffer cache misses.<br>\nthe only way to be cheaper than that is trusting the various hardware/vm flags that say \"I promise to be monotic\"</p>",
        "id": 235555607,
        "sender_full_name": "The 8472",
        "timestamp": 1619028366
    },
    {
        "content": "<p>i was about to link <a href=\"https://github.com/rust-lang/rust/issues/83093\">#83093</a>, but that's your PR :)</p>",
        "id": 235555803,
        "sender_full_name": "Mara",
        "timestamp": 1619028425
    },
    {
        "content": "<p>once that's in place we can probably just enable that on all x86 platforms</p>",
        "id": 235556024,
        "sender_full_name": "Mara",
        "timestamp": 1619028488
    },
    {
        "content": "<p>ah that pr already does that</p>",
        "id": 235556259,
        "sender_full_name": "Mara",
        "timestamp": 1619028569
    },
    {
        "content": "<p>well, it might still suck for people who want to do really fine-grained measurements. it gets more expensive the more parallelism you have. while tsc in itself is constant overhead no matter how many threads.</p>",
        "id": 235556261,
        "sender_full_name": "The 8472",
        "timestamp": 1619028570
    },
    {
        "content": "<p>no, it doesn't</p>",
        "id": 235556290,
        "sender_full_name": "The 8472",
        "timestamp": 1619028580
    },
    {
        "content": "<p>I still think we shouldn't add the overhead to <em>all</em> Linux platforms, given that Linux promises that monotonic is monotonic.</p>",
        "id": 235556303,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619028585
    },
    {
        "content": "<p>oh i'm misreading it</p>",
        "id": 235556355,
        "sender_full_name": "Mara",
        "timestamp": 1619028603
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/pull/83093/files#diff-3183738c82f747fd92bb573712073aa384b16e14d154bf2df18f41077edc31cbR318-R320\">https://github.com/rust-lang/rust/pull/83093/files#diff-3183738c82f747fd92bb573712073aa384b16e14d154bf2df18f41077edc31cbR318-R320</a><br>\nOS_MONOTONIC is the old check, so x86 would still be trusted.</p>",
        "id": 235556405,
        "sender_full_name": "The 8472",
        "timestamp": 1619028616
    },
    {
        "content": "<blockquote>\n<p>I still think we shouldn't add the overhead to all Linux platforms, given that Linux promises that monotonic is monotonic.</p>\n</blockquote>\n<p>Well, we could add more checks to the standard lib too. E.g. probe for virtualization or the used clock source. It only has to be done once.</p>",
        "id": 235556734,
        "sender_full_name": "The 8472",
        "timestamp": 1619028727
    },
    {
        "content": "<p>Or we could tell users to report to their OS vendor (for enterprisy kernels) or upstream if they're recent?</p>",
        "id": 235556858,
        "sender_full_name": "The 8472",
        "timestamp": 1619028784
    },
    {
        "content": "<p>The problem is that in <a href=\"https://github.com/rust-lang/rust/issues/80674\">#80674</a> the buggy kernel is actually the Linux kernel running as the hypervisor, for which we can't easily get a version number from inside the VM.</p>",
        "id": 235557240,
        "sender_full_name": "Amanieu",
        "timestamp": 1619028960
    },
    {
        "content": "<p>Maybe we could provide an environment to override our <code>actually_monotonic()</code>?</p>\n<p><code>MY_CLOCK_IS_RIGHT=twice_a_day</code> vs. <code>MY_CLOCK_IS_RIGHT=always</code></p>",
        "id": 235558110,
        "sender_full_name": "The 8472",
        "timestamp": 1619029333
    },
    {
        "content": "<p>that'd help people with broken hardware without slowing everyone else down.</p>",
        "id": 235558200,
        "sender_full_name": "The 8472",
        "timestamp": 1619029379
    },
    {
        "content": "<p>If we can check that variable once and cache that check, such that we add ~0 overhead for people who don't set the variable, that sounds entirely reasonable.</p>",
        "id": 235559326,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619029897
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"330154\">@The 8472</span> You said \"broken hypervisors on old CPUs without nonstop_tsc\". Do such hypervisors pass enough information through that we can detect the flag on the CPU, or is the hypervisor setting that CPU flag without actually having the CPU flag set on the host CPU?</p>",
        "id": 235559608,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619030020
    },
    {
        "content": "<p>Perhaps there's another way we can detect those old buggy hypervisors, via some of the virtual-BIOS-advertised information that has changed in more recent kernels.</p>",
        "id": 235559743,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619030065
    },
    {
        "content": "<p>Some googling tells me that the CPU (E5-2690 v3) itself should have nonstop_tsc support, but the guest cpuinfo didn't show it. So the question is whether the hypervisor didn't pass that through or the kernel is too old to recognize it.</p>",
        "id": 235560896,
        "sender_full_name": "The 8472",
        "timestamp": 1619030589
    },
    {
        "content": "<p>Ah, well, another possible source of confusion... the user posted stats from a different system which may not be the initial system on which he encountered the bug. So I wouldn't trust this data too much yet.</p>",
        "id": 235561038,
        "sender_full_name": "The 8472",
        "timestamp": 1619030663
    },
    {
        "content": "<p>NONSTOP_TSC predates 3.10, so the kernel should have recognized it <br>\n<a href=\"https://github.com/torvalds/linux/commit/40fb17152c50a69dc304dd632131c2f41281ce44\">https://github.com/torvalds/linux/commit/40fb17152c50a69dc304dd632131c2f41281ce44</a><br>\n<a href=\"https://openbenchmarking.org/s/2%20x%20Intel%20Xeon%20E5-2690%20v3\">https://openbenchmarking.org/s/2%20x%20Intel%20Xeon%20E5-2690%20v3</a></p>",
        "id": 235563282,
        "sender_full_name": "The 8472",
        "timestamp": 1619031607
    },
    {
        "content": "<p>If the guest cpuinfo didn't pass through nonstop_tsc, why did the kernel assume the TSC was usable?</p>",
        "id": 235563775,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619031767
    },
    {
        "content": "<p>/me is confused.</p>",
        "id": 235563790,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619031772
    },
    {
        "content": "<p>It used <code>kvm_clock</code>, for which the host can also promise that it is reliable.</p>",
        "id": 235565342,
        "sender_full_name": "The 8472",
        "timestamp": 1619032378
    },
    {
        "content": "<p>I've seen clocks going back on x86_64 linux many times, incl. bare recent hardware from the certain major blue-coloured vendor, and on recent kernels.</p>",
        "id": 235592557,
        "sender_full_name": "nagisa",
        "timestamp": 1619044425
    },
    {
        "content": "<p>To the point that all of the code computing timestamp deltas at my current employer manually handles this possiblity.</p>",
        "id": 235592650,
        "sender_full_name": "nagisa",
        "timestamp": 1619044458
    },
    {
        "content": "<p>to give an estimate of how common this is on x86_64, AFAIR we'd observe a crash related to this ~once a month for every 50 processes (with 4 processes/server), where within each process the <code>duration_since</code> or <code>elapsed</code> are called with timestamps that should be approx 30ms apart, about 30 times per second.</p>",
        "id": 235593391,
        "sender_full_name": "nagisa",
        "timestamp": 1619044886
    },
    {
        "content": "<p>I can try digging up cpuinfo and similar info from the hardware on which this used to happen if anybody cares.</p>",
        "id": 235593898,
        "sender_full_name": "nagisa",
        "timestamp": 1619045183
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"123586\">@nagisa</span> It would be helpful to get <code>/proc/cpuinfo</code>, <code>dmesg</code>, and <code>/sys/devices/system/clocksource/clocksource0/current_clocksource</code> from a bare-metal system that has that issue.</p>",
        "id": 235599144,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619048760
    },
    {
        "content": "<p>One of the machines that had this issue surface 6 months ago:</p>\n<div class=\"codehilite\"><pre><span></span><code>$ cat /sys/devices/system/clocksource/clocksource0/current_clocksource\ntsc\n</code></pre></div>\n<p><a href=\"https://paste.rs/9UL\">procinfo</a>, <a href=\"https://paste.rs/EHh\">dmesg</a>. <a href=\"https://paste.rs/AEn\">another procinfo</a> from a machine we've seen this happen a year ago. Same <code>clocksource</code>. I believe at the time we were using the LTS kernel used in then current ubuntu LTS release  (4.4)</p>\n<p>Now that I checked both of these are sandy bridges and, in fact, the one of them is the same chip as from the original issue report. I do recall us finding, at the time, a number of issue reports around the <code>redhat</code>'s bugtracker as well as at <a href=\"https://stackoverflow.com/questions/3657289/linux-clock-gettimeclock-monotonic-strange-non-monotonic-behavior\">other places</a> about <code>CLOCK_MONOTONIC</code> not being actually monotonic.</p>",
        "id": 235653908,
        "sender_full_name": "nagisa",
        "timestamp": 1619087712
    },
    {
        "content": "<p>Sadly the fleet I have access to is not extensive enough to say whether this is a sandybridge specific issue or not.</p>",
        "id": 235657443,
        "sender_full_name": "nagisa",
        "timestamp": 1619089815
    },
    {
        "content": "<p>hmm, my i7-7700K and Ryzen 7 3800X both say <code>tsc</code> for that on Fedora 34</p>",
        "id": 235704568,
        "sender_full_name": "cuviper",
        "timestamp": 1619107742
    },
    {
        "content": "<p><code>available_clocksource</code> has <code>tsc hpet acpi_pm</code> -- do you know where this gets selected?</p>",
        "id": 235704675,
        "sender_full_name": "cuviper",
        "timestamp": 1619107796
    },
    {
        "content": "<p>actually, Intel tsc has been stable for a while now, no?</p>",
        "id": 235704886,
        "sender_full_name": "cuviper",
        "timestamp": 1619107870
    },
    {
        "content": "<p>ah, dmesg shows <code>clocksource: Switched to clocksource tsc</code>, so I guess that is detected</p>",
        "id": 235705110,
        "sender_full_name": "cuviper",
        "timestamp": 1619107958
    },
    {
        "content": "<p>it checks a few cpuid flags and then does a calibration loop against one of the other timers and if all that passes selects tsc as default, if not it'll pick some of the other ones. there also are boot options to specify a preference. you can also explicitly mark tsc as unreliable at boot.</p>",
        "id": 235707602,
        "sender_full_name": "The 8472",
        "timestamp": 1619109013
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/issues/84448\">#84448</a></p>",
        "id": 235742042,
        "sender_full_name": "The 8472",
        "timestamp": 1619123081
    },
    {
        "content": "<p>I would use this in production software if it wasn't an environment variable tbh.</p>",
        "id": 235746058,
        "sender_full_name": "nagisa",
        "timestamp": 1619124982
    },
    {
        "content": "<p>As is there's risk that setting the envvar like this would break in case something somehow read the clock before main.</p>",
        "id": 235746165,
        "sender_full_name": "nagisa",
        "timestamp": 1619125035
    },
    {
        "content": "<p>And so adding calls to read the clock in any code then could become a breaking/observable change.</p>",
        "id": 235746230,
        "sender_full_name": "nagisa",
        "timestamp": 1619125076
    },
    {
        "content": "<p>I will write these down as pr comments once I'm back at computer if I don't forget.</p>",
        "id": 235746448,
        "sender_full_name": "nagisa",
        "timestamp": 1619125187
    },
    {
        "content": "<p>setting an environment variable through your... environment is more difficult than setting it in main?</p>",
        "id": 235748076,
        "sender_full_name": "The 8472",
        "timestamp": 1619125541
    },
    {
        "content": "<p>oh I guess, you mean to just put in a build so it's always the default no matter where it's deployed. Yeah, that's not really how I thought about it. It's meant for known-bad hardware.</p>",
        "id": 235749008,
        "sender_full_name": "The 8472",
        "timestamp": 1619125800
    },
    {
        "content": "<p>Should we go as far as mentioning this in the panic message for <code>Instant</code> subtraction?</p>",
        "id": 235761866,
        "sender_full_name": "Amanieu",
        "timestamp": 1619132579
    },
    {
        "content": "<p>I have added a documentation section on <code>Instant</code> and linked there from <code>elapsed()#Panics</code></p>",
        "id": 235763138,
        "sender_full_name": "The 8472",
        "timestamp": 1619133395
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"143274\">Amanieu</span> <a href=\"#narrow/stream/219381-t-libs/topic/more.20broken.20clocks/near/235761866\">said</a>:</p>\n<blockquote>\n<p>Should we go as far as mentioning this in the panic message for <code>Instant</code> subtraction?</p>\n</blockquote>\n<p>I think we should.</p>",
        "id": 235764730,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1619134522
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/pull/84448#issuecomment-944421516\">Alternative proposal</a>:</p>\n<blockquote>\n<ol>\n<li>change <code>Instant::{elapsed, duration_since}</code> to saturating</li>\n<li>deprecate <code>Instant::saturating_duration_since</code></li>\n<li>remove all the backslide protection code</li>\n</ol>\n<p>Instead of panics backslides would now saturate to zero. The downside is that <code>duration_since</code> would no longer catch the programming error where the start and end time of an interval are swapped, the developer either has to sanity-check the output or switch to <code>checked_duration_since</code> to catch it.<br>\nAnother downside would be that backslides become observable via <code>checked_duration_since</code>, especially on systems that don't have a clock that guarantees monotonicity.</p>\n</blockquote>",
        "id": 257735640,
        "sender_full_name": "The 8472",
        "timestamp": 1634319303
    },
    {
        "content": "<p>I love this idea. &lt;3</p>",
        "id": 257818538,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634377626
    }
]