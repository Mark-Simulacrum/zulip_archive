[
    {
        "content": "<p>So we talked about trying to gather various bits of parallel data -- do we think we'll be able to do that by this Friday? It seems ok if not, but we should just put off the meeting</p>",
        "id": 165560684,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1557775477
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116009\">@nikomatsakis</span> I'm waiting on flags from <span class=\"user-mention\" data-user-id=\"116466\">@Zoxc</span>; I hope to start the implementation groundwork tomorrow morning. I am optimistic for Friday having relevant stats, though maybe not all (depending in part on how quickly I can get it done and get eddyb and you to run relevant commands)</p>",
        "id": 165583579,
        "sender_full_name": "simulacrum",
        "timestamp": 1557795767
    },
    {
        "content": "<p>You can use <code>-Z threads=n</code> for rustc and <code> -j n</code> for cargo</p>",
        "id": 165583749,
        "sender_full_name": "Zoxc",
        "timestamp": 1557795994
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116466\">@Zoxc</span> Should I use both simultaneously? Or test with -j{1,2,4,8} and -Zthreads={1,2,4,8} with both varying independently?</p>",
        "id": 165584237,
        "sender_full_name": "simulacrum",
        "timestamp": 1557796600
    },
    {
        "content": "<p>er, simultaneously meaning \"same n\"</p>",
        "id": 165584248,
        "sender_full_name": "simulacrum",
        "timestamp": 1557796612
    },
    {
        "content": "<p>Good question. I think just setting <code>-Z thread</code> will suffice. Using <code>-j1</code> might be more accurate if measuring overhead though</p>",
        "id": 165587253,
        "sender_full_name": "Zoxc",
        "timestamp": 1557801239
    },
    {
        "content": "<p>okay, will investigate</p>",
        "id": 165588612,
        "sender_full_name": "simulacrum",
        "timestamp": 1557803025
    },
    {
        "content": "<p>Depending on time will maybe try to do both</p>",
        "id": 165588617,
        "sender_full_name": "simulacrum",
        "timestamp": 1557803041
    },
    {
        "content": "<p>Are we testing parallel-rustc on any arbitrary crates/rust projects? Or we can narrow the list to some representative ones? This question comes from a confusion of mine that when we say we need to do experiment on parallel-rusts, is that means we need to guarantee the parallel-rustc will not add significant overhead to compiling any crates? Or actually we primarily want to have the parallel-rustc to perform consistently on any kinds of hardware (CPU architectures, num of cores/threads, etc.). I believe we should do both. And those two are actually orthogonal with each other, so that we can do them separately.</p>",
        "id": 165624730,
        "sender_full_name": "lwshang",
        "timestamp": 1557842422
    },
    {
        "content": "<p>We are testing it for now on perf.r-l.o and hopefully some other hardware (but that is a bit up in the air); with the set of crates we use there and the hardware we use there</p>",
        "id": 165625424,
        "sender_full_name": "simulacrum",
        "timestamp": 1557843039
    },
    {
        "content": "<p>That make sense. I want to know the plan further. When we move on to the public experiment phases. I believe we want to have comparison data for both kinds of testing I just mentioned.</p>",
        "id": 165625539,
        "sender_full_name": "lwshang",
        "timestamp": 1557843138
    },
    {
        "content": "<p>Oh, yeah, definitely -- I don't think we've established exactly what our goals are but we would want to test on different hardware/OS/etc and such</p>",
        "id": 165625863,
        "sender_full_name": "simulacrum",
        "timestamp": 1557843378
    },
    {
        "content": "<p>Have we got enough confidence from perf.rlo so that we can move on to the following phases?</p>",
        "id": 165625999,
        "sender_full_name": "lwshang",
        "timestamp": 1557843483
    },
    {
        "content": "<p>It's our best bet at this point for initial evaluation and should be relatively representative IMO</p>",
        "id": 165626864,
        "sender_full_name": "simulacrum",
        "timestamp": 1557844102
    },
    {
        "content": "<p>at least for these purposes</p>",
        "id": 165626891,
        "sender_full_name": "simulacrum",
        "timestamp": 1557844137
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116466\">@Zoxc</span> What rustc/cargo have the parallel-rustc feature? I've tried nightly and local build of master branch, neither has the feature.</p>",
        "id": 165649371,
        "sender_full_name": "lwshang",
        "timestamp": 1557859213
    },
    {
        "content": "<p>You need to set [rust] parallel-compiler = true in config.toml</p>",
        "id": 165649589,
        "sender_full_name": "Zoxc",
        "timestamp": 1557859348
    },
    {
        "content": "<p>should I clean the previous build? I just tried with adding such config. And use <code>./x.py build -i --stage 1</code>, the feature is still not available. I may get it work with a completely new compilation of rustc. But it takes long time (about 1 hour on my i5 desktop). I just want to avoid some redundant compilation.</p>",
        "id": 165650946,
        "sender_full_name": "lwshang",
        "timestamp": 1557860302
    },
    {
        "content": "<p>Yeah, you need to do x.py clean</p>",
        "id": 165651632,
        "sender_full_name": "Zoxc",
        "timestamp": 1557860675
    },
    {
        "content": "<p>While I'm waiting for the compilation, I just fill some info in the HackMD (<a href=\"https://hackmd.io/KmHulVmISKu7L2HmNgbPgg?edit\" target=\"_blank\" title=\"https://hackmd.io/KmHulVmISKu7L2HmNgbPgg?edit\">https://hackmd.io/KmHulVmISKu7L2HmNgbPgg?edit</a>) which was created by <span class=\"user-mention\" data-user-id=\"116009\">@nikomatsakis</span> last week. Niko mentions \"Mark\" to provide some guide on configuration/setup information. Which Mark should we call? I'm new here and don't want to bother some one else.</p>",
        "id": 165653700,
        "sender_full_name": "lwshang",
        "timestamp": 1557862004
    },
    {
        "content": "<p>That would be <span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span></p>",
        "id": 165654421,
        "sender_full_name": "Zoxc",
        "timestamp": 1557862560
    },
    {
        "content": "<p>I've verified that the parallel compiler works (i.e., can build all of perf) and have initial stats here <a href=\"https://perf.rust-lang.org/compare.html?start=80e7cde2238e837a9d6a240af9a3253f469bb2cf&amp;end=6f087ac1c17723a84fd45f445c9887dbff61f8c0\" target=\"_blank\" title=\"https://perf.rust-lang.org/compare.html?start=80e7cde2238e837a9d6a240af9a3253f469bb2cf&amp;end=6f087ac1c17723a84fd45f445c9887dbff61f8c0\">https://perf.rust-lang.org/compare.html?start=80e7cde2238e837a9d6a240af9a3253f469bb2cf&amp;end=6f087ac1c17723a84fd45f445c9887dbff61f8c0</a> but they're mostly meaningless (i.e., no -Zthreads and such was specified -- so I guess this is the -j4 with -Zthreads=1 but not confident that's correct). I will be collecting further stats over tonight and tomorrow hopefully</p>",
        "id": 165654609,
        "sender_full_name": "simulacrum",
        "timestamp": 1557862702
    },
    {
        "content": "<p>That should be -j8 + Zthreads=8, <a href=\"https://perf.rust-lang.org/compare.html?start=80e7cde2238e837a9d6a240af9a3253f469bb2cf&amp;end=6f087ac1c17723a84fd45f445c9887dbff61f8c0&amp;stat=wall-time\" target=\"_blank\" title=\"https://perf.rust-lang.org/compare.html?start=80e7cde2238e837a9d6a240af9a3253f469bb2cf&amp;end=6f087ac1c17723a84fd45f445c9887dbff61f8c0&amp;stat=wall-time\">https://perf.rust-lang.org/compare.html?start=80e7cde2238e837a9d6a240af9a3253f469bb2cf&amp;end=6f087ac1c17723a84fd45f445c9887dbff61f8c0&amp;stat=wall-time</a></p>",
        "id": 165654909,
        "sender_full_name": "Zoxc",
        "timestamp": 1557862951
    },
    {
        "content": "<p>ah okay so the default is num_cpus for threads, got it</p>",
        "id": 165655387,
        "sender_full_name": "simulacrum",
        "timestamp": 1557863263
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> so is there something you want me or <span class=\"user-mention\" data-user-id=\"119009\">@eddyb</span> to test?</p>",
        "id": 165745198,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1557947744
    },
    {
        "content": "<p>and do we think we'll have the set of data we were looking for ready? (or some subset?)</p>",
        "id": 165745210,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1557947758
    },
    {
        "content": "<p>We should have at the least a very good set of data</p>",
        "id": 165750778,
        "sender_full_name": "simulacrum",
        "timestamp": 1557951670
    },
    {
        "content": "<p>I want to confirm that the data I've currently gathered is correct (i.e., I'm gathering good data and not just spinning CPU cycles) and hope to get you two a script by tonight that'll dump relevant data in a directory</p>",
        "id": 165750826,
        "sender_full_name": "simulacrum",
        "timestamp": 1557951717
    },
    {
        "content": "<p>(But even if we don't get data from <span class=\"user-mention\" data-user-id=\"119009\">@eddyb</span> or you I think we will have enough to discuss)</p>",
        "id": 165750891,
        "sender_full_name": "simulacrum",
        "timestamp": 1557951740
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116009\">@nikomatsakis</span> <span class=\"user-mention\" data-user-id=\"119009\">@eddyb</span> If I could get you to run <a href=\"https://gist.github.com/Mark-Simulacrum/b5dd679b03ad3b979ffad2dfea2a7efc\" target=\"_blank\" title=\"https://gist.github.com/Mark-Simulacrum/b5dd679b03ad3b979ffad2dfea2a7efc\">https://gist.github.com/Mark-Simulacrum/b5dd679b03ad3b979ffad2dfea2a7efc</a> that'd be great; I would edit <a href=\"https://gist.github.com/Mark-Simulacrum/b5dd679b03ad3b979ffad2dfea2a7efc#file-collect-parallel-data-sh-L39\" target=\"_blank\" title=\"https://gist.github.com/Mark-Simulacrum/b5dd679b03ad3b979ffad2dfea2a7efc#file-collect-parallel-data-sh-L39\">this line</a> to go up to the number of cores you have</p>",
        "id": 165764969,
        "sender_full_name": "simulacrum",
        "timestamp": 1557963427
    },
    {
        "content": "<p>this will take approximately 2ish hours per \"run\" at least on perf.r-l.o's collector; the script as-is takes around 10 hours or so</p>",
        "id": 165765037,
        "sender_full_name": "simulacrum",
        "timestamp": 1557963526
    },
    {
        "content": "<p>if we don't get data though I've updated the doc with the data I've already collected</p>",
        "id": 165765054,
        "sender_full_name": "simulacrum",
        "timestamp": 1557963567
    },
    {
        "content": "<p><a href=\"https://hackmd.io/KmHulVmISKu7L2HmNgbPgg?both#Measurements\" target=\"_blank\" title=\"https://hackmd.io/KmHulVmISKu7L2HmNgbPgg?both#Measurements\">https://hackmd.io/KmHulVmISKu7L2HmNgbPgg?both#Measurements</a></p>",
        "id": 165765055,
        "sender_full_name": "simulacrum",
        "timestamp": 1557963569
    },
    {
        "content": "<p>That is not whole-crate-graph since I haven't had time to do that but is at least a start</p>",
        "id": 165765056,
        "sender_full_name": "simulacrum",
        "timestamp": 1557963583
    },
    {
        "content": "<p>thanks for gathering that data, <span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span>!</p>",
        "id": 165795624,
        "sender_full_name": "mw",
        "timestamp": 1558001014
    },
    {
        "content": "<p>I think we need whole-crate-graph data too, since that is the case where the jobserver comes into play</p>",
        "id": 165795704,
        "sender_full_name": "mw",
        "timestamp": 1558001065
    },
    {
        "content": "<p>Yeah, I'm working on some manual runs</p>",
        "id": 165802709,
        "sender_full_name": "simulacrum",
        "timestamp": 1558007969
    },
    {
        "content": "<p>though I suspect the reality might be that whole-crate graph is quite rare since most people are incrementally recompiling 1-2 crates at the end</p>",
        "id": 165802932,
        "sender_full_name": "simulacrum",
        "timestamp": 1558008132
    },
    {
        "content": "<p>for local builds yes, but for CI builds it's different</p>",
        "id": 165804264,
        "sender_full_name": "mw",
        "timestamp": 1558009298
    },
    {
        "content": "<p>true -- I could imagine us having Cargo try to detect how many crates it's building and set different -Z flags based on that</p>",
        "id": 165804516,
        "sender_full_name": "simulacrum",
        "timestamp": 1558009468
    },
    {
        "content": "<p>(well, -Cthreads= I guess)</p>",
        "id": 165804525,
        "sender_full_name": "simulacrum",
        "timestamp": 1558009485
    },
    {
        "content": "<p>Just read those compelling metrics. I'm thinking about a confusion about the threshold. Do we really care the overhead when run with \"-Zthreads=1\"?  This question is somehow important when it comes to how will the compiler behave in the future. </p>\n<p>In my envision, the compiler may automatically pick a sensible number of threads for parallel query. As long as the number is greater than 1,  parallel-rustc will provide significant performance gain to users. </p>\n<p>When rustc is running on an one-core/one-thread machine, can we make the compiler run as current \"single\" thread version instead of the real \"-Zthread=1\" which introduce overhead? Here, I'm only talking about the technical capability. Of course if the user explicitly set \"-Zthreads=1\", we should let the compiler run as the real parallel compiler even though only one thread can be used.</p>\n<p>If that is the case, then I believe we can definitely move on to the public experimental phases. Users can only encounter bad overhead when they force the compiler to run with \"-Zthreads=1\" which we will notify the users in the announcement blog.</p>",
        "id": 165820501,
        "sender_full_name": "lwshang",
        "timestamp": 1558020370
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"220056\">@lwshang</span> we do care about the overhead with -Zthreads=1, if for no other reason than sometimes there is only one core avail</p>",
        "id": 165833141,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558029613
    },
    {
        "content": "<p>In order to eliminate that overhead, we'd effectively have to ship two copies of the compiler</p>",
        "id": 165833215,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558029636
    },
    {
        "content": "<p>One compiled to avoid locks etc</p>",
        "id": 165833219,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558029641
    },
    {
        "content": "<p>and one not</p>",
        "id": 165833221,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558029642
    },
    {
        "content": "<p>not likely feasible</p>",
        "id": 165833225,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558029647
    },
    {
        "content": "<p>besides, the more we can optimize that overhead, the better for everyone</p>",
        "id": 165833239,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558029659
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> i'm giving your executable a try :)</p>",
        "id": 165833282,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558029694
    },
    {
        "content": "<p>your script, that is</p>",
        "id": 165833285,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558029698
    },
    {
        "content": "<p>As shown in the HackMD file, \"-Zthreads=1\" introduce a overhead about 10%~20%. Our consensus on the threshold suggest that such overhead should be smaller than 5%. Therefore, may I say that the parallel-rustc feature is not ready considering performance with \"-Zthreads=1\". </p>\n<p>Further, I have several questions following. Should we now focus on improving the implementation? Do we know how to improve it? Can we complete such improvement under our current roadmap?</p>",
        "id": 165838749,
        "sender_full_name": "lwshang",
        "timestamp": 1558033179
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116466\">@Zoxc</span> mentioned some global locks that I suspect we can eliminate or at least look at measuring and reducing contention</p>",
        "id": 165857628,
        "sender_full_name": "simulacrum",
        "timestamp": 1558048626
    },
    {
        "content": "<p>I plan to have rustc stage 0 timing data with/without parallel compiler as bootstrap and with various thread configurations; I am also currently doing  for Cargo. Depending on when that completes, I will try to get servo, cranelift, and/or ripgrep started so that it's ready by tomorrow morning</p>",
        "id": 165857707,
        "sender_full_name": "simulacrum",
        "timestamp": 1558048699
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> presuming that this process worked (don't see any <em>obvious</em> errors) the data files are at <a href=\"http://smallcultfollowing.com/perf-data.tar.gz\" target=\"_blank\" title=\"http://smallcultfollowing.com/perf-data.tar.gz\">http://smallcultfollowing.com/perf-data.tar.gz</a></p>",
        "id": 165862574,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558054171
    },
    {
        "content": "<p>Will get those uploaded tonight or tomorrow morning. Thanks!</p>",
        "id": 165869080,
        "sender_full_name": "simulacrum",
        "timestamp": 1558063369
    },
    {
        "content": "<p>I've retrieved the file if you want to take it down.</p>",
        "id": 165869090,
        "sender_full_name": "simulacrum",
        "timestamp": 1558063389
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> cool; is the data workable? :)</p>",
        "id": 165901643,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558100993
    },
    {
        "content": "<p>I think so -- working on getting links into the doc now</p>",
        "id": 165901693,
        "sender_full_name": "simulacrum",
        "timestamp": 1558101005
    },
    {
        "content": "<p>Nice</p>",
        "id": 165901702,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558101012
    },
    {
        "content": "<p>Thanks <span aria-label=\"heart\" class=\"emoji emoji-2764\" role=\"img\" title=\"heart\">:heart:</span> for all your help on this</p>",
        "id": 165901705,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558101018
    },
    {
        "content": "<blockquote>\n<p>Further, I have several questions following. Should we now focus on improving the implementation? Do we know how to improve it? Can we complete such improvement under our current roadmap?</p>\n</blockquote>\n<p>BTW, <span class=\"user-mention\" data-user-id=\"220056\">@lwshang</span>, I think this is exactly the kinds of questions I hope we will discuss this week! :)</p>",
        "id": 165901728,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558101045
    },
    {
        "content": "<p>it looks like some crates didn't compile on your machine for whatever reason (e.g., script servo) but we at least have some data</p>",
        "id": 165901920,
        "sender_full_name": "simulacrum",
        "timestamp": 1558101191
    },
    {
        "content": "<p>interestingly it looks like the 14 core machine is... slower? than perf's collection server</p>",
        "id": 165901929,
        "sender_full_name": "simulacrum",
        "timestamp": 1558101204
    },
    {
        "content": "<p>i.e. this is the perf vs your machine single-thread (i.e., current master) compiler <a href=\"https://perf.rust-lang.org/compare.html?start=single-threaded&amp;end=niko-single-thread&amp;stat=wall-time\" target=\"_blank\" title=\"https://perf.rust-lang.org/compare.html?start=single-threaded&amp;end=niko-single-thread&amp;stat=wall-time\">https://perf.rust-lang.org/compare.html?start=single-threaded&amp;end=niko-single-thread&amp;stat=wall-time</a></p>",
        "id": 165901946,
        "sender_full_name": "simulacrum",
        "timestamp": 1558101229
    },
    {
        "content": "<p>interesting</p>",
        "id": 165902050,
        "sender_full_name": "mw",
        "timestamp": 1558101304
    },
    {
        "content": "<p>Spawning 28 threads probably account for the hello-world overhead. Clock speeds are likely lower too</p>",
        "id": 165902062,
        "sender_full_name": "Zoxc",
        "timestamp": 1558101319
    },
    {
        "content": "<p>Then there's some wins due to LLVM using 14 cores</p>",
        "id": 165902096,
        "sender_full_name": "Zoxc",
        "timestamp": 1558101362
    },
    {
        "content": "<p>maybe, yeah</p>",
        "id": 165902156,
        "sender_full_name": "simulacrum",
        "timestamp": 1558101381
    },
    {
        "content": "<p>Did niko only bench with 8 threads?</p>",
        "id": 165902311,
        "sender_full_name": "Zoxc",
        "timestamp": 1558101490
    },
    {
        "content": "<p>up to 8 threads</p>",
        "id": 165902328,
        "sender_full_name": "simulacrum",
        "timestamp": 1558101507
    },
    {
        "content": "<p>(but even that is quite a bit slower than on perf.rlo, so ...)</p>",
        "id": 165902342,
        "sender_full_name": "simulacrum",
        "timestamp": 1558101520
    },
    {
        "content": "<p>i.e. this is perf 8 threads vs Niko 8 threads: <a href=\"https://perf.rust-lang.org/compare.html?start=parallel-rustc-8&amp;end=niko-parallel-rustc-8&amp;stat=wall-time\" target=\"_blank\" title=\"https://perf.rust-lang.org/compare.html?start=parallel-rustc-8&amp;end=niko-parallel-rustc-8&amp;stat=wall-time\">https://perf.rust-lang.org/compare.html?start=parallel-rustc-8&amp;end=niko-parallel-rustc-8&amp;stat=wall-time</a></p>",
        "id": 165902394,
        "sender_full_name": "simulacrum",
        "timestamp": 1558101561
    },
    {
        "content": "<p>There seems to be a 0.3s startup cost on niko's machine</p>",
        "id": 165902487,
        "sender_full_name": "Zoxc",
        "timestamp": 1558101615
    },
    {
        "content": "<p>I was mostly interested in contention though, probably should use 1 CGU and all cores for that</p>",
        "id": 165902566,
        "sender_full_name": "Zoxc",
        "timestamp": 1558101685
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"124287\">@mw</span> Did you peek at <a href=\"https://github.com/rust-lang/rust/pull/60035\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/pull/60035\">https://github.com/rust-lang/rust/pull/60035</a> yet?</p>",
        "id": 165902674,
        "sender_full_name": "Zoxc",
        "timestamp": 1558101784
    },
    {
        "content": "<p>That would be helpful for incremental+parallel performance, but it's quite large</p>",
        "id": 165902700,
        "sender_full_name": "Zoxc",
        "timestamp": 1558101830
    },
    {
        "content": "<p>only superficially. I figured I'd wait until the half the commits weren't called \"wip\" anymore :)</p>",
        "id": 165902755,
        "sender_full_name": "mw",
        "timestamp": 1558101846
    },
    {
        "content": "<p>Well I can squash them all together to one =P</p>",
        "id": 165902777,
        "sender_full_name": "Zoxc",
        "timestamp": 1558101876
    },
    {
        "content": "<p>perf results looked promising, but not entirely uncontroversial</p>",
        "id": 165902790,
        "sender_full_name": "mw",
        "timestamp": 1558101885
    },
    {
        "content": "<p>I did review the \"preliminaries\" PR though</p>",
        "id": 165902831,
        "sender_full_name": "mw",
        "timestamp": 1558101923
    },
    {
        "content": "<p>The key idea is to use the same <code>DepNodeIndex</code> across session, this avoids having locks when marking nodes as green, since we no longer need to allocate new indices.</p>",
        "id": 165902946,
        "sender_full_name": "Zoxc",
        "timestamp": 1558102023
    },
    {
        "content": "<p>If you think it's ready to review, I can take a look next week</p>",
        "id": 165903093,
        "sender_full_name": "mw",
        "timestamp": 1558102142
    },
    {
        "content": "<p>I'm wondering how <code>-j</code> flag of <code>cargo</code> interact with the <code>parallel-rustc</code>.  Correct me if I have any wrong or inaccurate understanding below.</p>\n<p>From my intuition,<code>-j</code> controls maximum number of concurrent jobs during <code>cargo build</code>. Such jobs refer to the crates that can be compiled at the same time (no inter-dependency). Meanwhile,  <code>parallel-rustc</code> try to make the compilation inside one crate to be concurrent.</p>\n<p>On a N-core machine, we run with <code>cargo build -jN</code>. If at some time point during the build, there are N dependencies can be compiled at the same time (told by dep graph), then roughly all the computation power of the machine is being used. In that case, we may not expect a performance gain by having <code>parallel-rustc</code> enabled and the overhead introduced by the feature may cause a performance regression. When the dep graph makes it impossible to have N concurrent jobs to run, we can expect the <code>parallel-rustc</code> to provide significant performance improvement.</p>",
        "id": 166216166,
        "sender_full_name": "lwshang",
        "timestamp": 1558475436
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"220056\">@lwshang</span> yes, that is correct.</p>",
        "id": 166246888,
        "sender_full_name": "mw",
        "timestamp": 1558513546
    }
]