[
    {
        "content": "<p>Action items from today:</p>",
        "id": 185232744,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1578589462
    },
    {
        "content": "<ul>\n<li>alex and others will work on filling out <a href=\"https://hackmd.io/rZZtwhJMTHm_Ivszf6uWmg?both\" target=\"_blank\" title=\"https://hackmd.io/rZZtwhJMTHm_Ivszf6uWmg?both\">https://hackmd.io/rZZtwhJMTHm_Ivszf6uWmg?both</a></li>\n</ul>",
        "id": 185232767,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1578589474
    },
    {
        "content": "<p>have you recorded the meeting?</p>",
        "id": 185232883,
        "sender_full_name": "Santiago Pastorino",
        "timestamp": 1578589552
    },
    {
        "content": "<ul>\n<li><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> will work on making the cargo jobserver changes behind a flag</li>\n</ul>",
        "id": 185232995,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1578589605
    },
    {
        "content": "<ul>\n<li>@cuviper will look into rayon and see if our lazy spawn strategy might cause any issues with internals</li>\n</ul>",
        "id": 185232999,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1578589607
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116266\">@Santiago Pastorino</span> oh rats we forgot to record :(</p>",
        "id": 185233045,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1578589637
    },
    {
        "content": "<p>no worries, should have asked for that before</p>",
        "id": 185234661,
        "sender_full_name": "Santiago Pastorino",
        "timestamp": 1578590684
    },
    {
        "content": "<p>RE the effect of a custom lazy spawns, I found only a few ways that rayon would actually notice that threads are missing:</p>\n<ol>\n<li>if you don't actually spawn _any_ threads, then calls into the threadpool will block forever</li>\n<li><code>Registry::wait_until_primed</code> will block until all threads have checked in from their main loop. This is only called from <code>ThreadPoolBuilder::build_global</code>, which doesn't affect rustc, and frankly I don't think we really need it anyway. The loose justification is just to help warm up for benchmarking purposes.</li>\n<li>\n<p><code>Registry::wait_until_stopped</code> will block until all threads have indicated that they've exited their main loop. This is <code>#[cfg(test)]</code> only on vanilla rayon, but rustc-rayon calls this at the end of <code>ThreadPoolBuilder::build_scoped</code> to make sure they all call the release handler before we exit.<br>\n    - if rustc uses a fully custom spawn handler instead, then <code>ThreadPoolBuilder::build</code>, it could handle that differently too</p>\n</li>\n<li>\n<p>rustc-rayon's deadlock detection will count all threads as \"active\" by default, so those lazy threads will keep the <code>deadlock_check</code> from ever triggering.</p>\n</li>\n</ol>",
        "id": 185268094,
        "sender_full_name": "cuviper",
        "timestamp": 1578612620
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"138448\">@cuviper</span> to clarify, is the conclusion that if we unconditionally spawn one rayon thread then we should be able to slowly spawn the others?</p>",
        "id": 185615443,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1579021133
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> point 3 means you'd have to spawn all the threads eventually. you might be able to fake that by running them directly during the threadpool shutdown, if you can find the right time to do that.<br>\nfor point 4, I don't know if anything is actually depending on that being effective? if so, I think that's a problem, and if not, maybe it should be removed</p>",
        "id": 185615958,
        "sender_full_name": "cuviper",
        "timestamp": 1579021404
    },
    {
        "content": "<p>hm ok, if we're forced to do it anyway then that may be a bit of a bummer, but this may just be part of rustc-rayon reviews/etc</p>",
        "id": 185616098,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1579021479
    },
    {
        "content": "<p>both of those are specific to rustc-rayon extensions. maybe they can be tweaked to help in this regard</p>",
        "id": 185617492,
        "sender_full_name": "cuviper",
        "timestamp": 1579022288
    },
    {
        "content": "<p>Ok to follow up on some of the investigation from the internals thread as well, I fille din some more results here -- <a href=\"https://hackmd.io/rZZtwhJMTHm_Ivszf6uWmg?both\" target=\"_blank\" title=\"https://hackmd.io/rZZtwhJMTHm_Ivszf6uWmg?both\">https://hackmd.io/rZZtwhJMTHm_Ivszf6uWmg?both</a></p>",
        "id": 185640630,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1579036234
    },
    {
        "content": "<p>in general I wasn't really able to reproduce anything, sort of as expected, but I did develop a few theories for why the regressions were so drastic</p>",
        "id": 185640670,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1579036249
    },
    {
        "content": "<p>the primary one is that the jobserver management bug in rustc which allows many rustc's to get spawned could blow memory limits easily, triggering swap, causing a big slowdown</p>",
        "id": 185640707,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1579036272
    },
    {
        "content": "<p>another one though is that when the parallel compiler is single threaded it's slower than it is today (as we already know) and if you're build is already extremely parallel (as some of them were) then you're getting very little benefit, this is the \"dynamically avoiding atomics\" topic in zulip right now</p>",
        "id": 185640737,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1579036309
    },
    {
        "content": "<p>Overall I don't see anything glaring or too too worrisome, I think we need to keep on fixing bugs though of course</p>",
        "id": 185640802,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1579036338
    },
    {
        "content": "<p>I plan to get my work item done tonight</p>",
        "id": 185641224,
        "sender_full_name": "simulacrum",
        "timestamp": 1579036620
    }
]