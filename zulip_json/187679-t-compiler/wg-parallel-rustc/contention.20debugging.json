[
    {
        "content": "<p>I tried to do some loose profiling of locks and such but that mostly failed (wasn't able to get low enough overhead / enough samples)</p>",
        "id": 181975908,
        "sender_full_name": "simulacrum",
        "timestamp": 1574811999
    },
    {
        "content": "<p>mostly pointed at <code>env::var</code> calls in MIR interpret and Cargo, along with Cargo's interner</p>",
        "id": 181975925,
        "sender_full_name": "simulacrum",
        "timestamp": 1574812022
    },
    {
        "content": "<p>neither of which seemed... plausible to actually be a hot spot</p>",
        "id": 181975940,
        "sender_full_name": "simulacrum",
        "timestamp": 1574812032
    },
    {
        "content": "<p>hopefully Santiago will have more success</p>",
        "id": 181975952,
        "sender_full_name": "simulacrum",
        "timestamp": 1574812051
    },
    {
        "content": "<p>Hm well env accesses are actually locked</p>",
        "id": 181983769,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574821234
    },
    {
        "content": "<p>So not entirely implausible if something is slamming env vars</p>",
        "id": 181983773,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574821257
    },
    {
        "content": "<p>We may need to instrument rustc to try to learn about contended locks</p>",
        "id": 181983791,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574821277
    },
    {
        "content": "<p><a href=\"https://hackmd.io/CBdNMYStS_GH2MawrgARfw?view#lock-contention\" target=\"_blank\" title=\"https://hackmd.io/CBdNMYStS_GH2MawrgARfw?view#lock-contention\">https://hackmd.io/CBdNMYStS_GH2MawrgARfw?view#lock-contention</a></p>",
        "id": 181984065,
        "sender_full_name": "simulacrum",
        "timestamp": 1574821686
    },
    {
        "content": "<p>I think this pretty much does not see parking_lot locks at all, which might explain high percetage here (though unclear -- the numbers from perf stat indicate that it sees all calls to futex, at least)</p>",
        "id": 181984075,
        "sender_full_name": "simulacrum",
        "timestamp": 1574821721
    },
    {
        "content": "<p><code>cargo::core::interning::InternedString::new</code>, <code>&lt;rustc::mir::interpret::error::InterpErrorInfo as core::convert::From&lt;...&gt;&gt;</code>, <code>cargo::util::profile::enabled_level</code></p>",
        "id": 181984094,
        "sender_full_name": "simulacrum",
        "timestamp": 1574821793
    },
    {
        "content": "<p>I think the cargo ones probably don't matter in practice</p>",
        "id": 181984151,
        "sender_full_name": "simulacrum",
        "timestamp": 1574821904
    },
    {
        "content": "<p>like, maybe we could get wins, but ultimately that's not important, cargo is mostly idle</p>",
        "id": 181984159,
        "sender_full_name": "simulacrum",
        "timestamp": 1574821922
    },
    {
        "content": "<p><code>&lt;rustc::mir::interpret::error::InterpErrorInfo as core::convert::From&lt;...&gt;&gt;</code> I plan to file a PR switching it out for a lazy static or something like that</p>",
        "id": 181984215,
        "sender_full_name": "simulacrum",
        "timestamp": 1574822006
    },
    {
        "content": "<p>unfortunately our normal benchmarks (perf.rlo) probably won't notice, as the uncontended lock is presumably quite cheap</p>",
        "id": 181984445,
        "sender_full_name": "simulacrum",
        "timestamp": 1574822323
    },
    {
        "content": "<p>Cargo should be easy to fix yeah, the string one seems the most plausible</p>",
        "id": 181986152,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574825005
    },
    {
        "content": "<p>Can you swap out parking lot for libstd to get contention numbers?</p>",
        "id": 181986154,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574825022
    },
    {
        "content": "<p>hm, maybe, I can try that</p>",
        "id": 182016672,
        "sender_full_name": "simulacrum",
        "timestamp": 1574860054
    },
    {
        "content": "<p>std's APIs are different I think and sufficiently so that this might not be readily possible</p>",
        "id": 182017038,
        "sender_full_name": "simulacrum",
        "timestamp": 1574860281
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> also -- forgot to note this -- you'll want to run <code>sudo rm -f /dev/shm/sem.jobserver-rust*</code> between builds; obviously that's not tenable long-term but for now I believe is necessary-ish</p>",
        "id": 182023563,
        "sender_full_name": "simulacrum",
        "timestamp": 1574864953
    },
    {
        "content": "<p><code>sync.rs</code> has a custom <code>Lock</code> wrapper, so API doesn't matter too much (<code>LockGuard</code> is reused though)</p>",
        "id": 182026202,
        "sender_full_name": "Zoxc",
        "timestamp": 1574866508
    },
    {
        "content": "<p>parking lot parks ~2000 times total in the first 3 seconds from raw_mutex...</p>",
        "id": 182026390,
        "sender_full_name": "simulacrum",
        "timestamp": 1574866620
    },
    {
        "content": "<p>~6000 total futex calls from parking lot</p>",
        "id": 182026952,
        "sender_full_name": "simulacrum",
        "timestamp": 1574866954
    },
    {
        "content": "<p>so parking lot is not the source of trouble</p>",
        "id": 182026956,
        "sender_full_name": "simulacrum",
        "timestamp": 1574866959
    },
    {
        "content": "<p>(this is on a run with perf stat reporting 457,198 futex calls)</p>",
        "id": 182026972,
        "sender_full_name": "simulacrum",
        "timestamp": 1574866975
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116466\">@Zoxc</span> yeah, std RwLock/Mutex don't support mapping afaict</p>",
        "id": 182026997,
        "sender_full_name": "simulacrum",
        "timestamp": 1574866994
    },
    {
        "content": "<p>initial findings suggest that contention is happening in:</p>\n<ul>\n<li>(100s of milliseconds - 1.5 seconds(!)) <code>rustc_rayon_core::sleep::Sleep::wake_specific_thread</code> </li>\n<li>(100s of nanoseconds) jobserver helper thread (Condvar/Mutex around a usize, fairly easily replaceable)</li>\n</ul>",
        "id": 182041807,
        "sender_full_name": "simulacrum",
        "timestamp": 1574876560
    },
    {
        "content": "<p>utilized this patch to libstd to gather data: <a href=\"https://gist.github.com/Mark-Simulacrum/6dfa7678f2d449175aa1f3d8856340f7\" target=\"_blank\" title=\"https://gist.github.com/Mark-Simulacrum/6dfa7678f2d449175aa1f3d8856340f7\">https://gist.github.com/Mark-Simulacrum/6dfa7678f2d449175aa1f3d8856340f7</a></p>",
        "id": 182041953,
        "sender_full_name": "simulacrum",
        "timestamp": 1574876649
    },
    {
        "content": "<p>it has fairly high overhead, though, so measurements may not be super reliable</p>",
        "id": 182041971,
        "sender_full_name": "simulacrum",
        "timestamp": 1574876662
    },
    {
        "content": "<p>overhead is much lower if I adjust the elapsed higher</p>",
        "id": 182042200,
        "sender_full_name": "simulacrum",
        "timestamp": 1574876802
    },
    {
        "content": "<p>(initial measurements were with ~100 nanoseconds)</p>",
        "id": 182042218,
        "sender_full_name": "simulacrum",
        "timestamp": 1574876817
    },
    {
        "content": "<p>but I can get ~65 job units in the 3 second run if I adjust to 300, which is about the same as I get without this</p>",
        "id": 182042255,
        "sender_full_name": "simulacrum",
        "timestamp": 1574876849
    },
    {
        "content": "<p>trying to run some more benchmarks and get a good sense of how reliable this is</p>",
        "id": 182042624,
        "sender_full_name": "simulacrum",
        "timestamp": 1574877118
    },
    {
        "content": "<p>I am consistently seeing stalls for hundreds of milliseconds in rustc_rayon_core::sleep::Sleep::wake_specific_thread</p>",
        "id": 182043278,
        "sender_full_name": "simulacrum",
        "timestamp": 1574877589
    },
    {
        "content": "<p>but maybe that's expected?</p>",
        "id": 182043286,
        "sender_full_name": "simulacrum",
        "timestamp": 1574877592
    },
    {
        "content": "<p>e.g., 51.508068ms; 772.124223ms; 773.960212ms; 58.569277ms; 951.101171ms; 51.476288ms; 866.302171ms; 867.230675ms</p>",
        "id": 182043676,
        "sender_full_name": "simulacrum",
        "timestamp": 1574877837
    },
    {
        "content": "<p>trying a different tack -- perf record can give me the mutex pointers and those are always stable in our case so going to log those on creation time, hopefully there's a) not too many mutexes being created and b) creation isn't as such hot enough that it causes problems</p>",
        "id": 182044479,
        "sender_full_name": "simulacrum",
        "timestamp": 1574878472
    },
    {
        "content": "<p>hm, okay, so that sort of didn't succeed - the address that is given to futex is not of the <code>pthread_mutex_t</code> it seems</p>",
        "id": 182054884,
        "sender_full_name": "simulacrum",
        "timestamp": 1574885651
    },
    {
        "content": "<p>it does seem like a lot of the futex calls are not coming from rustc? but this does not jive well with what we're seeing elsewhere, i.e., in alex's <a href=\"http://foo.rs\" target=\"_blank\" title=\"http://foo.rs\">foo.rs</a> tool</p>\n<div class=\"codehilite\"><pre><span></span>    808 cargo\n 236398 ld\n 125675 rustc\n</pre></div>",
        "id": 182056481,
        "sender_full_name": "simulacrum",
        "timestamp": 1574886918
    },
    {
        "content": "<p>I wonder if that tool is buggy</p>",
        "id": 182056490,
        "sender_full_name": "simulacrum",
        "timestamp": 1574886925
    },
    {
        "content": "<p>otoh, 125k is still pretty high</p>",
        "id": 182057193,
        "sender_full_name": "simulacrum",
        "timestamp": 1574887452
    },
    {
        "content": "<p>I am consistently getting most futex calls in ld, not rustc</p>",
        "id": 182060649,
        "sender_full_name": "simulacrum",
        "timestamp": 1574890578
    },
    {
        "content": "<p>and total futex call count is still around 400k so I don't think it's because I've made changes</p>",
        "id": 182060668,
        "sender_full_name": "simulacrum",
        "timestamp": 1574890612
    },
    {
        "content": "<p>I am kinda confused about the futex calls from ld as I didn't think that was parallelized -- I wonder if the ld I have is not the normal ld</p>",
        "id": 182060887,
        "sender_full_name": "simulacrum",
        "timestamp": 1574890791
    },
    {
        "content": "<p>ah, indeed, I'm using lld as my default linker it seems</p>",
        "id": 182060945,
        "sender_full_name": "simulacrum",
        "timestamp": 1574890814
    },
    {
        "content": "<p>What in the world is the linker using futexes for</p>",
        "id": 182072921,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574902482
    },
    {
        "content": "<p>Isn't it single threaded?</p>",
        "id": 182072926,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574902488
    },
    {
        "content": "<p>lld is not, apparently, according to some random googling</p>",
        "id": 182072930,
        "sender_full_name": "simulacrum",
        "timestamp": 1574902502
    },
    {
        "content": "<p>Interesting, so my script should probably change</p>",
        "id": 182072941,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574902520
    },
    {
        "content": "<p>And only account for rustc things</p>",
        "id": 182072944,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574902528
    },
    {
        "content": "<p>(to be clear, my interpretation of your script was that it tried to do so)</p>",
        "id": 182072954,
        "sender_full_name": "simulacrum",
        "timestamp": 1574902548
    },
    {
        "content": "<p>The linker we don't care too much about and likely just adds noise</p>",
        "id": 182072955,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574902548
    },
    {
        "content": "<p>Oh, nvmd then</p>",
        "id": 182072959,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574902559
    },
    {
        "content": "<p>though maybe it failed since it does seem like the numbers I'm getting are heavily inconsistent with the theory that the script was successful at doing so</p>",
        "id": 182073004,
        "sender_full_name": "simulacrum",
        "timestamp": 1574902585
    },
    {
        "content": "<p>it's also possible that <code>perf trace</code> has different properties than what I'm doing now, which is (I think) more low level</p>",
        "id": 182073026,
        "sender_full_name": "simulacrum",
        "timestamp": 1574902636
    },
    {
        "content": "<p>right now I've largely been struggling to figure out how to track down callers of e.g. <code>Mutex::new</code> -- collecting backtraces via std::backtrace is apparently way too slow (I get something like an order of magnitude, or two, slowdown)</p>",
        "id": 182073088,
        "sender_full_name": "simulacrum",
        "timestamp": 1574902694
    },
    {
        "content": "<p>I'm going to move it to just the lock() method or something</p>",
        "id": 182073107,
        "sender_full_name": "simulacrum",
        "timestamp": 1574902756
    },
    {
        "content": "<p>I'd actually warn against that</p>",
        "id": 182073183,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574902820
    },
    {
        "content": "<p>Or take.the numbers with a grain of salt</p>",
        "id": 182073184,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574902826
    },
    {
        "content": "<p>Backtraces are single threaded and extremely expensive to capture</p>",
        "id": 182073191,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574902842
    },
    {
        "content": "<p>So it may not be a very reliable way to gather contention data that way</p>",
        "id": 182073200,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1574902860
    },
    {
        "content": "<p>to be clear the new plan only collects the backtrace once we've already detected at least some contention</p>",
        "id": 182073218,
        "sender_full_name": "simulacrum",
        "timestamp": 1574902892
    },
    {
        "content": "<p>but yes, I agree that it's likely not perfect</p>",
        "id": 182073221,
        "sender_full_name": "simulacrum",
        "timestamp": 1574902898
    },
    {
        "content": "<p>(I'm grasping at straws a bit, to be honest)</p>",
        "id": 182073225,
        "sender_full_name": "simulacrum",
        "timestamp": 1574902907
    },
    {
        "content": "<p>one possible easy win that I should try though I have no numbers to back me up is to try and switch rayon over to parking_lot</p>",
        "id": 182073288,
        "sender_full_name": "simulacrum",
        "timestamp": 1574902980
    },
    {
        "content": "<p>since at least primitive benchmarks seem to make us believe that parking_lot is much better at both the contended and uncontended cases, and I am seeing at least some evidence pointing at rayon being a cause of contention</p>",
        "id": 182073313,
        "sender_full_name": "simulacrum",
        "timestamp": 1574903020
    },
    {
        "content": "<p>(in particular, parking lot currently barely calls futex)</p>",
        "id": 182073362,
        "sender_full_name": "simulacrum",
        "timestamp": 1574903054
    },
    {
        "content": "<p>parking lot didn't seem to help much</p>",
        "id": 182140906,
        "sender_full_name": "simulacrum",
        "timestamp": 1574983302
    },
    {
        "content": "<p>here's flamegraph of the futex calls in the first 3 seconds of cargo check on cargo for master, the rayon branch, and rayon+semaphores:</p>\n<p><a href=\"/user_uploads/4715/Jz9g2FT-0ugfyTLlDWWiS1EC/futex-rayon.svg\" target=\"_blank\" title=\"futex-rayon.svg\">futex-rayon.svg</a> <a href=\"/user_uploads/4715/--VgbxKWJZiSsG4r8JPKEKnl/futex-master.svg\" target=\"_blank\" title=\"futex-master.svg\">futex-master.svg</a> <a href=\"/user_uploads/4715/AYnG1WarMDHa_kOv522NhTnR/futex-semaphores.svg\" target=\"_blank\" title=\"futex-semaphores.svg\">futex-semaphores.svg</a></p>",
        "id": 182142208,
        "sender_full_name": "simulacrum",
        "timestamp": 1574985674
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> <code>sudo --preserve-env perf record -m256 --switch-output=600M --aio=4 -e syscalls:sys_enter_futex --call-graph dwarf -o ~/perf.data -- timeout 3s $CARGO check -j$T</code> with RUSTC and CARGO set appropriately and T=28, then <code>perf script</code> into stack-collapse / flamegraph (I use inferno, but shouldn't matter)</p>",
        "id": 182390347,
        "sender_full_name": "simulacrum",
        "timestamp": 1575321181
    },
    {
        "content": "<p>(I also have my own custom perf script that's quite a bit faster, but I haven't gotten around to publishing it -- it should be about equivalent in terms of output though)</p>",
        "id": 182390385,
        "sender_full_name": "simulacrum",
        "timestamp": 1575321211
    },
    {
        "content": "<p>woo thanks!</p>",
        "id": 182390386,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1575321211
    },
    {
        "content": "<p>I'll try and type up the other commands I was using</p>",
        "id": 182390407,
        "sender_full_name": "simulacrum",
        "timestamp": 1575321226
    }
]