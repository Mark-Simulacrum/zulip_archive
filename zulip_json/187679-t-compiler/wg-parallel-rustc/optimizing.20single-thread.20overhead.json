[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> -- it occurred to me that you might be interested in helping to optimize the \"single-threaded overhead\" of rustc running in parallel mode? </p>\n<p>/me tries to nerd-swipe :)</p>",
        "id": 179363803,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1572376413
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116009\">@nikomatsakis</span> Possibly, but I've just done 2 solid months of rustc perf work and I kind of need to do some Firefox stuff for a bit :/</p>",
        "id": 179377560,
        "sender_full_name": "njn",
        "timestamp": 1572385602
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> heh ok =)</p>",
        "id": 179444340,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1572449584
    },
    {
        "content": "<p>How do I build a single-threaded parallel rustc? <code>parallel-compiler = true</code> in <code>config.toml</code> is obvious, but I can't see anything in there about the number of threads</p>",
        "id": 179481655,
        "sender_full_name": "njn",
        "timestamp": 1572472536
    },
    {
        "content": "<p><code>rustc -Zthreads=N</code> at runtime, perhaps</p>",
        "id": 179481762,
        "sender_full_name": "njn",
        "timestamp": 1572472614
    },
    {
        "content": "<p>yes, the default is one thread today I believe, but I think it's good practice when benchmarking to always set -Zthreads</p>",
        "id": 179481826,
        "sender_full_name": "simulacrum",
        "timestamp": 1572472664
    },
    {
        "content": "<p>note that this only affects threads during query-parallelism, not codegen parallelism</p>",
        "id": 179481838,
        "sender_full_name": "simulacrum",
        "timestamp": 1572472679
    },
    {
        "content": "<p>Yes, default is 1 thread in librustc/session/config.rs, ethanks</p>",
        "id": 179482053,
        "sender_full_name": "njn",
        "timestamp": 1572472808
    },
    {
        "content": "<p>I did some basic comparisons between serial and parallel with one thread.</p>",
        "id": 179501411,
        "sender_full_name": "njn",
        "timestamp": 1572497339
    },
    {
        "content": "<p><code>packed-simd</code> has the greatest slowdown for a <code>check-clean</code> build, of about 7%</p>",
        "id": 179501424,
        "sender_full_name": "njn",
        "timestamp": 1572497359
    },
    {
        "content": "<p>That 7% is dominated by the fact that <code>get_query</code> is more expensive</p>",
        "id": 179501426,
        "sender_full_name": "njn",
        "timestamp": 1572497372
    },
    {
        "content": "<p>This is mostly due to sharding (which in a serial compiler doesn't really do anything) of the query cache. In particular, when a key is looked up it now gets hashed twice: once in <code>get_shard_by_value</code> to find the shard, and then again to look up the hash table within the shard.</p>",
        "id": 179501559,
        "sender_full_name": "njn",
        "timestamp": 1572497634
    },
    {
        "content": "<p>There is also some locking overhead, unsurprisingly.</p>",
        "id": 179501583,
        "sender_full_name": "njn",
        "timestamp": 1572497642
    },
    {
        "content": "<p>I think the duplicate hashing is the biggest source of overhead.</p>",
        "id": 179501605,
        "sender_full_name": "njn",
        "timestamp": 1572497658
    },
    {
        "content": "<p>It would be nice if we could compute the hash first, then say \"get the shard/value using this hash value\", but I don't think <code>HashMap</code> allows that.</p>",
        "id": 179501702,
        "sender_full_name": "njn",
        "timestamp": 1572497844
    },
    {
        "content": "<p>Unless <code>RawEntryBuilderMut</code> can be used for this?</p>",
        "id": 179501856,
        "sender_full_name": "njn",
        "timestamp": 1572498113
    },
    {
        "content": "<p><code>from_key_hashed_nocheck</code> might do the trick</p>",
        "id": 179502025,
        "sender_full_name": "njn",
        "timestamp": 1572498382
    },
    {
        "content": "<p>Indeed, I think <code>RawEntryBuilder::from_key_hashed_nocheck</code> does exactly what I want! And <code>Sharded</code> already has <code>get_shard_by_hash</code>. I will try this out tomorrow, see how it goes.</p>",
        "id": 179502179,
        "sender_full_name": "njn",
        "timestamp": 1572498722
    },
    {
        "content": "<p>Did you look at wall-times? 7% seem low compared to the last time this was benchmarked, unless non-<code>check-clean</code> builds have more overhead. Also which processor do you use?</p>",
        "id": 179516792,
        "sender_full_name": "Zoxc",
        "timestamp": 1572516556
    },
    {
        "content": "<p><code>try_get</code> hashes the key 3 times now. I tried reducing the 2 hashes to 1 before, but LLVM didn't inline stuff well, so it was a performance regressions. Keys are usually integers too, which means hashing is pretty cheap.</p>",
        "id": 179518649,
        "sender_full_name": "Zoxc",
        "timestamp": 1572517956
    },
    {
        "content": "<p>Does every query type get its own QueryCache? I count 188 query types</p>",
        "id": 179598167,
        "sender_full_name": "njn",
        "timestamp": 1572580836
    },
    {
        "content": "<p>And in the parallel compiler they are sharded into 32 pieces,</p>",
        "id": 179598175,
        "sender_full_name": "njn",
        "timestamp": 1572580847
    },
    {
        "content": "<p>And each QueryCache has two hashmaps.</p>",
        "id": 179598180,
        "sender_full_name": "njn",
        "timestamp": 1572580856
    },
    {
        "content": "<p>So that's a total of 188 * 32 * 2 = 12,032 hashmaps?</p>",
        "id": 179598192,
        "sender_full_name": "njn",
        "timestamp": 1572580875
    },
    {
        "content": "<p>Yes</p>",
        "id": 179599100,
        "sender_full_name": "Zoxc",
        "timestamp": 1572582350
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> nice :)</p>",
        "id": 179663227,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1572640799
    },
    {
        "content": "<p>@nikomatsakis: <a href=\"https://github.com/rust-lang/rust/pull/66013\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/pull/66013\">https://github.com/rust-lang/rust/pull/66013</a> is the PR that eliminates the repeated hashing</p>",
        "id": 179668654,
        "sender_full_name": "njn",
        "timestamp": 1572645066
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/pull/66012\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/pull/66012\">https://github.com/rust-lang/rust/pull/66012</a> is an interesting one -- I de-querified <code>trivial_dropck_outlives</code> and it was a perf win for the serial compiler because the cost of querying was actually larger than the cost of re-computing. And the benefit for the parallel compiler could only be higher, because the cost of querying is higher in the parallel compiler</p>",
        "id": 179668773,
        "sender_full_name": "njn",
        "timestamp": 1572645161
    },
    {
        "content": "<p>FWIW, here are my notes on parallel overhead from yesterday:</p>\n<ul>\n<li>Overhead is almost all in get_query()<ul>\n<li>Lock (<a href=\"http://atomic.rs\" target=\"_blank\" title=\"http://atomic.rs\">atomic.rs</a>, parking_lot) (outweighs the reduction in RefCell use)</li>\n<li><a href=\"http://sharded.rs\" target=\"_blank\" title=\"http://sharded.rs\">sharded.rs</a></li>\n<li>codegen/regalloc due to more code in get_query</li>\n<li>memcpy: QueryJob is bigger (144 bytes), memcpy is needed to initialize it</li>\n<li>Note: 188 query kinds, 32 QueryCaches each = 5056, 2 HashMaps each: 12,032 HashMaps!</li>\n</ul>\n</li>\n<li>__tls_get_addr: Rayon calls this a bit</li>\n</ul>",
        "id": 179668889,
        "sender_full_name": "njn",
        "timestamp": 1572645248
    },
    {
        "content": "<p>Even in the serial compiler, get_query is hot, and just the instruction counts for entering/exiting it is often 2-3% of all instructions. But it's almost impossible to inline because it has hundreds (thousands?) of call sites, via the query getters</p>",
        "id": 179669887,
        "sender_full_name": "njn",
        "timestamp": 1572646163
    },
    {
        "content": "<p>The whole query-based design of the compiler is a bit antithetical to parallelization -- everything centres around a giant chunk of shared state</p>",
        "id": 180084272,
        "sender_full_name": "njn",
        "timestamp": 1573080283
    },
    {
        "content": "<p>the sharding helps, but still</p>",
        "id": 180084279,
        "sender_full_name": "njn",
        "timestamp": 1573080289
    },
    {
        "content": "<p>I wonder if some per-thread caching might help</p>",
        "id": 180084294,
        "sender_full_name": "njn",
        "timestamp": 1573080304
    },
    {
        "content": "<p>I definitely think we should explore whether e.g. lockless or eventually consistent hashmaps could be good for us -- a lot of the time, we're fine if we're not quite seeing the most current state for queries</p>",
        "id": 180093197,
        "sender_full_name": "simulacrum",
        "timestamp": 1573088836
    },
    {
        "content": "<p>(or at least most queries)</p>",
        "id": 180093201,
        "sender_full_name": "simulacrum",
        "timestamp": 1573088842
    },
    {
        "content": "<p>Am I correct that this is essentially append-only data ? This is not the worst kind of shared state because at least you don't have the concurrent memory reclamation hell to take care of.</p>",
        "id": 180153547,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1573147066
    },
    {
        "content": "<p>Well, for the hashtable elements at least. Bucket management would be another story...</p>",
        "id": 180153656,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1573147136
    },
    {
        "content": "<p>we never delete from the hash tables and so forth so I <em>think</em> we could plausibly be able to come up with something without too much trouble :)</p>",
        "id": 180158582,
        "sender_full_name": "simulacrum",
        "timestamp": 1573150506
    },
    {
        "content": "<p>One thing that troubles me about the sharding is that it's one-size-fits-all, even though the various query types have very different properties</p>",
        "id": 180185271,
        "sender_full_name": "njn",
        "timestamp": 1573166684
    },
    {
        "content": "<p>Also, it's not respondent to the number of threads.</p>",
        "id": 180185281,
        "sender_full_name": "njn",
        "timestamp": 1573166698
    },
    {
        "content": "<p>In contrast, per-thread caching is responsive to the number of threads.</p>",
        "id": 180185307,
        "sender_full_name": "njn",
        "timestamp": 1573166721
    },
    {
        "content": "<p>I don't think sharding is very problematic. There doesn't seem to be much performance overhead to having too many shards, so we just have to make sure there's enough shards to reduce contention on core counts it make sense to run rustc with.</p>",
        "id": 180577446,
        "sender_full_name": "Zoxc",
        "timestamp": 1573605308
    },
    {
        "content": "<p>The query-based design is pretty bad for performance overall (expect for incremental performance). I wonder if we can have \"thread-local\" queries which use thread-local state instead of global state. It would be useful for cheap queries (like symbol names). I would have to think hard about how that would affect recursion detection and incremental things though.</p>",
        "id": 180577636,
        "sender_full_name": "Zoxc",
        "timestamp": 1573605505
    },
    {
        "content": "<p>To clarify something:</p>\n<p>When we say that the query-based design is \"bad\" for parallelism, we are specifically referring to the centralized caches? It seems pretty obviously <em>good</em> for parallelism to me in the sense that the more we can compose the compiler into side-effect-free functions, the better? I find this a bit confusing. It seems like it's more a question of how to arrange the execution than the query-based design <em>itself</em>.</p>",
        "id": 180742263,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1573746084
    }
]