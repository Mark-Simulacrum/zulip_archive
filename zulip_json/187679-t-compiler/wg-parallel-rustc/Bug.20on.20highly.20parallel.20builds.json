[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> What's the nature of the bug with highly parallel builds?</p>",
        "id": 183781873,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576697178
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> I think Alex was referring to the fact that at least on Linux we've measured fairly high contention inside the kernel due to our current strategy for limiting parallelism (i.e., GNU make jobserver, which on linux is a machine-global pipe), which can lead to abysmal performance with lots of threads</p>",
        "id": 183782726,
        "sender_full_name": "simulacrum",
        "timestamp": 1576697692
    },
    {
        "content": "<p>we have a few ideas for mitigation and are currently working on the likely solution which is making cargo be the source of truth for jobserver communication (though rustc will support, and always default to, normal jobserver operation)</p>",
        "id": 183782793,
        "sender_full_name": "simulacrum",
        "timestamp": 1576697739
    },
    {
        "content": "<p>yes this has to do with how we're limiting parallelism</p>",
        "id": 183784232,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576698705
    },
    {
        "content": "<p>the bug we've seen is that when you <code>write</code> to a pipe on linux, it wakes up <em>everyone</em> waiting on it</p>",
        "id": 183784282,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576698722
    },
    {
        "content": "<p>so, in the worst case, when you do <code>-Zthreds=72</code> you have 72^2 threads</p>",
        "id": 183784310,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576698738
    },
    {
        "content": "<p>72 rustc's, each with 72 threads</p>",
        "id": 183784316,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576698742
    },
    {
        "content": "<p>that's a lot of people to wake up all the time</p>",
        "id": 183784323,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576698748
    },
    {
        "content": "<p>and a lot of wasted time churning around</p>",
        "id": 183784327,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576698751
    },
    {
        "content": "<p>we plan to fix this by not actually having 72^2 threads waiting</p>",
        "id": 183784346,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576698767
    },
    {
        "content": "<p>but only 72 :)</p>",
        "id": 183784351,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576698773
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> This is actually a known bug that's being fixed in Linux <em>right now</em>.</p>",
        "id": 183785285,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699397
    },
    {
        "content": "<p>There was a large pipe rework in upstream Linux, and in the course of evaluating that, Linus noticed that both the old and the new pipe behavior had the \"thundering herd\" problem.</p>",
        "id": 183785312,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699419
    },
    {
        "content": "<p>So that's getting fixed now, which should substantially speed up <code>make -j72</code> as well.</p>",
        "id": 183785333,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699432
    },
    {
        "content": "<p>The only reason it isn't <em>already</em> fixed is that there's a bug in <code>make</code> that fixing it uncovered. :)</p>",
        "id": 183785411,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699472
    },
    {
        "content": "<p>sounds promising!</p>\n<p>Do we need to do anything on our side to get that behavior?</p>\n<p>I expect we'll still want our changes since macOS etc probably won't get that fix for a while</p>",
        "id": 183785424,
        "sender_full_name": "simulacrum",
        "timestamp": 1576699484
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> You wouldn't need to do anything. The main thing would be that you could just drop the limit to 4 threads on recent Linux.</p>",
        "id": 183785470,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699510
    },
    {
        "content": "<p>where recent is \"unreleased\"?</p>",
        "id": 183785483,
        "sender_full_name": "simulacrum",
        "timestamp": 1576699522
    },
    {
        "content": "<p>Also, does macOS actually have the thundering herd problem?</p>",
        "id": 183785489,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699525
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> Where recent right now is \"change not yet in git master, being evaluated\".</p>",
        "id": 183785503,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699539
    },
    {
        "content": "<p>We've not done benchmarking on macOS, but sort of assumed so -- it's also hard to expose as macOS with more than a 4-6 cores is pretty hard to get (modulo just released mac pros)</p>",
        "id": 183785590,
        "sender_full_name": "simulacrum",
        "timestamp": 1576699577
    },
    {
        "content": "<p>Yeah, that's fair. :)</p>",
        "id": 183785625,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699604
    },
    {
        "content": "<p>/me really wants someone to build a scalable macOS cloud.</p>",
        "id": 183785637,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699620
    },
    {
        "content": "<p>I imagine we'll not really get any benefits from these changes for at least a year or two, right? That's my impression of approximate timeline on getting kernel changes pushed into end-user's hands</p>",
        "id": 183785824,
        "sender_full_name": "simulacrum",
        "timestamp": 1576699755
    },
    {
        "content": "<p>i.e., there's no point in waiting or anything like that</p>",
        "id": 183785838,
        "sender_full_name": "simulacrum",
        "timestamp": 1576699768
    },
    {
        "content": "<p>so we probably want to move ahead with our fixes regardless</p>",
        "id": 183785921,
        "sender_full_name": "simulacrum",
        "timestamp": 1576699822
    },
    {
        "content": "<p>even if they get even more improved in the eventuality</p>",
        "id": 183785925,
        "sender_full_name": "simulacrum",
        "timestamp": 1576699829
    },
    {
        "content": "<p>Also <span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> -- I think you're not quite right that we'd have 72 threads waiting with our proposed fix, since at least I was expecting that we'd loosely have 1 thread per pipe</p>",
        "id": 183785961,
        "sender_full_name": "simulacrum",
        "timestamp": 1576699874
    },
    {
        "content": "<p>(just 72 threads total but that's not too interesting)</p>",
        "id": 183785977,
        "sender_full_name": "simulacrum",
        "timestamp": 1576699885
    },
    {
        "content": "<p>The changes should get into one of the next two or so kernels, I'd guess.</p>",
        "id": 183785986,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699904
    },
    {
        "content": "<p>It won't take years.</p>",
        "id": 183786044,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699925
    },
    {
        "content": "<p>Also, there's still value in interoperating with the standard jobserver.</p>",
        "id": 183786061,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576699942
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> whoa that's awesome! (that linux is fixing the root of the issue)</p>",
        "id": 183786123,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576699987
    },
    {
        "content": "<p>Yeah. I'm currently trying to grab the WIP patches and test them, to see how that does.</p>",
        "id": 183786149,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576700009
    },
    {
        "content": "<p>Well to be clear our proposal is entirely interoperable, rustc won't lose compat, just be more limited with default jobserver</p>",
        "id": 183786157,
        "sender_full_name": "simulacrum",
        "timestamp": 1576700012
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> oh true, it's more of a moral equivalent</p>",
        "id": 183786159,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576700014
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> so far we tested out switching to literal posix semaphores, and that simple change ended up fixing the scaling issues effectively on my 28-core machine</p>",
        "id": 183786176,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576700036
    },
    {
        "content": "<p>it's still not <em>perfect</em> because rustc immediately spawns ncores threads, which is quite a lot</p>",
        "id": 183786227,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576700048
    },
    {
        "content": "<p>so we need to fix that a bit, but the scaling was <em>much</em> better with posix semaphores</p>",
        "id": 183786233,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576700057
    },
    {
        "content": "<p>(where sempahores presumably don't have the thundering herd issue)</p>",
        "id": 183786239,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576700063
    },
    {
        "content": "<p>but yeah we're unlikely to stick with vanilla jobservers for macos/windows which are likely to have similar problems</p>",
        "id": 183786255,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576700076
    },
    {
        "content": "<p>Windows has pipe-equivalents that guarantee to only wake up one waiter.</p>",
        "id": 183787323,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576700885
    },
    {
        "content": "<p>I don't know macOS internals well enough to know if it does.</p>",
        "id": 183787338,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576700893
    },
    {
        "content": "<p>I think I'm mostly trying to figure out if you can <em>avoid</em> doing that work by instead making sure you're using the right OS primitive.</p>",
        "id": 183787364,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576700924
    },
    {
        "content": "<p>Hypothetically, if Windows and macOS could work around this problem by using something pipe-like that isn't a POSIX pipe, and Linux worked fine with recent kernels and we could limit to 4 threads on older kernels, would you still want to do the extra architectural work for a workaround?</p>",
        "id": 183787406,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576700969
    },
    {
        "content": "<p>I at least expect that there are other advantages to letting Cargo know this information -- for example, I've had some thoughts about scheduling jobserver tokens down in a more intelligent way than what any system primitive would give us I think</p>",
        "id": 183787681,
        "sender_full_name": "simulacrum",
        "timestamp": 1576701167
    },
    {
        "content": "<p>How so?</p>",
        "id": 183787716,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701210
    },
    {
        "content": "<p>/me is interested in ways to improve parallel builds.</p>",
        "id": 183787723,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701219
    },
    {
        "content": "<p>e.g. right now if you're building lots of crates it might make sense to make sure each rustc has 2 threads (presuming you have the cores) rather than have one rustc with 10 threads and the rest stuck at 1 or something like that</p>",
        "id": 183787804,
        "sender_full_name": "simulacrum",
        "timestamp": 1576701281
    },
    {
        "content": "<p>I mean, as long as you stay 100% CPU bound, you're doing work that will need to happen regardless. The problem comes in if you drop below 100% CPU.</p>",
        "id": 183787841,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701320
    },
    {
        "content": "<p>well, yes, but it's not necessarily true that you want to e.g. run codegen immediately for some crate if you won't need it for a while</p>",
        "id": 183787859,
        "sender_full_name": "simulacrum",
        "timestamp": 1576701346
    },
    {
        "content": "<p>Yeah, scheduling at a crate level may make sense.</p>",
        "id": 183787870,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701355
    },
    {
        "content": "<p>On that note, though...</p>",
        "id": 183787912,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701363
    },
    {
        "content": "<p>At that point it might make sense to adopt a \"pull\" model where we try to build <em>later</em> crates and let those drive what we build next.</p>",
        "id": 183787930,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701385
    },
    {
        "content": "<p>Simulating the idea of earliest-deadline-first.</p>",
        "id": 183787949,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701402
    },
    {
        "content": "<p>yes, or something like that -- certainly right now rustc isn't really capable of communicating that information up in an on demand fashion</p>",
        "id": 183787992,
        "sender_full_name": "simulacrum",
        "timestamp": 1576701462
    },
    {
        "content": "<p>For instance, almost any project I build that has syn in its dependency graph tends to wind up at a bottleneck where it's building <em>only</em> syn at some point.</p>",
        "id": 183787995,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701465
    },
    {
        "content": "<p>but e.g. rust analyzer I think has a model like this</p>",
        "id": 183788000,
        "sender_full_name": "simulacrum",
        "timestamp": 1576701470
    },
    {
        "content": "<p>So it'd be nice to start syn as soon as possible.</p>",
        "id": 183788008,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701476
    },
    {
        "content": "<p>I think that's kind of what I'm getting at -- we would potentially want to give syn as many threads as we can</p>",
        "id": 183788141,
        "sender_full_name": "simulacrum",
        "timestamp": 1576701571
    },
    {
        "content": "<p>and no system primitive would allow that level of prioritization I expect</p>",
        "id": 183788208,
        "sender_full_name": "simulacrum",
        "timestamp": 1576701604
    },
    {
        "content": "<p>True.</p>",
        "id": 183788379,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701735
    },
    {
        "content": "<p>I think the ideal model effectively looks like \"run everything at once, at two priority levels, higher for pull and lower for things we know we'll need eventually but we don't need yet\".</p>",
        "id": 183788415,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701775
    },
    {
        "content": "<p>You know the \"readahead\" trick on Linux, of \"figure out everything you need to read from disk, and next time start reading it in the order you'll need it\"? We could get a decent approximation of the same thing if we have an estimate for how long (in CPU time) each crate takes to build, assume that future builds will take similar amounts of time, and prioritize accordingly.</p>",
        "id": 183788552,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701882
    },
    {
        "content": "<p>Anyway, I think I've found the Linux patch I need to test.</p>",
        "id": 183788589,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576701920
    },
    {
        "content": "<p>I would agree that we probably want our own scheduling system even if linux/windows are fixed</p>",
        "id": 183788973,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576702237
    },
    {
        "content": "<p>I think at the bare minimum I agree that this linux kernel change may take a very long time to propagate, and we want to ramp up default parallelism sooner</p>",
        "id": 183788993,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576702259
    },
    {
        "content": "<p>but I also agree that we can probably more cleverly prioritize crates</p>",
        "id": 183789012,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576702270
    },
    {
        "content": "<p>e.g. cargo already has some degree of scheduling heuristics</p>",
        "id": 183789022,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576702276
    },
    {
        "content": "<p>which can help keep everything saturated in theory</p>",
        "id": 183789026,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576702283
    },
    {
        "content": "<p>and we could apply similar heuristics to \"we have a token, and N rustc instances want a token, who gets it?\"</p>",
        "id": 183789043,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576702297
    },
    {
        "content": "<p>And we could always simplify the logic in the future, if we ever get to the point that we can start relying on fixed versions.</p>",
        "id": 183789384,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576702547
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> oh another thing you may want to do for more parallelism on a 72-core machine is to set <code>-Ccodegen-units=100000000</code></p>",
        "id": 183790011,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576702995
    },
    {
        "content": "<p>or just set <code>CARGO_INCREMENTAL=1</code> in release mode</p>",
        "id": 183790015,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576703005
    },
    {
        "content": "<p>which may drop perf a bit but not a huge amount due to thinlto</p>",
        "id": 183790029,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576703015
    },
    {
        "content": "<p>I'll give those a try too, once I have numbers for baseline and <code>-Zthreads=72</code> with the patch I'm testing.</p>",
        "id": 183791155,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576703732
    },
    {
        "content": "<p>man that's a whole new world for me, compiling a new kernel and then running it locally <span aria-label=\"ghost\" class=\"emoji emoji-1f47b\" role=\"img\" title=\"ghost\">:ghost:</span></p>",
        "id": 183791249,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576703784
    },
    {
        "content": "<p>I'm also using the compile of the kernel I'm testing to cross-check the results. :)</p>",
        "id": 183792094,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576704370
    },
    {
        "content": "<p>Kernel compile, on the kernel without the patch:</p>\n<div class=\"codehilite\"><pre><span></span>real    0m41.377s\nuser    10m1.718s\nsys 2m25.655s\n</pre></div>",
        "id": 183792118,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576704414
    },
    {
        "content": "<p>Kernel compile on the kernel with the patch:</p>\n<div class=\"codehilite\"><pre><span></span>real    0m40.463s\nuser    9m55.603s\nsys 2m25.449s\n</pre></div>\n\n\n<p>Looks like it doesn't make <em>that</em> much difference to kernel compiles, but considering that half that time is serialized compressing/linking at the end anyway, that's still decent.</p>",
        "id": 183793890,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576705718
    },
    {
        "content": "<p>Now to try wasmtime...</p>",
        "id": 183793900,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576705725
    },
    {
        "content": "<p>Didn't help the wasmtime build with the default threads=4:</p>\n<div class=\"codehilite\"><pre><span></span>real    1m10.096s\nuser    17m38.136s\nsys 0m24.415s\n</pre></div>",
        "id": 183794188,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576705922
    },
    {
        "content": "<p>that looks pretty suspiciously high</p>",
        "id": 183794276,
        "sender_full_name": "simulacrum",
        "timestamp": 1576705979
    },
    {
        "content": "<p>I would expect system time to be far lower</p>",
        "id": 183794285,
        "sender_full_name": "simulacrum",
        "timestamp": 1576705986
    },
    {
        "content": "<p>maybe that's with cold disk cache?</p>",
        "id": 183794296,
        "sender_full_name": "simulacrum",
        "timestamp": 1576705996
    },
    {
        "content": "<p>Nope, that's the second build.</p>",
        "id": 183794385,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706052
    },
    {
        "content": "<p>For reference, the wasmtime build on the unpatched kernel with parallel rustc (and default threads) was:</p>\n<div class=\"codehilite\"><pre><span></span>real    1m9.191s\nuser    16m55.383s\nsys 0m23.964s\n</pre></div>",
        "id": 183794418,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706082
    },
    {
        "content": "<p>Which is basically the same wall-clock (slightly better) and noticeably better user time.</p>",
        "id": 183794437,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706098
    },
    {
        "content": "<p>The kernel patch did slightly improve the results with <code>RUSTFLAGS=-Zthreads=72</code>, but that's still much worse than 4:</p>\n<div class=\"codehilite\"><pre><span></span>real    1m27.163s\nuser    40m44.138s\nsys 3m4.341s\n</pre></div>",
        "id": 183794724,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706309
    },
    {
        "content": "<p>hm, I think I was remembering when we were benchmarking first 3 seconds of a build which might explain my expectation of lower system times</p>",
        "id": 183794744,
        "sender_full_name": "simulacrum",
        "timestamp": 1576706325
    },
    {
        "content": "<p>could you try on both with <code>timeout 3s cargo ...</code>?</p>",
        "id": 183794777,
        "sender_full_name": "simulacrum",
        "timestamp": 1576706354
    },
    {
        "content": "<p>For comparison, <code>-Zthreads=72</code> <em>without</em> this kernel patch gave 2m1s real-time and 50m user.</p>",
        "id": 183794785,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706362
    },
    {
        "content": "<p>So the kernel patch to fix thundering herd wakeups for pipes is a <em>huge</em> relative improvement for -Zthreads=72, but there's still a problem there.</p>",
        "id": 183794821,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706390
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> This system takes a while to reboot, so I'd like to get all the results I can with the kernel patch before I switch back to without.</p>",
        "id": 183794884,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706420
    },
    {
        "content": "<p>ah, okay, wasn't sure if you were working on a separate machine or something</p>",
        "id": 183794904,
        "sender_full_name": "simulacrum",
        "timestamp": 1576706440
    },
    {
        "content": "<p>I'm doing these tests on a separate system than the one I'm chatting with, but that separate system takes minutes to reboot.</p>",
        "id": 183794944,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706466
    },
    {
        "content": "<p>(server BIOSes, sigh)</p>",
        "id": 183794966,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706475
    },
    {
        "content": "<p>fwiw, one thing that might be helpful is to get some loose syscall counts/timing</p>",
        "id": 183794993,
        "sender_full_name": "simulacrum",
        "timestamp": 1576706489
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> But purely subjectively, without the kernel patch, cargo spent a few seconds showing 0 crates compiled, while <em>with</em> the patch, the \"slow startup\" problem doesn't seem to be true anymore.</p>",
        "id": 183795012,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706502
    },
    {
        "content": "<p>that's the primary behavior we're expecting to fix, so that's good to hear</p>",
        "id": 183795080,
        "sender_full_name": "simulacrum",
        "timestamp": 1576706529
    },
    {
        "content": "<p>I can easily get some <code>-Ztimings</code> data from this patched kernel.</p>",
        "id": 183795402,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706798
    },
    {
        "content": "<p>Also, I'm trying some experiments with different <code>-Zthreads</code> values.</p>",
        "id": 183795420,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706811
    },
    {
        "content": "<p>For instance, <code>-Zthreads=16</code> gives:</p>\n<div class=\"codehilite\"><pre><span></span>real    1m7.279s\nuser    18m13.828s\nsys 0m38.424s\n</pre></div>",
        "id": 183795444,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706831
    },
    {
        "content": "<p>That's a bit more user time, but a few seconds wall-clock improvement over 4 threads, and the improvement definitely isn't noise.</p>",
        "id": 183795464,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706870
    },
    {
        "content": "<p>Subjectively, this feels like there's some critical non-linear scaling issue that dominates with high numbers of threads but not with low numbers of threads.</p>",
        "id": 183795540,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576706921
    },
    {
        "content": "<p>I tend to agree -- I think we've not yet tracked down what that is. It might be that we just don't have enough data or so, or maybe we're not yet parallel enough (i.e., the compiler itself is not sufficiently able to utilize that parallelism)</p>",
        "id": 183795719,
        "sender_full_name": "simulacrum",
        "timestamp": 1576707115
    },
    {
        "content": "<p>/me would be happy to help get the data you need.</p>",
        "id": 183796705,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576707941
    },
    {
        "content": "<p>Do you have experience or time to dig in? One of the problems we've had is that none of us really know how to figure out what the problem is (or what the tools are really either)</p>",
        "id": 183796824,
        "sender_full_name": "simulacrum",
        "timestamp": 1576708000
    },
    {
        "content": "<p>This is <em>entirely</em> subjective, but having worked on parallel programming for many years, this doesn't feel like \"we don't have enough parallelism opportunities in the compiler\". This feels like either \"something is scaling quadratically or worse\" or \"something is blocking/spinning/waking-up unnecessarily\".</p>",
        "id": 183796838,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708018
    },
    {
        "content": "<p>I do have the experience, and some of the time.</p>",
        "id": 183796847,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708027
    },
    {
        "content": "<p>(Parallel programming was my dissertation topic. ;) )</p>",
        "id": 183796869,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708048
    },
    {
        "content": "<p>We'd be happy to receive help, and if you can even write up some quick \"things to look at\" or so that would be amazing</p>",
        "id": 183796891,
        "sender_full_name": "simulacrum",
        "timestamp": 1576708062
    },
    {
        "content": "<p>I can try and help out with how to get builds and such with the parallelism enabled</p>",
        "id": 183796948,
        "sender_full_name": "simulacrum",
        "timestamp": 1576708086
    },
    {
        "content": "<p>It depends on the nature of the problem you're debugging. But to a first approximation, if the problem is \"too much user time\", perf is great for this.</p>",
        "id": 183797158,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708242
    },
    {
        "content": "<p>(\"too much blocking\" is harder to debug, but possible.)</p>",
        "id": 183797171,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708257
    },
    {
        "content": "<p>I think we've not had any major success with basic use of perf -- i.e., parallel compiler does not differ from non-parallel</p>",
        "id": 183797197,
        "sender_full_name": "simulacrum",
        "timestamp": 1576708276
    },
    {
        "content": "<p>(if you mean just <code>perf record</code> and friends)</p>",
        "id": 183797205,
        "sender_full_name": "simulacrum",
        "timestamp": 1576708288
    },
    {
        "content": "<p>That'd be really surprising. If user time is going from (in my case) 16 minutes to 40-50 minutes, perf really should point to <em>something</em> there...</p>",
        "id": 183797274,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708326
    },
    {
        "content": "<p>A thought crosses my mind...</p>",
        "id": 183797286,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708337
    },
    {
        "content": "<p>It's possible we've just not spent enough time on high enough core machines</p>",
        "id": 183797308,
        "sender_full_name": "simulacrum",
        "timestamp": 1576708362
    },
    {
        "content": "<p>Sometimes, these scaling issues show up <em>much</em> better on bigger systems, precisely because there's more contention. They also can show up more on multi-socket systems, because unnecessary communication is slower between sockets.</p>",
        "id": 183797318,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708372
    },
    {
        "content": "<p>O(n^2) gets much more noticeable at 72 than 16. :)</p>",
        "id": 183797344,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708391
    },
    {
        "content": "<p>(I also don't recall specifically looking at this in recent time, though I do recall doing something like this a few months ago, so things may have also changed since then)</p>",
        "id": 183797347,
        "sender_full_name": "simulacrum",
        "timestamp": 1576708394
    },
    {
        "content": "<p>So, in addition to me personally working on this: is there someone on the parallel rustc team who has room to host an 88-way box somewhere? I'll mail them one. :)</p>",
        "id": 183797368,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708422
    },
    {
        "content": "<p>It's rackmount hardware, and produces enough noise that you don't want it under or atop your desk, but if someone has somewhere to rack it and make it available for everyone on the parallel rustc team to use it...</p>",
        "id": 183797449,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708488
    },
    {
        "content": "<p>I am uncertain. I suspect the answer might be no -- and we might have more luck with just one-off running something on EC2, though I'd need to take a look at pricing there.</p>",
        "id": 183797473,
        "sender_full_name": "simulacrum",
        "timestamp": 1576708516
    },
    {
        "content": "<p>Pricing there is about $5/hour for a comparable box.</p>",
        "id": 183797501,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708554
    },
    {
        "content": "<p>That's not counting storage and bandwidth.</p>",
        "id": 183797584,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708610
    },
    {
        "content": "<p>(We do have some budget from AWS so we might be able to afford it, I'm just not sure off the top of my head :)</p>",
        "id": 183797612,
        "sender_full_name": "simulacrum",
        "timestamp": 1576708646
    },
    {
        "content": "<p>Just confirmed, <code>c5.metal</code> is what you want, which is $4.08/hour.</p>",
        "id": 183797783,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708800
    },
    {
        "content": "<p>That's a 96-way parallel system.</p>",
        "id": 183797834,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708817
    },
    {
        "content": "<p>(That said, I would love to give this hardware to Rust, where it'll be free for anyone to do this kind of testing on an ongoing basis.)</p>",
        "id": 183797908,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576708908
    },
    {
        "content": "<p>It's one of my desires to find a way for the infra team to be able to accept this sort of hardware donation :)</p>",
        "id": 183798001,
        "sender_full_name": "simulacrum",
        "timestamp": 1576708951
    },
    {
        "content": "<p>It doesn't <em>have</em> to be the infra team. I'd accept \"someone in the project can arrange to rack it, one-off\". So if anyone has an office near a lab...</p>",
        "id": 183798064,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576709023
    },
    {
        "content": "<p>(It can always become infra later when there's a path for that.)</p>",
        "id": 183798083,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576709037
    },
    {
        "content": "<p>sure, yeah, I agree with that</p>",
        "id": 183798128,
        "sender_full_name": "simulacrum",
        "timestamp": 1576709046
    },
    {
        "content": "<p>Are there debug symbols available that would allow directly running perf on the nightly-2019-12-18 build, or am I going to need to build from source?</p>",
        "id": 183799092,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576709880
    },
    {
        "content": "<p>you're going to need to build from source unfortunately</p>",
        "id": 183799300,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710057
    },
    {
        "content": "<p>you can either directly checkout the SHA of that nightly or toggle <code>parallel-compiler = true</code> in config.toml (not sure how much experience you have with rustc compiler dev)</p>",
        "id": 183799334,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710091
    },
    {
        "content": "<p>Fairly little. Mind pointing me to the \"build rustc and cargo from source and <code>rustup link</code> them 101\"? :)</p>",
        "id": 183799403,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710132
    },
    {
        "content": "<p>Even without source, though, I'm seeing some obvious perf issues:</p>\n<div class=\"codehilite\"><pre><span></span>  13.07%  rustc            librustc_driver-0d78d9a30be443c5.so          [.] std::thread::local::LocalKey&lt;T&gt;::try_with\n  10.95%  rustc            librustc_driver-0d78d9a30be443c5.so          [.] crossbeam_epoch::internal::Global::try_advance\n   6.93%  rustc            [unknown]                                    [k] 0xffffffff91a00163\n   5.86%  rustc            librustc_driver-0d78d9a30be443c5.so          [.] crossbeam_deque::Stealer&lt;T&gt;::steal\n   4.14%  rustc            librustc_driver-0d78d9a30be443c5.so          [.] &lt;core::iter::adapters::chain::Chain&lt;A,B&gt; as core::iter::traits::iterator::Iterator&gt;::try_fold\n</pre></div>",
        "id": 183799468,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710198
    },
    {
        "content": "<p>I'm going to grab a similar profile <em>without</em> <code>-Zthreads=72</code> and see what things stand out as differences.</p>",
        "id": 183799489,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710236
    },
    {
        "content": "<p>you shouldn't need to build cargo (at least yet) -- it doesn't change for this</p>",
        "id": 183799579,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710308
    },
    {
        "content": "<p>Good to know.</p>",
        "id": 183799588,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710328
    },
    {
        "content": "<p>otherwise the doc to look at is <a href=\"https://rust-lang.github.io/rustc-guide/building/how-to-build-and-run.html\" target=\"_blank\" title=\"https://rust-lang.github.io/rustc-guide/building/how-to-build-and-run.html\">https://rust-lang.github.io/rustc-guide/building/how-to-build-and-run.html</a></p>",
        "id": 183799591,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710334
    },
    {
        "content": "<p>which essentially boils down to cloning the repo, and then this sequence of commands or so:</p>\n<div class=\"codehilite\"><pre><span></span>cp config.toml.example config.toml\n$EDITOR config.toml\n# edit parallel-compiler to be &quot;true&quot; and uncomment it\n./x.py build --stage 1 src/libtest\nrustup toolchain link stage1 build/&lt;host-triple&gt;/stage1\n# cd elsewhere\ncargo +stage1 build ...\n</pre></div>",
        "id": 183799676,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710412
    },
    {
        "content": "<p>and you'll only need to run the link step once</p>",
        "id": 183799686,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710422
    },
    {
        "content": "<p>ah, you might actually need to enable debug symbols as well</p>",
        "id": 183799698,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710441
    },
    {
        "content": "<p>as <a href=\"https://rust-lang.github.io/rustc-guide/building/how-to-build-and-run.html#create-a-configtoml\" target=\"_blank\" title=\"https://rust-lang.github.io/rustc-guide/building/how-to-build-and-run.html#create-a-configtoml\">https://rust-lang.github.io/rustc-guide/building/how-to-build-and-run.html#create-a-configtoml</a> notes</p>",
        "id": 183799709,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710465
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> ^</p>",
        "id": 183799710,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710469
    },
    {
        "content": "<p>Thanks, I'll give that a try.</p>",
        "id": 183799713,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710477
    },
    {
        "content": "<p>Also, yeah, just a simple <code>perf record</code> is showing some massive scaling issues.</p>",
        "id": 183799766,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710509
    },
    {
        "content": "<p>With the default threads=4, a parallel rustc has free and malloc at the top of the profile.</p>",
        "id": 183799777,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710526
    },
    {
        "content": "<p>And various parts of <code>libLLVM-9-rust-1.41.0-nightly.so</code>.</p>",
        "id": 183799785,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710538
    },
    {
        "content": "<p>With threads=72, all of the top hits look like scaling failures.</p>",
        "id": 183799811,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710580
    },
    {
        "content": "<p>aha, yeah, so looks like we probably just weren't benchmarking with enough threds</p>",
        "id": 183799908,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710659
    },
    {
        "content": "<p>the first thing I'd look at is probably bumping the shard bits here: <a href=\"https://github.com/rust-lang/rust/blob/master/src/librustc_data_structures/sharded.rs#L13-L17\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/blob/master/src/librustc_data_structures/sharded.rs#L13-L17\">https://github.com/rust-lang/rust/blob/master/src/librustc_data_structures/sharded.rs#L13-L17</a></p>",
        "id": 183799944,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710690
    },
    {
        "content": "<p>(should be a matter of bumping that constant up)</p>",
        "id": 183799952,
        "sender_full_name": "simulacrum",
        "timestamp": 1576710698
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> another issue is that there's a branch of rayon which in theory greatly improves threads going to sleep</p>",
        "id": 183800095,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576710820
    },
    {
        "content": "<p>that may help a bit here as well</p>",
        "id": 183800097,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576710824
    },
    {
        "content": "<p>this is great to have this system to test on though</p>",
        "id": 183800102,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576710828
    },
    {
        "content": "<p>At the moment, it <em>looks</em> like the bottleneck may be in crossbeam.</p>",
        "id": 183800170,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710864
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> another possibility perhaps, if you can find one crate that isn't getting nearly the speedup you'd think, I was taking a look at <code>-Z self-profile</code> graphs and saw very little work actually being stolen</p>",
        "id": 183800204,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576710905
    },
    {
        "content": "<p>so it may just be that rustc isn't great at saturating cores right now</p>",
        "id": 183800217,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576710918
    },
    {
        "content": "<p>I'm less worried about that, and more worried about the massively increased user time (total CPU time spent) when increasing the number of threads.</p>",
        "id": 183800233,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576710948
    },
    {
        "content": "<p>Interestingly, it looks like the pile of CPU spent on TLS in <code>std::thread::local::LocalKey&lt;T&gt;::try_with</code> is coming from <code>crossbeam_deque::Stealer&lt;T&gt;::steal</code>.</p>",
        "id": 183800322,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711020
    },
    {
        "content": "<p>that sounds like plausibly rayon not quite doing a good job</p>",
        "id": 183800355,
        "sender_full_name": "simulacrum",
        "timestamp": 1576711057
    },
    {
        "content": "<p>Would you recommend testing the same commit that nightly-2019-12-18 used, or testing latest master and just enabling parallelism?</p>",
        "id": 183800489,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711181
    },
    {
        "content": "<p>shouldn't matter in practice</p>",
        "id": 183800556,
        "sender_full_name": "simulacrum",
        "timestamp": 1576711205
    },
    {
        "content": "<p>might be a bit harder to test latest master</p>",
        "id": 183800562,
        "sender_full_name": "simulacrum",
        "timestamp": 1576711212
    },
    {
        "content": "<p>but if you're setting threads up already then you should be fine</p>",
        "id": 183800570,
        "sender_full_name": "simulacrum",
        "timestamp": 1576711220
    },
    {
        "content": "<p>(i.e., current master does not default to 4 threads but rather 1)</p>",
        "id": 183800584,
        "sender_full_name": "simulacrum",
        "timestamp": 1576711229
    },
    {
        "content": "<p>I'm fine with manually specifying <code>RUSTFLAGS=-Zthreads=4</code>. :)</p>",
        "id": 183800689,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711322
    },
    {
        "content": "<p>What about the pile of other changes that got made and reverted?</p>",
        "id": 183800719,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711330
    },
    {
        "content": "<p>I saw a bunch of type-related changes, for instance (usize vs u64...).</p>",
        "id": 183800737,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711342
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> What's the rayon branch that improves threads going to sleep?</p>",
        "id": 183800812,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711425
    },
    {
        "content": "<p>shouldn't matter in practice</p>",
        "id": 183800819,
        "sender_full_name": "simulacrum",
        "timestamp": 1576711430
    },
    {
        "content": "<p>Alright.</p>",
        "id": 183800885,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711462
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> the rayon branch should help with user time because the current bug for rayon is that threads take way too long to go to sleep, which would burn a lot of user time</p>",
        "id": 183800894,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576711472
    },
    {
        "content": "<p>That makes sense; that's absolutely worth testing.</p>",
        "id": 183800925,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711498
    },
    {
        "content": "<p>the branch was last tested in <a href=\"https://github.com/rust-lang/rust/pull/66608\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/pull/66608\">https://github.com/rust-lang/rust/pull/66608</a></p>",
        "id": 183800940,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576711506
    },
    {
        "content": "<p>I'm not sure if it's easily switchable-too</p>",
        "id": 183800953,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576711514
    },
    {
        "content": "<p>we're mostly waiting on rayon to merge the changes itself :)</p>",
        "id": 183800958,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576711522
    },
    {
        "content": "<p>Are the changes solid enough that they're expected to be merged, or do they need further hammering-on?</p>",
        "id": 183801040,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711584
    },
    {
        "content": "<p>It doesn't look too hard to switch to.</p>",
        "id": 183801045,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711592
    },
    {
        "content": "<p>I'd expect some of <a href=\"https://tokio.rs/blog/2019-10-scheduler/\" target=\"_blank\" title=\"https://tokio.rs/blog/2019-10-scheduler/\">https://tokio.rs/blog/2019-10-scheduler/</a> could apply well to Rayon too. I don't think anyone has optimized Rayon for 72 threads.</p>",
        "id": 183801217,
        "sender_full_name": "Zoxc",
        "timestamp": 1576711765
    },
    {
        "content": "<p>That does look interesting, thank you for the pointer.</p>",
        "id": 183801438,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576711979
    },
    {
        "content": "<p>Having a backoff from stealing stuff when idling would probably help a bit too. Rayon just tries to steal stuff in a loop currently</p>",
        "id": 183801607,
        "sender_full_name": "Zoxc",
        "timestamp": 1576712129
    },
    {
        "content": "<p>Which makes sense if you expect to saturate the system, but not if you expect to cooperate with other parallel work.</p>",
        "id": 183801672,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576712175
    },
    {
        "content": "<p>Successfully built rustc master (<code>Build completed successfully in 0:12:13</code>, most of which was LLVM), trying that out to see if I get comparable results.</p>",
        "id": 183801720,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576712242
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> I don't know the status of the rayon changes myself, <span class=\"user-mention\" data-user-id=\"116009\">@nikomatsakis</span> would be able to speak more to that (the status of the rayon branch and how close it is to landing upstream)</p>",
        "id": 183801835,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1576712354
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> I tried building a parallel rustc based on your instructions, but even with <code>RUSTFLAGS=-Zthreads=72</code> (or 4) I don't seem to get any parallelism.</p>",
        "id": 183803267,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576713676
    },
    {
        "content": "<p>/me will try again later.</p>",
        "id": 183803276,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576713684
    },
    {
        "content": "<p>hm okay</p>",
        "id": 183803466,
        "sender_full_name": "simulacrum",
        "timestamp": 1576713725
    },
    {
        "content": "<p>you might need to <code>x.py clean</code> to get rid of artifacts, which might not be getting cleaned up for whatever reason after toggling on parallel-compiler = true in config.toml</p>",
        "id": 183803498,
        "sender_full_name": "simulacrum",
        "timestamp": 1576713760
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 183821986,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576739858
    },
    {
        "content": "<p>That seems to have helped, I think.</p>",
        "id": 183822238,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1576740249
    }
]