[
    {
        "content": "<p>I would like to point out a pretty concerning in my opinion trend â€“ people are increasingly using perf runs as a stand-in to evaluate changes to the library, where a microbenchmark of the function in question is much more appropriate.</p>",
        "id": 246105729,
        "sender_full_name": "nagisa",
        "timestamp": 1626360843
    },
    {
        "content": "<p>By \"the library\" here, do you mean std?</p>",
        "id": 246105912,
        "sender_full_name": "rylev",
        "timestamp": 1626360924
    },
    {
        "content": "<p>Sure.</p>",
        "id": 246105936,
        "sender_full_name": "nagisa",
        "timestamp": 1626360936
    },
    {
        "content": "<p>The idea in running perf runs is \"hoping\" that the compiler uses the changed APIs in any significant way.</p>",
        "id": 246106043,
        "sender_full_name": "nagisa",
        "timestamp": 1626360972
    },
    {
        "content": "<p>I'll agree that perf.rlo runs are bad way to gauge changes to std when it comes to perf.</p>",
        "id": 246106078,
        "sender_full_name": "rylev",
        "timestamp": 1626360993
    },
    {
        "content": "<p>I don't have insight into whether this is a growing trend though</p>",
        "id": 246106097,
        "sender_full_name": "rylev",
        "timestamp": 1626361005
    },
    {
        "content": "<p>I'm happy to start collecting these instances. I did comment to that effect a couple times on separate PRs.</p>",
        "id": 246106169,
        "sender_full_name": "nagisa",
        "timestamp": 1626361046
    },
    {
        "content": "<p>I think those that work on perf.rlo see it as a tool for measuring compiler performance and changes to the _compiler_</p>",
        "id": 246106198,
        "sender_full_name": "rylev",
        "timestamp": 1626361065
    },
    {
        "content": "<p>I wonder what it would take to support <code>x.py bench</code>?</p>",
        "id": 246106202,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626361068
    },
    {
        "content": "<p>And <code>#[bench]</code> functions within the compiler.</p>",
        "id": 246106290,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626361096
    },
    {
        "content": "<p>That would make microbenchmarks much more appealing.</p>",
        "id": 246106304,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626361109
    },
    {
        "content": "<p>Perhaps we can note somewhere that perf.rlo runs are largely meant for compiler performance checking and that while it cannot hurt to run it for std changes, a lack of performance change in the compiler does not mean the std change did not introduce performance regressions to std itself</p>",
        "id": 246106325,
        "sender_full_name": "rylev",
        "timestamp": 1626361123
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/pull/83403\">https://github.com/rust-lang/rust/pull/83403</a> is an example of this I feel, and <span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> mentions another PR in relation to <code>Vec</code> which I imagine was evaluated in much the similar way.</p>",
        "id": 246106354,
        "sender_full_name": "nagisa",
        "timestamp": 1626361138
    },
    {
        "content": "<p>I wonder too if we could auto detect this case from the bot, and issue a warning</p>",
        "id": 246106460,
        "sender_full_name": "rylev",
        "timestamp": 1626361188
    },
    {
        "content": "<p>Changes to existing vec code paths do have significant impact on the compiler though.</p>",
        "id": 246106821,
        "sender_full_name": "The 8472",
        "timestamp": 1626361254
    },
    {
        "content": "<p>Yeah, I specifically did a perf run for that PR because I thought it might have an outsized impact on the compiler itself.</p>",
        "id": 246106942,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626361279
    },
    {
        "content": "<p>e.g. <a href=\"https://github.com/rust-lang/rust/issues/86938\">#86938</a></p>",
        "id": 246106960,
        "sender_full_name": "The 8472",
        "timestamp": 1626361284
    },
    {
        "content": "<p>I think the point still stands even if this particular case isn't too egregious</p>",
        "id": 246107089,
        "sender_full_name": "rylev",
        "timestamp": 1626361318
    },
    {
        "content": "<p><em>nod</em>, I've absolutely seen people use a perf run as a substitute for a better benchmark, in cases where there's no particular reason to believe that the compiler is calling the function in question at all.</p>",
        "id": 246107492,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626361474
    },
    {
        "content": "<p>I think it'd be perfectly valid to detect a perf invocation on a PR that doesn't change anything outside of the standard library, and print a warning sentence.</p>",
        "id": 246107565,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626361511
    },
    {
        "content": "<blockquote>\n<p>I wonder what it would take to support x.py bench?</p>\n</blockquote>\n<p><code>#[bench]</code> only measures wall time and on top has a self-tuning number of rounds. I think it would require changes to the test harness to run each benchmark with perf counters.</p>",
        "id": 246107591,
        "sender_full_name": "The 8472",
        "timestamp": 1626361522
    },
    {
        "content": "<p>In the cargo meeting the other day, we talked about the idea of replacing the existing bench framework with something like criterion or similar, by way of striving towards stability.</p>",
        "id": 246107709,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626361564
    },
    {
        "content": "<p>also, getting meaningful results locally is hard. I have to disable clock boosts, pin the benchmark to single core, set CGU=1 and disable incremental compilation. which of course makes things extremely slow. so I actually need to do two passes, once to compile the std changes and another one to run it pinned.</p>",
        "id": 246108240,
        "sender_full_name": "The 8472",
        "timestamp": 1626361798
    },
    {
        "content": "<p>since it's measuring wall-time</p>",
        "id": 246108334,
        "sender_full_name": "The 8472",
        "timestamp": 1626361830
    },
    {
        "content": "<p>And even then I sometimes ask myself whether I'm just fiddling with the knobs until I get nice-looking results. Which means I end up comparing things with perf record/report instead.</p>",
        "id": 246108479,
        "sender_full_name": "The 8472",
        "timestamp": 1626361915
    },
    {
        "content": "<p>comparing <code>perf report</code> output also requires CGUs=1, otherwise minor inlining changes will make results uncomparable.</p>",
        "id": 246108681,
        "sender_full_name": "The 8472",
        "timestamp": 1626362002
    },
    {
        "content": "<p>The microbenchmarks work fine to identify a 2x or 10x improvement, but for something like 20% improvement you have to jump through hoops to make sure it's not noise or the compiler just rearranging things randomly.</p>\n<p>Yesterday I had a -44% change in iterator code that disappeared once I flipped all the switches to get more reliable benchmarks.</p>",
        "id": 246109255,
        "sender_full_name": "The 8472",
        "timestamp": 1626362261
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span>  <code>x.py bench --stage 0 library/std</code> already works fine</p>",
        "id": 246110743,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626362878
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"232545\">@Joshua Nelson</span> ! Well then. That's awesome.</p>",
        "id": 246110843,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626362908
    },
    {
        "content": "<p>Oh, I read that as including its results in perf rlo</p>",
        "id": 246110845,
        "sender_full_name": "The 8472",
        "timestamp": 1626362911
    },
    {
        "content": "<p>What would be <em>really</em> cool is to hook rustc-perf up to x.py bench somehow so you don't have to fiddle with it yourself</p>",
        "id": 246110848,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626362911
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"232545\">@Joshua Nelson</span> yes, that's what I was about to ask for.</p>",
        "id": 246110867,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626362924
    },
    {
        "content": "<p><code>@bors try @rust-timer bench</code></p>",
        "id": 246110909,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626362944
    },
    {
        "content": "<p>I don't understand how that's related to my suggestion</p>",
        "id": 246110961,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626362964
    },
    {
        "content": "<p>(nor how it differs from <code>queue</code>)</p>",
        "id": 246110981,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626362975
    },
    {
        "content": "<p>Because it runs the microbenchmarks instead of the compiler perf suite?</p>",
        "id": 246111153,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626363034
    },
    {
        "content": "<p>I think I interpreted \"hook rustc-perf up\" as \"hook the bot that runs rustc-perf up\".</p>",
        "id": 246111248,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626363075
    },
    {
        "content": "<p>Oh I see you want the reverse, you want to run x.py bench on PRs</p>",
        "id": 246111252,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363076
    },
    {
        "content": "<p>I was suggesting that x.py locally runs rustc-perf</p>",
        "id": 246111279,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363087
    },
    {
        "content": "<p>Ah, I understand now.</p>",
        "id": 246111293,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626363093
    },
    {
        "content": "<p>Yes, I would love to have a simpler way to run the perf suite locally, too.</p>",
        "id": 246111361,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626363123
    },
    {
        "content": "<p>I guess the advantage of your suggestion is that you get a more reproducible environment? But it seems like if we can do that in CI we should be able to do that locally too, at least if we prompt the user to close everything else that's open</p>",
        "id": 246111471,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363158
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/Using.20perf.20as.20a.20replacement.20for.20library.20microbenchmarks/near/246111361\">said</a>:</p>\n<blockquote>\n<p>Yes, I would love to have a simpler way to run the perf suite locally, too.</p>\n</blockquote>\n<p>I have this on my list to work on. It is actually pretty easy now.</p>",
        "id": 246111484,
        "sender_full_name": "rylev",
        "timestamp": 1626363165
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> hmm I'm surprised to hear that, I'll have to try it out this weekend</p>",
        "id": 246111571,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363198
    },
    {
        "content": "<p>I would also love to see the microbenchmark suite become a default part of the perf suite, to help detect cases where compilation becomes faster but the compiled code becomes slower, or vice versa. That's separate from wanting a way to invoke just the microbenchmark suite on a PR that we don't expect to affect compile time at all.</p>",
        "id": 246111573,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1626363198
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232545\">Joshua Nelson</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/Using.20perf.20as.20a.20replacement.20for.20library.20microbenchmarks/near/246111571\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"224872\">rylev</span> hmm I'm surprised to hear that, I'll have to try it out this weekend</p>\n</blockquote>\n<p>Please file bugs. self-profile doesn't work locally unfortunately</p>",
        "id": 246111655,
        "sender_full_name": "rylev",
        "timestamp": 1626363235
    },
    {
        "content": "<blockquote>\n<p>But it seems like if we can do that in CI we should be able to do that locally too, at least if we prompt the user to close everything else that's open</p>\n</blockquote>\n<p>See my previous comments about running #[bench] correctly being hard.</p>",
        "id": 246111706,
        "sender_full_name": "The 8472",
        "timestamp": 1626363244
    },
    {
        "content": "<p>And of course any of this benchmark stuff doesn't exist yet</p>",
        "id": 246111715,
        "sender_full_name": "rylev",
        "timestamp": 1626363245
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"330154\">@The 8472</span> right: my point is that x.py could do all those things automatically</p>",
        "id": 246111840,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363295
    },
    {
        "content": "<p>(it can't fix being slow though)</p>",
        "id": 246111883,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363314
    },
    {
        "content": "<p>it can't disable turbo since that requires root privs. at least it would need a small suid helper for that</p>",
        "id": 246111891,
        "sender_full_name": "The 8472",
        "timestamp": 1626363318
    },
    {
        "content": "<p>Sure, we can say \"run this command as sudo\" and exit with an error or something</p>",
        "id": 246111935,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363339
    },
    {
        "content": "<p>I guess this would be quite difficult to get working on Windows</p>",
        "id": 246112065,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363382
    },
    {
        "content": "<p>And non-trivial on Mac</p>",
        "id": 246112079,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363387
    },
    {
        "content": "<p>sudo isn't the right choice, it can leave root-owned files behind. a helper binary to only do the minimal privileged stuff necessary would be right.</p>\n<p>that or getting #[bench] to use perf counters instead of wall time</p>",
        "id": 246112209,
        "sender_full_name": "The 8472",
        "timestamp": 1626363442
    },
    {
        "content": "<p>+1 using perf counters seems like the right approach</p>",
        "id": 246112237,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363461
    },
    {
        "content": "<p>That's what rustc-perf uses</p>",
        "id": 246112242,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363465
    },
    {
        "content": "<p>Oh wait I think we're talking about different things</p>",
        "id": 246112266,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363480
    },
    {
        "content": "<p>Perf counters shouldn't depend too much on pinning a separate core and all that stuff</p>",
        "id": 246112340,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363499
    },
    {
        "content": "<p>I'm talking about benchmarking the compiler locally, not libstd</p>",
        "id": 246112370,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363510
    },
    {
        "content": "<p>running rustc-perf locally already works though. well, not all of it, but some of the benchmarks.</p>",
        "id": 246112439,
        "sender_full_name": "The 8472",
        "timestamp": 1626363541
    },
    {
        "content": "<p>Sure, but it's annoying. I'm just suggesting that we have x.py bench figure out how to run things for you.</p>",
        "id": 246112503,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363569
    },
    {
        "content": "<p>ah</p>",
        "id": 246112529,
        "sender_full_name": "The 8472",
        "timestamp": 1626363577
    },
    {
        "content": "<p>yes, totally different thing</p>",
        "id": 246112539,
        "sender_full_name": "The 8472",
        "timestamp": 1626363584
    },
    {
        "content": "<p>That way the UI is unified between rustc and libstd</p>",
        "id": 246112541,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1626363585
    },
    {
        "content": "<p>that's a different topic though, this one is about std microbenchmarks</p>",
        "id": 246112801,
        "sender_full_name": "The 8472",
        "timestamp": 1626363687
    }
]