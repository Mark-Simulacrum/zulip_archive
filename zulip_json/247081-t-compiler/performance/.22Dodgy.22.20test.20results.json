[
    {
        "content": "<p>We currently indicate a \"dodgy\" test result by putting a \"?\" next to the percent change. A \"dodgy\" test result is generally meant to indicate that these test results are less reliable than other test results because they historically have much more variance. A couple of thoughts:</p>\n<ul>\n<li>Does anyone actually pay attention to the \"?\" - I know I have stopped really giving much thought to it. It's very easy to ignore.</li>\n<li>Dodginess is defined as having a significance threshold over 0.2%. Is this a good definition? (note: significance threshold itself is defined as the q3 fence over the past 100 results of the benchmark - i.e., q3 + (q3 - q1) * IQR_MULTIPLIER where IQR_MULTIPLIER is currently 3) </li>\n<li>Currently 54% of test cases are marked as dodgy.</li>\n</ul>\n<p>Some questions then;</p>\n<ul>\n<li>Should we make \"dodginess\" more apparent?</li>\n<li>Should we change the definition of dodginess? If so, how? A high significance threshold indicates high variance but perhaps we should be more direct about measuring variance. Also, our data is just naturally a bit variable. Perhaps we need to be more lenient with what we consider too variable.</li>\n</ul>",
        "id": 277390748,
        "sender_full_name": "rylev",
        "timestamp": 1648805292
    },
    {
        "content": "<p>I think we should drop dodginess entirely from the comparison UI and probably have a separate view on the graphs page (or perhaps detailed-query pages can show this / another column on comparison) that actually just gives the user the %chg boundaries, potentially as a small box-and-whisker plot or so</p>",
        "id": 277402710,
        "sender_full_name": "simulacrum",
        "timestamp": 1648812583
    },
    {
        "content": "<p>What does \"historically\" mean? I've been running the system locally and I get much higher significance factors and no ?s. If the ? requires some extra state, that would make sense.</p>",
        "id": 277464104,
        "sender_full_name": "Ben Kimock (Saethlin)",
        "timestamp": 1648839414
    },
    {
        "content": "<p>I have learned to ignore the \"?\" because so many results have it. I've never quite understood the difference between \"dodgy\" and \"not significant\". It feels like there should be a single measure indicating if a particular measurement is worth paying attention to.</p>",
        "id": 277480390,
        "sender_full_name": "nnethercote",
        "timestamp": 1648848755
    },
    {
        "content": "<p>The ? predates the significance logic iirc</p>",
        "id": 277517150,
        "sender_full_name": "The 8472",
        "timestamp": 1648890235
    },
    {
        "content": "<p>I originally marked one or two benchmarks with ? because they had high variance. That was before all the significance and relevance (a third categorizations of results!) stuff was added, and it's been changed a lot over time.</p>",
        "id": 277523349,
        "sender_full_name": "nnethercote",
        "timestamp": 1648898259
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"120827\">Ben Kimock (Saethlin)</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/.22Dodgy.22.20test.20results/near/277464104\">said</a>:</p>\n<blockquote>\n<p>What does \"historically\" mean? I've been running the system locally and I get much higher significance factors and no ?s. If the ? requires some extra state, that would make sense.</p>\n</blockquote>\n<p>\"Historically\" means the last 50-100 changes for a given test case (benchmark, profile, scenario trio). We look at those historical changes and determine if the variance looks high. This is done by looking at the significance threshold for a test case. This significance threshold is the threshold at which we declare whether a particular test case result is <em>actually</em> a real performance change or rather just noise. The significance threshold is computed using <a href=\"https://www.statisticshowto.com/upper-and-lower-fences/#:~:text=Upper%20and%20lower%20fences%20cordon,%E2%80%93%20(1.5%20*%20IQR).\">the upper IQR fence</a> of the historical data. A dodgy test case (i.e., a test case with a \"?\") is that which has an upper IQR fence greater than 0.2% - i.e., changes must be above 0.2% to be considered real.</p>",
        "id": 277694585,
        "sender_full_name": "rylev",
        "timestamp": 1649059159
    },
    {
        "content": "<p>Terms like these should be documented in the <a href=\"https://github.com/rust-lang/rustc-perf/blob/master/docs/glossary.md\">glossary</a>. <a href=\"https://github.com/rust-lang/rustc-perf/pull/1266\">#1266</a> has some changes to the glossary.</p>",
        "id": 277694807,
        "sender_full_name": "rylev",
        "timestamp": 1649059250
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rustc-perf/pull/1270\">#1270</a> seems to also be discussing some of this terminology as well.</p>",
        "id": 277694864,
        "sender_full_name": "rylev",
        "timestamp": 1649059291
    },
    {
        "content": "<p>My hope is that the changes in <a href=\"https://github.com/rust-lang/rustc-perf/pull/1266\">#1266</a> (now merged) and in <a href=\"https://github.com/rust-lang/rustc-perf/pull/1271\">#1271</a> help clarify all of this even more.</p>",
        "id": 277705327,
        "sender_full_name": "rylev",
        "timestamp": 1649064754
    },
    {
        "content": "<p>I'm not sure if the current strategy takes into account only statistical significance of the differences but not effect size. This would alleviate the problems with short number of series, etc.<br>\nThis is a good intro <a href=\"https://towardsdatascience.com/a-definitive-guide-to-effect-size-9bc93f00db86\">https://towardsdatascience.com/a-definitive-guide-to-effect-size-9bc93f00db86</a></p>",
        "id": 277713042,
        "sender_full_name": "pachi",
        "timestamp": 1649069410
    },
    {
        "content": "<p>This is taken into account by measuring the \"magnitude\" which you will find in how we determine \"relevance\". Whether we're doing it in the optimal way or not is another question.</p>",
        "id": 277719846,
        "sender_full_name": "rylev",
        "timestamp": 1649073593
    },
    {
        "content": "<p>There are some statistics (such as Cohen's d) that help to qualify those magnitudes so they could help to filter out some uninteresting results in the same way that the inner bounds are computed (to exclude outlayers). I'm no expert and these are time series, so I'm sure there could be some statistic that is better suited for this kind of data.</p>",
        "id": 277723717,
        "sender_full_name": "pachi",
        "timestamp": 1649075639
    },
    {
        "content": "<p>I'm sorry for not being more helpful... it's just that there's something missing in that kind of analysis.</p>",
        "id": 277723826,
        "sender_full_name": "pachi",
        "timestamp": 1649075677
    },
    {
        "content": "<p>Here's a PR which removes the concept of dodginess: <a href=\"https://github.com/rust-lang/rustc-perf/pull/1275\">https://github.com/rust-lang/rustc-perf/pull/1275</a></p>",
        "id": 277843384,
        "sender_full_name": "rylev",
        "timestamp": 1649147475
    }
]