[
    {
        "content": "<p>New project group can spew stuff here</p>",
        "id": 225316827,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612540716
    },
    {
        "content": "<p>For future reference the meeting was at <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bdesign.20meeting.5D.202021-02-05.3A.20perf.2Erlo.20site/\">https://rust-lang.zulipchat.com/#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bdesign.20meeting.5D.202021-02-05.3A.20perf.2Erlo.20site/</a></p>",
        "id": 225317004,
        "sender_full_name": "bjorn3",
        "timestamp": 1612540792
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116113\">@lqd</span> <span class=\"user-mention\" data-user-id=\"352985\">@tm</span> <span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> <span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> and <span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> all had their <span aria-label=\"raised hands\" class=\"emoji emoji-1f64c\" role=\"img\" title=\"raised hands\">:raised_hands:</span> raised</p>",
        "id": 225317087,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612540807
    },
    {
        "content": "<p>I seem to recall anp doing some investigation on the statistics front back when designing lolbench, à la automatic regression detection, I can't find what I'm thinking of rn but they may remember</p>",
        "id": 225317127,
        "sender_full_name": "lqd",
        "timestamp": 1612540827
    },
    {
        "content": "<p>What I'd like for us to do (among others' ideas):</p>\n<ul>\n<li>Gather pain points that impact the target audience (rustc devs who want to track the perf impact of their changes)</li>\n<li>Understand the full flow of how these devs interact with perf.rlo and how the tool and perf team interact back (e.g., perf triage)</li>\n<li>brainstorm ideas for how to improve this flow and address the painpoints</li>\n</ul>",
        "id": 225317149,
        "sender_full_name": "rylev",
        "timestamp": 1612540831
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"133247\">bjorn3</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/lets.20keep.20talking.20about.20perf.2Erlo/near/225317004\">said</a>:</p>\n<blockquote>\n<p>For future reference the meeting was at <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bdesign.20meeting.5D.202021-02-05.3A.20perf.2Erlo.20site/\">https://rust-lang.zulipchat.com/#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bdesign.20meeting.5D.202021-02-05.3A.20perf.2Erlo.20site/</a></p>\n</blockquote>\n<p>that's right, and the agenda/notes were largely this: <a href=\"https://hackmd.io/MqzuXVWnS22pVZgnFS5Tlg#collected-pain-points\">https://hackmd.io/MqzuXVWnS22pVZgnFS5Tlg#collected-pain-points</a></p>",
        "id": 225317174,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612540841
    },
    {
        "content": "<p>right, we can start with the current list of pain points and isolate the ones that impact target audience</p>",
        "id": 225317257,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612540875
    },
    {
        "content": "<p>separate them from the ones that don't (or have minimal impact to rustc devs)</p>",
        "id": 225317286,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612540888
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> I'll make a fresh doc now, so that we don't clobber the notes from the meeting</p>",
        "id": 225317381,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612540922
    },
    {
        "content": "<p>Yes, I don't want to <em>only</em> focus on the pain points listed though because it was a small group and we more than likely did not get a holistic accounting of pain points. But it's good to make sure those that were relevant and listed are addressed.</p>",
        "id": 225317476,
        "sender_full_name": "rylev",
        "timestamp": 1612540955
    },
    {
        "content": "<p>Categorized Pain Points: <a href=\"https://hackmd.io/5VHT_I8wRDebYVYoTeNK-Q\">https://hackmd.io/5VHT_I8wRDebYVYoTeNK-Q</a></p>",
        "id": 225317488,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612540961
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224872\">rylev</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/lets.20keep.20talking.20about.20perf.2Erlo/near/225317476\">said</a>:</p>\n<blockquote>\n<p>Yes, I don't want to <em>only</em> focus on the pain points listed though because it was a small group and we more than likely did not get a holistic accounting of pain points. But it's good to make sure those that were relevant and listed are addressed.</p>\n</blockquote>\n<p>Yep, we should definitely try to flesh out the list some more</p>",
        "id": 225317536,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612540981
    },
    {
        "content": "<p>Beyond that getting a more holistic understanding of the current process and trying to decide how to improve that process would be a great thing to do</p>",
        "id": 225317571,
        "sender_full_name": "rylev",
        "timestamp": 1612540994
    },
    {
        "content": "<p>we can try to take action in the meantime based on what we've identified</p>",
        "id": 225317611,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612541000
    },
    {
        "content": "<p>There are three things that would help with some of the identified issues:<br>\nAllowing annotating spikes with text to manually point out the pr that caused it<br>\nAuto running timer for all prs in a roll-up with regressions<br>\nGiving a sense of scale for individual tests (medium swings in small tests are less critical) in the table overview</p>",
        "id": 225318069,
        "sender_full_name": "Esteban Küber",
        "timestamp": 1612541156
    },
    {
        "content": "<p>Esteban, where were your hands when I asked for them!</p>",
        "id": 225318320,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612541240
    },
    {
        "content": "<p>;)</p>",
        "id": 225318327,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612541243
    },
    {
        "content": "<p>other possible pain points: </p>\n<ul>\n<li>unclear on how representative some of the current benchmarks are</li>\n<li>coverage and scale: which parts of the compiler are \"well covered\" by the current benchmarks, and which are not; at which points do some pieces of rustc stop scaling</li>\n<li>performance budget is limited, in that we are not easily able to add everything we'd like benchmarked</li>\n<li>no data about IO, while incremental benchmarks hitting the disk more can be impactful, and hard to see</li>\n<li>(maybe just a problem for me: possible lack of statistics / insight into the benchmarked crates, what and <em>why</em> they exercize what they do)</li>\n</ul>",
        "id": 225318470,
        "sender_full_name": "lqd",
        "timestamp": 1612541298
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> just as a follow up to the point about <code>@rust-timer queue</code>. We do still include in our target audience devs who <em>don't</em> run the timer but their PR is later found to be causing perf issues by the triage report, right?</p>",
        "id": 225318751,
        "sender_full_name": "rylev",
        "timestamp": 1612541423
    },
    {
        "content": "<blockquote>\n<p>Esteban, where were your hands when I asked for them!</p>\n</blockquote>\n<p>I'm more of an ideas man 🤪</p>",
        "id": 225319035,
        "sender_full_name": "Esteban Küber",
        "timestamp": 1612541540
    },
    {
        "content": "<p>(want to help but not sure how much I can allocate)</p>",
        "id": 225319077,
        "sender_full_name": "Esteban Küber",
        "timestamp": 1612541560
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224872\">rylev</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/lets.20keep.20talking.20about.20perf.2Erlo/near/225318751\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> just as a follow up to the point about <code>@rust-timer queue</code>. We do still include in our target audience devs who <em>don't</em> run the timer but their PR is later found to be causing perf issues by the triage report, right?</p>\n</blockquote>\n<p>Yes, that is true. I probably phrased it too narrowly. Being able to evaluate individual PR's is the point, not the mechanism by which one does that.</p>",
        "id": 225319153,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612541592
    },
    {
        "content": "<p>/me is reaching the end of the day and has a stream on trait objects coming up but will continue this effort over the coming days</p>",
        "id": 225319833,
        "sender_full_name": "rylev",
        "timestamp": 1612541870
    },
    {
        "content": "<p>I too have a packed day</p>",
        "id": 225319938,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612541901
    },
    {
        "content": "<p>I've found that it's difficult to reproduce perf results locally - not just that it's noisy, but that I don't know the commands to run, and I'll end up opening PRs even though it takes longer just because it's easier</p>",
        "id": 225348216,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1612554908
    },
    {
        "content": "<p>And in general things other than <code>@rust-timer queue</code> don't seem to get much attention</p>",
        "id": 225348364,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1612554977
    },
    {
        "content": "<p>Pain point: not perf.rlo exactly, but...it's easy for regressions to sneak in, and it can take a long time before they get noticed or addressed (if ever!).</p>\n<p>A small set of smoke-test benchmarks could be useful. The set could include a few benchmarks that are low-variance and good predictors of real regressions. Ideally, this would cover memory usage regressions too, so we'd need to devise a low variance but realistic benchmark for that.</p>\n<p>Resources permitting, we could test all PRs against these before merging. At minimum, identification of a regression could be posted to the PR. Or we could block merging pending further investigation.</p>",
        "id": 225359280,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1612560108
    },
    {
        "content": "<blockquote>\n<p>no data about IO, while incremental benchmarks hitting the disk more can be impactful, and hard to see</p>\n</blockquote>\n<p>It could make sense to record <code>/proc/pressure/*</code> output at the end of each benchmark. It distinguishes cpu, io and memory (paging) bottlenecks.</p>",
        "id": 225359530,
        "sender_full_name": "The 8472",
        "timestamp": 1612560242
    },
    {
        "content": "<p>/me googles <code>/proc/pressure/</code></p>",
        "id": 225359644,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612560285
    },
    {
        "content": "<p><a href=\"https://lwn.net/Articles/759781/\">https://lwn.net/Articles/759781/</a></p>",
        "id": 225359669,
        "sender_full_name": "The 8472",
        "timestamp": 1612560299
    },
    {
        "content": "<p>I use it to monitor things at my full-time. I wonder if it's going to be an insightful metric for batch jobs like what the compiler is.</p>",
        "id": 225360214,
        "sender_full_name": "nagisa",
        "timestamp": 1612560545
    },
    {
        "content": "<p>(in general having no pressure is better than having any, but the nature of batch jobs incur it almost by definition)</p>",
        "id": 225360318,
        "sender_full_name": "nagisa",
        "timestamp": 1612560595
    },
    {
        "content": "<p>One pain point that went unmentioned is us only measuring one t1 platform.</p>",
        "id": 225360673,
        "sender_full_name": "nagisa",
        "timestamp": 1612560768
    },
    {
        "content": "<p>CPU pressure could mean you're oversubscribing cores and could maybe size your thread pools better. significant IO pressure could hint at bad IO patterns. Generally I'm seeing low numbers when compiling rust locally, so things probably are fine already. But tracking it should be fairly cheap since you only need to snapshot it at the start and end of a job.</p>",
        "id": 225360703,
        "sender_full_name": "The 8472",
        "timestamp": 1612560784
    },
    {
        "content": "<p>Yeah that's very fair. Collecting it is super cheap (though requires a recent Linux kernel)</p>",
        "id": 225360795,
        "sender_full_name": "nagisa",
        "timestamp": 1612560837
    },
    {
        "content": "<p>Hm, is there a way to get it for a cgroup/process group and not system-wide?</p>",
        "id": 225361028,
        "sender_full_name": "nagisa",
        "timestamp": 1612560956
    },
    {
        "content": "<p>yes, it's somewhere in the cgroup virtual filesystem</p>",
        "id": 225361104,
        "sender_full_name": "The 8472",
        "timestamp": 1612560987
    },
    {
        "content": "<blockquote>\n<p>In systems where control groups are in use, there will also be a set of files (cpu.pressure, memory.pressure, and io.pressure) associated with each group.</p>\n</blockquote>",
        "id": 225361186,
        "sender_full_name": "The 8472",
        "timestamp": 1612561036
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> should we try to schedule some sort of regular synchronization point? I don't know what cadence would make the most sense; I just want to keep up the same kind of energy that we observed on Friday.</p>",
        "id": 225593698,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612811891
    },
    {
        "content": "<p>Yes, I’m relatively free in the morning and early afternoon Europe time.</p>",
        "id": 225593896,
        "sender_full_name": "rylev",
        "timestamp": 1612811989
    },
    {
        "content": "<p>I made a tiny bit of progress on the doc today but was going to dive in fully tomorrow</p>",
        "id": 225593978,
        "sender_full_name": "rylev",
        "timestamp": 1612812008
    },
    {
        "content": "<p>Simple action item: Change page layout so that the metric selector is consistently on the top of the page. <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span></p>",
        "id": 225597033,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612813482
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> you and I briefly discussed \"where is the perf data stored\" last week, but I didn't dive in and get sufficient info to e.g. grab the data myself. I do see <a href=\"https://github.com/rust-lang/rustc-perf/pull/840/files\">rustc-perf PR #840</a>, which says we no longer publish the database. Is that because its too expensive to serve it up to the world? Or some other reason?</p>",
        "id": 225599291,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612814435
    },
    {
        "content": "<p>Not too expensive to <em>serve</em> but rather impractical to export. The current tooling took more than 30 minutes (albeit in a small ec2 machine), and that was a good 1/2-3/4 year ago I think</p>",
        "id": 225599398,
        "sender_full_name": "simulacrum",
        "timestamp": 1612814492
    },
    {
        "content": "<p>for things like changing page layout or really anything about the frontend, you can generally just point the API queries at <a href=\"http://perf.rust-lang.org\">perf.rust-lang.org</a> and that should work fine</p>",
        "id": 225599541,
        "sender_full_name": "simulacrum",
        "timestamp": 1612814547
    },
    {
        "content": "<p>Yeah my Q about DB storage was unrelated to the layout thing; it was more the result of me skimming over the website</p>",
        "id": 225599629,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612814588
    },
    {
        "content": "<p>A more serious suggestion I have in mind: Reduce the speed-bumps between a benchmark result (i.e. a specific datum for a given benchmark+metric) and how to perform the same experiment locally (maybe to point of providing a command line invocation that should spit out the metric for one's local host).</p>",
        "id": 225599734,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612814633
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/lets.20keep.20talking.20about.20perf.2Erlo/near/225599541\">said</a>:</p>\n<blockquote>\n<p>you can generally just point the API queries at <a href=\"http://perf.rust-lang.org\">perf.rust-lang.org</a> and that should work fine</p>\n</blockquote>\n<p>does that require having the results database ?</p>",
        "id": 225601252,
        "sender_full_name": "lqd",
        "timestamp": 1612815439
    },
    {
        "content": "<p>no, you edit the js here - <a href=\"https://github.com/rust-lang/rustc-perf/blob/master/site/static/shared.js#L1\">https://github.com/rust-lang/rustc-perf/blob/master/site/static/shared.js#L1</a> - to <a href=\"http://perf.rust-lang.org/perf\">perf.rust-lang.org/perf</a></p>",
        "id": 225601396,
        "sender_full_name": "simulacrum",
        "timestamp": 1612815489
    },
    {
        "content": "<p>and now I remember I've done this before to test the compare links ...</p>",
        "id": 225601547,
        "sender_full_name": "lqd",
        "timestamp": 1612815562
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/lets.20keep.20talking.20about.20perf.2Erlo/near/225597033\">said</a>:</p>\n<blockquote>\n<p>Simple action item: Change page layout so that the metric selector is consistently on the top of the page. <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span></p>\n</blockquote>\n<p>you got it <a href=\"https://github.com/rust-lang/rustc-perf/pull/841\">https://github.com/rust-lang/rustc-perf/pull/841</a></p>",
        "id": 225602907,
        "sender_full_name": "lqd",
        "timestamp": 1612816127
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> <span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> I think what would be most useful for me is to get some sense of what would be helpful from me to get this effort off the ground; let me know if e.g. a sync call would be good or similar. I can also write up some architecture/design docs on perf.rlo today but it would take some time and it's not <em>that</em> big so maybe not too useful.</p>",
        "id": 225609463,
        "sender_full_name": "simulacrum",
        "timestamp": 1612818984
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/lets.20keep.20talking.20about.20perf.2Erlo/near/225599734\">said</a>:</p>\n<blockquote>\n<p>A more serious suggestion I have in mind: Reduce the speed-bumps between a benchmark result (i.e. a specific datum for a given benchmark+metric) and how to perform the same experiment locally (maybe to point of providing a command line invocation that should spit out the metric for one's local host).</p>\n</blockquote>\n<p>I've had an idea that's somewhat along these lines (but might also tip-toe over the line into off-topic):</p>\n<p>You could pick some ten year old desktop spec, emulate that in all the ways that count, and then run your benchmarks in that - not everyone is going to have fast local hardware, so benchmarking natively produces results that aren't comparable, but as long as your hardware is fast enough to emulate our desired slow spec, you can produce comparable benchmark results locally. Depending how you package that up, I could imagine the portability resembling that of a docker container, because there will necessarily be some surrounding environment that you'd normally want to avoid benchmarking natively (in contrast, setting up perf locally requires system dependencies that sys crates require, etc, there's more work). I'm not necessarily seriously proposing this, because it would likely be to slower to produce benchmark results, which is undesirable, and it could be unrealistic (we might want to check that we can utilize today's hardware fully), but I think it's a fun idea.</p>",
        "id": 225610326,
        "sender_full_name": "davidtwco",
        "timestamp": 1612819385
    },
    {
        "content": "<p>I think this is basically what cachegrind/valgrind does, though that's too much a slowdown to really be viable I suspect. :)</p>",
        "id": 225610617,
        "sender_full_name": "simulacrum",
        "timestamp": 1612819522
    },
    {
        "content": "<p>Like Iai? <a href=\"https://bheisler.github.io/post/criterion-rs-0-3-4/\">https://bheisler.github.io/post/criterion-rs-0-3-4/</a></p>",
        "id": 225610807,
        "sender_full_name": "Eh2406",
        "timestamp": 1612819609
    },
    {
        "content": "<p>Huh, interesting - I had no idea. I'll need to read into that.</p>",
        "id": 225610972,
        "sender_full_name": "davidtwco",
        "timestamp": 1612819690
    },
    {
        "content": "<p>Yeah, iai is based on valgrind under the hood I think. But indeed.</p>",
        "id": 225611621,
        "sender_full_name": "simulacrum",
        "timestamp": 1612820009
    },
    {
        "content": "<p>Since instruction counts are more stable than wall time is there a reason why the bootstrap timings are not gathered via perf stat? At least the totals.</p>",
        "id": 225626094,
        "sender_full_name": "The 8472",
        "timestamp": 1612827751
    },
    {
        "content": "<p>Not really; I wanted to get something up quickly and the existing statistical instrumentation did not map cleanly to the needs of bootstrap collection - in particular, the timing info is currently extracted from built-in support in rustc (print-step-timings) rather than perf stat or similar.</p>",
        "id": 225626271,
        "sender_full_name": "simulacrum",
        "timestamp": 1612827914
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> I took some time to analyze the feedback, document the current performance process and ask questions about how we can improve it: <a href=\"https://hackmd.io/5VHT_I8wRDebYVYoTeNK-Q#Feedback-Analysis\">https://hackmd.io/5VHT_I8wRDebYVYoTeNK-Q#Feedback-Analysis</a> We should discuss this to make sure we're capturing everything, and then we can brainstorm ideas of how to improve the process.</p>",
        "id": 225701996,
        "sender_full_name": "rylev",
        "timestamp": 1612882842
    },
    {
        "content": "<p>I have many ideas on how to improve things, but I'd like to hold off on sharing them until we've got a collective understanding of the current process and its weak points</p>",
        "id": 225702337,
        "sender_full_name": "rylev",
        "timestamp": 1612882942
    },
    {
        "content": "<blockquote>\n<p>This question relates to a lot of the feedback about it being hard to interpret the metrics and to know when metrics represent a “true” regression.</p>\n</blockquote>\n<p>Knowing the variance could help. Maybe the noise-run could be augmented by running the short-running ones in a loop to get enough samples. Or manually trigger multiple iterations for ones that are suspected to be noisy. Something like that.</p>",
        "id": 225704604,
        "sender_full_name": "The 8472",
        "timestamp": 1612883865
    },
    {
        "content": "<p>The instruction counts a quite precise, I think, but a large bias is built into the compiler, due to unpredictable effects of CGU partitioning. One recent measurement: a single extra call in rustc_privacy crate changed partitioning of 10% of mono-items in the crate. This indirect impact is often larger than direct one of code changes.</p>",
        "id": 225708624,
        "sender_full_name": "tm",
        "timestamp": 1612885362
    },
    {
        "content": "<p>I've wondered a <em>lot</em> about our CGU partitioning algorithm; right now its pretty heavily tilted towards one particular goal (namely estimated load balancing). I suspect there are other partitions that would not have quite such a heavy level of unpredictability w.r.t. the performance impact of code changes.</p>",
        "id": 225711905,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612886578
    },
    {
        "content": "<p>to bring this back to the topic at hand: Other metrics we probably should be gathering is about the CGU partitioning: How many parts, what were their estimated sizes, and what were their \"actual\" sizes (whatever \"actual\" may mean; bitcode size is one obvious choice; actual object code size is another...)</p>",
        "id": 225713576,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612887224
    },
    {
        "content": "<p>I'd like to break this problem into many parts that loosely follow the chronology of performance tracking as it currently stands: </p>\n<ul>\n<li>How do we improve the metrics we're actually gathering? </li>\n<li>How do we make sure the information is actually generated and looked at? (i.e., it's possible to just never worry about performance)</li>\n<li>How do we ensure that that information is interpreted correctly?</li>\n<li>How do we ensure that information is acted upon?</li>\n<li>How do we ensure regressions are fixed (or at least explicitly decided as \"won't fix\")</li>\n</ul>",
        "id": 225714285,
        "sender_full_name": "rylev",
        "timestamp": 1612887457
    },
    {
        "content": "<p>The conversation is currently geared heavily towards the first item, but I don't want us to forget about the others.</p>",
        "id": 225714615,
        "sender_full_name": "rylev",
        "timestamp": 1612887515
    },
    {
        "content": "<p>i've been wondering if an entirely different end, e.g. one based on Jupyter, would allow us to put more power into the hands of people viewing the site.</p>",
        "id": 225714734,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612887564
    },
    {
        "content": "<p>(I guess that <em>might</em> fall under the bullet of \"how do we ensure the information is interpreted correctly\". Though I suspect it may belong in a distinct bullet: How do we give the audience more power in how they can manipulate the dataset.)</p>",
        "id": 225714916,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612887627
    },
    {
        "content": "<p>Perhaps I'm missing a \"discover the underlying cause\" which is different than answering the question \"is there actually a regression happening\". I'd prefer to try to keep things more focused on the problem we're trying to solve rather than focusing too much on the tools themselves.</p>",
        "id": 225715392,
        "sender_full_name": "rylev",
        "timestamp": 1612887815
    },
    {
        "content": "<p>It's been my personal experience that once it's been identified that a regression is happening, finding out the cause is not often the hard part (though sometimes it is, so I'm not discounting the idea).</p>",
        "id": 225715547,
        "sender_full_name": "rylev",
        "timestamp": 1612887869
    },
    {
        "content": "<p>I'll say it another way, I'm personally avoiding offering solutions right now, because I really want us to get a better collective understanding of the problem set.</p>",
        "id": 225715956,
        "sender_full_name": "rylev",
        "timestamp": 1612888032
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"352985\">tm</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/lets.20keep.20talking.20about.20perf.2Erlo/near/225708624\">said</a>:</p>\n<blockquote>\n<p>The instruction counts a quite precise, I think</p>\n</blockquote>\n<p>\"precise, I think\" doesn't help during reviews. Actual numbers how large the variance on individual benchmarks is would be nice. Is 0.1% significant on that particular one or not?</p>\n<blockquote>\n<p>but a large bias is built into the compiler, due to unpredictable effects of CGU partitioning.</p>\n</blockquote>\n<p>opt build tests run with CGUs &gt; 1 too?</p>",
        "id": 225719536,
        "sender_full_name": "The 8472",
        "timestamp": 1612889400
    },
    {
        "content": "<p>I was referring to rustc specifically, which would use the default 16 CGUs for the compiler crates, one CGU for the standard library crates, and ∞ for compiler builtins. There is of course similar aspect with respect to benchmarks, but you would have to consult their Cargo.toml to see what they do exactly (usually defaults of 16 / 256 incremental).</p>",
        "id": 225723686,
        "sender_full_name": "tm",
        "timestamp": 1612890885
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"330154\">@The 8472</span> <span class=\"user-mention\" data-user-id=\"352985\">@tm</span> Would you be able to move this particular conversation to a new topic? That will help make sure this topic stays focused on the more \"meta\" point.</p>",
        "id": 225725579,
        "sender_full_name": "rylev",
        "timestamp": 1612891640
    },
    {
        "content": "<p>Big +1 for not looking at solutions until the problem space has been explored. But I was involved in developing some rudimentary statistical models of regressions in Lolbench, and would love to be involved in similar efforts for perf. So keep me in mind if that solution would be valuable.</p>",
        "id": 225727658,
        "sender_full_name": "Eh2406",
        "timestamp": 1612892473
    },
    {
        "content": "<p>I've got some feedback on a few points here and in <a href=\"https://hackmd.io/5VHT_I8wRDebYVYoTeNK-Q#Feedback-Analysis\">https://hackmd.io/5VHT_I8wRDebYVYoTeNK-Q#Feedback-Analysis</a>:</p>\n<blockquote>\n<ul>\n<li>the compare mode presentation, for each benchmark, has one \"full\" and then several incr variants. This might be leading to over-emphasis on incr performance.</li>\n</ul>\n</blockquote>\n<p>There are three main workloads caused by incr. comp. which are rather different from each other, and are important to keep track of:</p>\n<ul>\n<li>starting from an empty cache -&gt; important if we want incr. comp. to be the default (as it is atm for debug and check builds)</li>\n<li>re-building after a small(ish) change -&gt; probably the most common case</li>\n<li>re-compiling with no changes present -&gt; this happens if an upstream dependency changes</li>\n</ul>",
        "id": 225812715,
        "sender_full_name": "mw",
        "timestamp": 1612950012
    },
    {
        "content": "<p>I think it's a good idea to keep track of (at least some variation) of each of these three. However, for some crates with have a lot of different cases (because 'small change' can mean a myriad of different things). We could trim those down</p>",
        "id": 225813075,
        "sender_full_name": "mw",
        "timestamp": 1612950223
    },
    {
        "content": "<p><em>How</em> we present the different incr. cases is also a different question</p>",
        "id": 225813147,
        "sender_full_name": "mw",
        "timestamp": 1612950250
    },
    {
        "content": "<p>we could do something visually to make them less prominent</p>",
        "id": 225813185,
        "sender_full_name": "mw",
        "timestamp": 1612950273
    },
    {
        "content": "<p>and it would definitely be a good idea to provide descriptions for each benchmark, i.e. why it's there and what it measures</p>",
        "id": 225813283,
        "sender_full_name": "mw",
        "timestamp": 1612950327
    },
    {
        "content": "<blockquote>\n<ul>\n<li>Not clear which benchmarks matter. Am I supposed to weight all equally? (Does summary weight all equally?)</li>\n</ul>\n</blockquote>\n<p>Yes, we have lots of \"stress tests\" for particular code paths. I'm not quite sure what to make of those. They can definitely be useful to spot some regressions, but I'm not sure if they should be included in the weighted average</p>",
        "id": 225813451,
        "sender_full_name": "mw",
        "timestamp": 1612950435
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> Shall we schedule a meeting to talk over these points? I'm fairly open today earlier in the day (i.e., for the next 6 hours)</p>",
        "id": 225813487,
        "sender_full_name": "rylev",
        "timestamp": 1612950469
    },
    {
        "content": "<p>historically we tried to capture \"high impact\" crates, i.e. crates that are used by many projects. I think it's a good idea to have commonly used crates in the standard suite.</p>",
        "id": 225813589,
        "sender_full_name": "mw",
        "timestamp": 1612950522
    },
    {
        "content": "<blockquote>\n<ul>\n<li>noise, in particular on some benchmarks, is frequently present, and makes it easy to essentially ignore such benchmarks.</li>\n</ul>\n</blockquote>\n<p>Yup, noise often makes it hard to interpret benchmarks, and the smaller a benchmark the bigger the noise, usually. One problematic workaround to the noise problem is that people usually just look at the less noisy metrics (i.e. mostly instruction count). But we actually only care about wall-time and memory (EDIT: and disk) usage in practice (incidentally the two noisiest metrics <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span>) I have definitely seen cases where instruction count did not change, but wall time did (and significantly so) because of CPU caching effects, synchronization, or blocking I/O.</p>\n<p>The most reliable way of reducing noise (that I know of) is doing more benchmark iterations. Given that benchmarking parallelizes well, this is mostly a matter of resources. Reducing noise would go a long way in helping folks correctly interpret results.</p>\n<p>Another thing to look at would be the configuration of the benchmarking hardware. We probably don't want to measure how well the cooling system enables turbo boost and such <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span> </p>\n<p>Also, are there ways of making memory consumption less noisy, e.g. by using a different (but realistic) allocator?</p>",
        "id": 225814712,
        "sender_full_name": "mw",
        "timestamp": 1612951247
    },
    {
        "content": "<blockquote>\n<p>A small set of smoke-test benchmarks could be useful. The set could include a few benchmarks that are low-variance and good predictors of real regressions. </p>\n</blockquote>\n<p>This is a great idea!</p>",
        "id": 225814830,
        "sender_full_name": "mw",
        "timestamp": 1612951312
    },
    {
        "content": "<p>Despite what I said about instruction counts above, they can be very useful since they don't depend on the underlying hardware much. So, for a smoke-test it might actually be feasible to run that in a VM, if it only measures instruction counts. Memory consumption should also be mostly hardware independent, so that could be measured too. Given a low enough number of test cases and a high iteration count, it sounds like it should be possible to catch prominent regressions in a reasonable amount of time.</p>",
        "id": 225815305,
        "sender_full_name": "mw",
        "timestamp": 1612951605
    },
    {
        "content": "<blockquote>\n<p>The conversation is currently geared heavily towards the first item, but I don't want us to forget about the others.</p>\n</blockquote>\n<p>I want to second that statement. It's easy to get lost in technical details (when it's often simple UI improvements that make a big difference).</p>",
        "id": 225816468,
        "sender_full_name": "mw",
        "timestamp": 1612952393
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"124287\">mw</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/lets.20keep.20talking.20about.20perf.2Erlo/near/225814712\">said</a>:</p>\n<blockquote>\n<p>Also, are there ways of making memory consumption less noisy, e.g. by using a different (but realistic) allocator?</p>\n</blockquote>\n<p>The noise here comes from the fact that the compilation is multi-threaded and thus allocations to a global heap are done interleaved, etc. I imagine that memory consumption could be measured much more precisely in a single-threaded environment; or if the allocator implementation maintained per-thread heaps (though timing would still introduce non-determinism).</p>\n<p>All that said I don't think we can replace (easily?) the allocator llvm uses.</p>",
        "id": 225823851,
        "sender_full_name": "nagisa",
        "timestamp": 1612957133
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224872\">rylev</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/lets.20keep.20talking.20about.20perf.2Erlo/near/225813487\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> Shall we schedule a meeting to talk over these points? I'm fairly open today earlier in the day (i.e., for the next 6 hours)</p>\n</blockquote>\n<p>I didn’t see this until now</p>",
        "id": 225848348,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612969261
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> I’ve got a meeting in 29 minutes. Do you want to try to chat quickly now?</p>",
        "id": 225848404,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612969294
    },
    {
        "content": "<p>Otherwise, I’ll be busy until 12pm EST. (the <code>&lt;time</code> macro is not working for me at the moment in the app…)</p>",
        "id": 225848790,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612969442
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> sorry just saw this. I'm free in 15 minutes (noon EST)</p>",
        "id": 225868785,
        "sender_full_name": "rylev",
        "timestamp": 1612975550
    },
    {
        "content": "<p>hi hi</p>",
        "id": 225871894,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612976596
    },
    {
        "content": "<p>I'll just make a zoom mtg and post the invite here</p>",
        "id": 225871926,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612976614
    },
    {
        "content": "<p><a href=\"https://zoom.us/j/99832269996\">https://zoom.us/j/99832269996</a></p>",
        "id": 225872105,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612976678
    },
    {
        "content": "<p>Passcode: 384375</p>",
        "id": 225872122,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612976687
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> ^</p>",
        "id": 225872334,
        "sender_full_name": "pnkfelix",
        "timestamp": 1612976764
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"123586\">@nagisa</span> Yeah, those are both good points.</p>",
        "id": 225966305,
        "sender_full_name": "mw",
        "timestamp": 1613035884
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"123586\">nagisa</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/lets.20keep.20talking.20about.20perf.2Erlo/near/225823851\">said</a>:</p>\n<blockquote>\n<p>All that said I don't think we can replace (easily?) the allocator llvm uses.</p>\n</blockquote>\n<p>I hadn’t thought carefully about that detail. But I also wonder: Maybe that’s okay? I.e., I imagine we could still get very useful feedback even if we solely gather data based on swapping the Rust allocator alone...</p>",
        "id": 226049079,
        "sender_full_name": "pnkfelix",
        "timestamp": 1613073546
    },
    {
        "content": "<p>Isn't peak rss in majority of cases somewhere during LLVM codegen?</p>",
        "id": 226052522,
        "sender_full_name": "nagisa",
        "timestamp": 1613075155
    },
    {
        "content": "<p>Yes (I'm working in this area on this very issue)</p>",
        "id": 226052706,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1613075233
    },
    {
        "content": "<p>(that said, maybe it would make sense to collect peak-rss at various points of compilation and plot it somehow?)</p>",
        "id": 226053254,
        "sender_full_name": "nagisa",
        "timestamp": 1613075481
    },
    {
        "content": "<p>Funny you should mention that :)</p>\n<p><a href=\"/user_uploads/4715/-fH4ZMOD2SzettAUNx1EPnPE/codegen_comparison.png\">codegen_comparison.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/-fH4ZMOD2SzettAUNx1EPnPE/codegen_comparison.png\" title=\"codegen_comparison.png\"><img src=\"/user_uploads/4715/-fH4ZMOD2SzettAUNx1EPnPE/codegen_comparison.png\"></a></div><p>This was hacked together by reading RSS basically every loop iteration of the codegen coordinator loop.</p>",
        "id": 226054141,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1613075925
    },
    {
        "content": "<p>But yeah, doing it for real with say, <code>-Z self-profile</code> would be nice</p>",
        "id": 226054450,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1613076102
    },
    {
        "content": "<p>Building rustc when measuring perf takes a lot of time. Would it be possible (and accurate enough) to measure the bootstrap time of the try build instead of rebuilding rustc once again?</p>",
        "id": 226314139,
        "sender_full_name": "cjgillot",
        "timestamp": 1613321720
    },
    {
        "content": "<p>doesn't perf.rlo already record the bootstrap time?</p>",
        "id": 226314194,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1613321780
    },
    {
        "content": "<p>My impression is that it records it by rebuilding rustc once again. This is what I would like to avoid.</p>",
        "id": 226315283,
        "sender_full_name": "cjgillot",
        "timestamp": 1613323176
    },
    {
        "content": "<p>No, try builds are highly variable. Bootstrap timing on perf is accurate to sub-percent in total and just a few percent on individual crates.</p>",
        "id": 226318797,
        "sender_full_name": "simulacrum",
        "timestamp": 1613328310
    },
    {
        "content": "<p>Possible enhancement : allow to select the metric in the detailed query panel. The compare panel often focuses on instruction count, while the detailed panel computes wall-time. It makes difficult to track where in the compiler the regression happens.</p>",
        "id": 226554703,
        "sender_full_name": "cjgillot",
        "timestamp": 1613499899
    },
    {
        "content": "<p>FYI: I'm working on a document which aims to help us improve the performance tracking/triage process. The document will be purely aimed at the <em>process</em> so things like how to improve perf.rlo itself won't be discussed. I hope to have this done sometime today or tomorrow</p>",
        "id": 226672081,
        "sender_full_name": "rylev",
        "timestamp": 1613575473
    }
]