[
    {
        "content": "<p>@lqd had an interesting question: does the rustc-perf bot (that comments on CI runs in PRs) report on changes to bootstrap timings?</p>",
        "id": 269648778,
        "sender_full_name": "nnethercote",
        "timestamp": 1643321994
    },
    {
        "content": "<p>I suspect not, because <a href=\"https://github.com/rust-lang/rust/pull/93066\">https://github.com/rust-lang/rust/pull/93066</a> improved bootstrap by 8 seconds but there was no mention of that in the PR comments</p>",
        "id": 269648863,
        "sender_full_name": "nnethercote",
        "timestamp": 1643322016
    },
    {
        "content": "<p>no, those aren't connected at all to the regular stats</p>",
        "id": 269649401,
        "sender_full_name": "simulacrum",
        "timestamp": 1643322344
    },
    {
        "content": "<p>I'd love to switch them to using <code>perf stat</code>, and maybe directly integrate them as \"just crates\", though that might skew things too much</p>",
        "id": 269649473,
        "sender_full_name": "simulacrum",
        "timestamp": 1643322371
    },
    {
        "content": "<p>Not sure how easy it is to get the significance algorithm running on them without making them regular entries in our database</p>",
        "id": 269649524,
        "sender_full_name": "simulacrum",
        "timestamp": 1643322411
    },
    {
        "content": "<p>would switching them to <code>perf stat</code> also trivially give us data about max-rss?</p>",
        "id": 269679307,
        "sender_full_name": "pnkfelix",
        "timestamp": 1643337881
    },
    {
        "content": "<p>Yeah, if they used the same collection framework we'd get all the same statistics. (Memory usage might be a bit interesting since it's pretty strongly tied to parallelism, though.)</p>",
        "id": 269680027,
        "sender_full_name": "simulacrum",
        "timestamp": 1643338749
    },
    {
        "content": "<p>someone was recently asking me about Out of Memory issues they were getting trying to build the compiler, and I realized that I'm out of touch about what the current requirements are there</p>",
        "id": 269681876,
        "sender_full_name": "pnkfelix",
        "timestamp": 1643340526
    },
    {
        "content": "<p>(and I mentioned to them that perhaps reducing the number of parallel builds could help alleviate that issue...)</p>",
        "id": 269681920,
        "sender_full_name": "pnkfelix",
        "timestamp": 1643340585
    },
    {
        "content": "<p>yeah, with big/little and similar architectures getting more and more popular I suspect we'll either need Cargo to be better at scaling not just with numcpus but with number of gigabytes, since more cores may not mean more memory in more cases</p>",
        "id": 269682363,
        "sender_full_name": "simulacrum",
        "timestamp": 1643341076
    },
    {
        "content": "<p>Agreed. While I don't think we have the data to do this in <em>general</em> for arbitrary Rust builds, I think for rustc in particular we should clamp the autodetected number of CPUs for a build at something like RAM/1.5G or similar.</p>",
        "id": 269693398,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1643352217
    },
    {
        "content": "<p>if the jobserver protocol was more dynamic, I could imagine some kind of memory-sensitive scheme -- maybe even recording prior memory usage.</p>",
        "id": 269738117,
        "sender_full_name": "simulacrum",
        "timestamp": 1643377817
    },
    {
        "content": "<p>On linux we could monitor PSI and use that to dynamically steal/re-issue/not-consume tokens.<br>\nAnd I think better cargo-critical-path scheduling (which we talked about in another thread) might also cut down on memory consumption since it should lead to fewer crates being compiled at the same time because since the goal there would be to reduce crate-parallelism in favor of within-rustc parallelism when it's beneficial to do so.</p>",
        "id": 269740935,
        "sender_full_name": "The 8472",
        "timestamp": 1643378905
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"330154\">@The 8472</span> I'd be super concerned about the jobserver protocol deadlocking if tokens are not consistently re-emitted by anything consuming them.</p>",
        "id": 269799984,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1643402770
    },
    {
        "content": "<p>At most, I think it might make sense for the \"top-level\" thing running the jobserver to sometimes eat or inject tokens itself. But only the \"top-level\" thing that actually created the jobserver pipe.</p>",
        "id": 269800025,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1643402807
    },
    {
        "content": "<p>Yeah, that was the idea. Does the kernel wake up pipe waiters based on thread priorities? If so the jobserver crate could run with normal priority and the rustc instances with reduced priority, this way it would always be able to take away a few tokens when necessary</p>",
        "id": 269800278,
        "sender_full_name": "The 8472",
        "timestamp": 1643402935
    },
    {
        "content": "<p>I think anything more advanced than what we do now (fixed sized jobserver) needs a different protocol to work well</p>",
        "id": 269805467,
        "sender_full_name": "simulacrum",
        "timestamp": 1643405493
    }
]