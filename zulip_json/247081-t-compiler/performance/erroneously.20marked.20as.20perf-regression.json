[
    {
        "content": "<p>Hi all, my PR (<a href=\"https://github.com/rust-lang/rust/issues/88574\">#88574</a>) was marked as a <code>perf-regression</code>, but I think the reported regressions are spurious because they are in rustc benchmarks while this is a rustdoc-only change. Also, the reported perf changes were for <code>deep-vector</code>, which IIRC often shows spurious perf changes. Should I remove the <code>perf-regression</code> label? I wanted to check to make sure I follow the right protocol for this :)</p>",
        "id": 251788087,
        "sender_full_name": "Noah Lev",
        "timestamp": 1630617551
    },
    {
        "content": "<p>Yes, it's a fine to remove the label (since it's not actually a regression at all so justifying it doesn't really make any sense). Please leave a comment saying that the perf results are noise.</p>",
        "id": 251834646,
        "sender_full_name": "rylev",
        "timestamp": 1630654668
    },
    {
        "content": "<p>It is concerning to see a result as large as -0.56% in effectively a noop run, since 0.2% is our cut off for whether to consider changes real or not. But I think this is the price we pay for not running many iterations.</p>",
        "id": 251834832,
        "sender_full_name": "rylev",
        "timestamp": 1630654790
    },
    {
        "content": "<p>Ok, thanks! I've removed the label.</p>",
        "id": 251932817,
        "sender_full_name": "Noah Lev",
        "timestamp": 1630698399
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> also did a second perf run, and <a href=\"https://github.com/rust-lang/rust/pull/88574#issuecomment-912771379\">it showed the exact opposite results</a> of the first run, so that confirms the spuriousness.</p>",
        "id": 251932878,
        "sender_full_name": "Noah Lev",
        "timestamp": 1630698439
    },
    {
        "content": "<p>thatâ€™s an interesting idea: would it make any sense for us to automatically do a second run if we see only small (but supposedly real) deltas in the first run? (This reminds me again of the <a href=\"https://dl.acm.org/doi/pdf/10.1145/2555670.2464160\">Rigorous Benchmarking in Reasonable Time</a> paper)</p>",
        "id": 252351122,
        "sender_full_name": "pnkfelix",
        "timestamp": 1631039756
    },
    {
        "content": "<p>That could be interesting, although it'd have to be carefully designed so it didn't keep running perf over and over in an infinite loop ;)</p>",
        "id": 252501686,
        "sender_full_name": "Noah Lev",
        "timestamp": 1631122603
    },
    {
        "content": "<p>Also, it might make sense to only do it if the deltas are mixed, because I'd assume lots of PRs have small but supposedly real deltas, but they might all be slightly positive or slightly negative, and the case where they are mixed is more likely to be spurious.</p>",
        "id": 252501788,
        "sender_full_name": "Noah Lev",
        "timestamp": 1631122646
    },
    {
        "content": "<p>AIUI most benchmarks already are executed multiple times, so we're already throwing away data by showing only a single sample instead of multiple</p>",
        "id": 252502128,
        "sender_full_name": "The 8472",
        "timestamp": 1631122790
    }
]