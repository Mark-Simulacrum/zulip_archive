[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span> <span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> not sure I follow your logic on <a href=\"https://perf.rust-lang.org/compare.html?start=b5a2ccee81406303324016d03399fac68ceb6718&amp;end=7d3788b67103a424bf6612495afedb80c4fe8b10\">this perf regression</a>. Absolute regression numbers can be useful, but we provide regression in percentage terms for a reason. I agree that for benchmarks that are stress tests and not real world crates, we should probably be a bit more lenient, but an 8% regression isn't something we should just shrug off IMO.</p>",
        "id": 246857150,
        "sender_full_name": "rylev",
        "timestamp": 1626965247
    },
    {
        "content": "<p>For <code>cargo watch -x check -x test</code> style of uses we also want small values to stay small, not just shave off time from the big ones.</p>",
        "id": 246857750,
        "sender_full_name": "The 8472",
        "timestamp": 1626965508
    },
    {
        "content": "<p>I agree -- saying things like regressions are in the millisecond range is usually not terribly helpful, because a <em>lot</em> of our benchmarks (intentionally!) take less than a second to run. This means we can collect more data per run on different kinds of builds, but it does mean timing is going to really matter. If your benchmark takes 300ms, then 1ms is actually pretty big.</p>",
        "id": 246857756,
        "sender_full_name": "simulacrum",
        "timestamp": 1626965510
    },
    {
        "content": "<p>Hmm that's an interesting point. I think for the specific issue we're discussing, it's still worth landing with the regression. All of the real world crates (including hello-world) show &lt; 1% regression and some are even showing performance improvements with the exception of <code>wg-grammar</code>.</p>",
        "id": 246861238,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1626966835
    },
    {
        "content": "<p>I just realized part of the disconnect here might be that in last week's compiler team meeting, many of us saw the 20% regression on the <code>deeply-nested-check</code> benchmark and that framed the discussion while <span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> you're talking about the 8% regression on the <code>-debug</code> variant. I think that's a much more reasonable number than the 20% we saw originally.</p>",
        "id": 246862052,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1626967221
    },
    {
        "content": "<p>Perhaps all I'm saying is that it would be helpful if tests had an indication when they run for an extremely small amount of time (like &lt; 50ms or something).</p>",
        "id": 246862143,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1626967271
    },
    {
        "content": "<p>You can look at the wall-time metrics to get that</p>",
        "id": 246862190,
        "sender_full_name": "simulacrum",
        "timestamp": 1626967304
    },
    {
        "content": "<p>but, like, instructions are used for a reason</p>",
        "id": 246862205,
        "sender_full_name": "simulacrum",
        "timestamp": 1626967314
    },
    {
        "content": "<p>Yeah, that's definitely true.</p>",
        "id": 246863815,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1626968099
    },
    {
        "content": "<p>The real world crates we have are not indicative of all real world usage. An 8% regression in a stress test benchmark could potentially lead to a noticeable regressions in real world crates - maybe not 8%, but who’s to say? </p>\n<p>In general, if we’re going to ignore 8% regressions in benchmarks, we should be asking why we have that benchmark in the first place.</p>",
        "id": 246866697,
        "sender_full_name": "rylev",
        "timestamp": 1626969361
    },
    {
        "content": "<p>On the other hand, an acceptable justification could be “despite 8% regressions in some benchmarks, the combination of the fact that other benchmarks improved along with the utility of the change itself, makes it acceptable for there to be an 8% regression”.</p>",
        "id": 246867077,
        "sender_full_name": "rylev",
        "timestamp": 1626969535
    },
    {
        "content": "<p>But we should probably talk over some of this and come up with better guides for what is acceptable and what is not.</p>",
        "id": 246867139,
        "sender_full_name": "rylev",
        "timestamp": 1626969575
    },
    {
        "content": "<p>is this the correct link to the perf results btw ? isn't this PR a correctness fix for the incremental fingerprint errors we're trying to fix since march and almost have to accept the regressions <span aria-label=\"sweat\" class=\"emoji emoji-1f613\" role=\"img\" title=\"sweat\">:sweat:</span>  ?</p>",
        "id": 246867161,
        "sender_full_name": "lqd",
        "timestamp": 1626969591
    },
    {
        "content": "<p>I’m mostly in <span class=\"user-mention\" data-user-id=\"116113\">@lqd</span> ’s camp here. The real justification for letting that PR through is that it seems like the best option available for resolving a particular subclass of fingerprint issues</p>",
        "id": 247287179,
        "sender_full_name": "pnkfelix",
        "timestamp": 1627352644
    },
    {
        "content": "<p>We can work on figuring out ways to address the performance hit over time</p>",
        "id": 247287206,
        "sender_full_name": "pnkfelix",
        "timestamp": 1627352670
    },
    {
        "content": "<p>Also, there’s another way of framing the discussion, at least for the changes that are fixing fingerprint issues: As long as the fingerprint issues remain significant, people will be forced to turn incr-comp off (or lazily bypass it via <code>cargo clean</code>, etc). So for every PR that addresses a fingerprint issue, we should be focusing <strong>at most</strong> on the performance hit for “full” build types. All the <code>incr-*</code> variants should be heavily discounted, because if fingerprint bugs persist, then incr-comp is turned off (at least for some people), and as long as incr-comp is turned off, then the only perfomance regressions that matter are those that hit “full” build types.</p>",
        "id": 247287416,
        "sender_full_name": "pnkfelix",
        "timestamp": 1627352847
    },
    {
        "content": "<p>(well, okay, actually: After I narrowed the UI to just the “full” build types, it didn’t decrease the degree nor number of regressing benchmarks significantly. So that particular argument doesn’t really apply here; sorry for the noise.)</p>",
        "id": 247287637,
        "sender_full_name": "pnkfelix",
        "timestamp": 1627353044
    },
    {
        "content": "<p>btw <a href=\"https://github.com/rust-lang/rust/pull/84944\">https://github.com/rust-lang/rust/pull/84944</a> improves performance on a bunch of the bigger benchmark regressions seen in the \"correctness fix\" results above</p>",
        "id": 247298459,
        "sender_full_name": "lqd",
        "timestamp": 1627367313
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> <span class=\"user-mention\" data-user-id=\"116113\">@lqd</span> to be clear the issue at hand here is not the performance regression itself (I agree with your assessment that the fix is worth the regression) - it’s the lack of written justification. Your explanation above is not documented in the PR at all, and one just has to know about all this context to understand why the performance regression is acceptable.</p>",
        "id": 247308106,
        "sender_full_name": "rylev",
        "timestamp": 1627375541
    },
    {
        "content": "<p>aaah thanks for the clarification (it wasn't obvious to me that this was the issue, rather than the topic of absolute numbers on this PR's results, sorry about that), that makes sense</p>",
        "id": 247308267,
        "sender_full_name": "lqd",
        "timestamp": 1627375667
    },
    {
        "content": "<p>Well, the justification given was “this actually isn’t a performance regression” which I don’t agree with. This was a performance regression, just one that’s worth the cost.</p>",
        "id": 247308410,
        "sender_full_name": "rylev",
        "timestamp": 1627375755
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224872\">rylev</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/Absolute.20performance.20numbers/near/247308410\">said</a>:</p>\n<blockquote>\n<p>Well, the justification given was “this actually isn’t a performance regression” which I don’t agree with. This was a performance regression, just one that’s worth the cost.</p>\n</blockquote>\n<p>Can you point to where that statement was made? The only <a href=\"https://github.com/rust-lang/rust/pull/85868#issuecomment-880760352\">comment I can find</a> says \"This performance is worse than the two other approaches […], but this is less hacky, and more clearly correct.\"</p>",
        "id": 247337115,
        "sender_full_name": "pnkfelix",
        "timestamp": 1627394559
    },
    {
        "content": "<p>This was spurred originally by <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bweekly.5D.202021-07-22.20.2354818\">this comment in zulip</a> not by the pull request. My characterization is perhaps slightly unfair, but the gist is that the comment made was not of the nature \"the regression is acceptable because the fix is necessary\" but rather \"the performance was not so bad so let's just merge this\".</p>",
        "id": 247342561,
        "sender_full_name": "rylev",
        "timestamp": 1627397031
    },
    {
        "content": "<p>I think we need to do a better job of having guidance for characterizing how bad a perf regression is. We're pretty good now at knowing whether a regression has occurred or not but not what the actual magnitude of the regression is. This is trickier because the percentage regression means different things depending on the benchmark in question. For example, stress test benchmarks are likelier to show large regressions when they regress (because they typically hit the same code paths over and over) while real world crates might have smaller regressions since their code exercises more code paths than stress tests do (typically).</p>",
        "id": 247342947,
        "sender_full_name": "rylev",
        "timestamp": 1627397202
    },
    {
        "content": "<p>Once we have a mechanism for quantifying the severity of a regression, it will be easier to say things like \"regression is X bad, but weighed against the importance of the PR in question, an X bad regression is worth it\"</p>",
        "id": 247343065,
        "sender_full_name": "rylev",
        "timestamp": 1627397261
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224872\">rylev</span> <a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/Absolute.20performance.20numbers/near/247342561\">said</a>:</p>\n<blockquote>\n<p>This was spurred originally by <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bweekly.5D.202021-07-22.20.2354818\">this comment in zulip</a> not by the pull request. My characterization is perhaps slightly unfair, but the gist is that the comment made was not of the nature \"the regression is acceptable because the fix is necessary\" but rather \"the performance was not so bad so let's just merge this\".</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> you probably meant <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bweekly.5D.202021-07-22.20.2354818/near/246856159\">this comment</a>, right? (Zulip links are non-trivial. <span aria-label=\"sad\" class=\"emoji emoji-2639\" role=\"img\" title=\"sad\">:sad:</span> )</p>",
        "id": 247356372,
        "sender_full_name": "pnkfelix",
        "timestamp": 1627403011
    },
    {
        "content": "<p>Ah yes sorry!</p>",
        "id": 247357838,
        "sender_full_name": "rylev",
        "timestamp": 1627403754
    },
    {
        "content": "<p>okay that helps a lot to provide context of conversation</p>",
        "id": 247358438,
        "sender_full_name": "pnkfelix",
        "timestamp": 1627404063
    }
]