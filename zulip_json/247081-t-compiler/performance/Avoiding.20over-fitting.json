[
    {
        "content": "<p>I've been thinking about perf a little bit, and have a question: How do we avoid over-fitting our optimizations to the test suite? Obviously many real world benchmarks and good statistical practices (eg not \"averaging\" the various perf-rlo results to get an aggregate) are important, but is there anything concrete outside of that? Is the assumption just that the test suite is diverse enough and compiler performance complicated enough that over-fitting would be unreasonably difficult? (note that I have no particular reason to think that this is happening, I'm mostly just asking from a \"curiosity\" perspective)</p>",
        "id": 273297741,
        "sender_full_name": "Jake",
        "timestamp": 1645830796
    }
]