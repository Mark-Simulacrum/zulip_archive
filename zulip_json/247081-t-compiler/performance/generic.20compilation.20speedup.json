[
    {
        "content": "<p>Hi,<br>\nI had some ideas on how to speed up generic functions &amp; types and I wanted to know what you think about. I apologize in advance if this is nonsense or if it is already done! Notice: I have more background in C++ than in Rust</p>\n<p>The main problem with generic programming is that you cannot pre-compile generic functions in libraries. If every library is generic, then you are recompiling everything all the time. What I propose here is a (partial) solution to that problem.</p>\n<p>As far as I understand rustc, a generic function is stored in MIR, then during monomorphization each instance get a MIR representation which is then compiled to LLVM IR.</p>\n<p>Essentially, the compilation can be seen as a function <br>\nF:  (MIR code of generic function, Types T1,T2,...) -&gt; LLVM IR.</p>\n<p>This function can be seen as some kind of \"interpreter\" of the MIR code: you parse the MIR and output the corresponding LLVM codegen.</p>\n<p>Here come the proposal:<br>\nFor a fixed generic function with MIR code called \"fun\", we write the function G such that<br>\nG: (Types T1,....) -&gt; LLVM IR<br>\n     T1,....                  -&gt; F(fun,T1,...)</p>\n<p>G is a specialization of F for a specific generic function. What if we could write the function G in rust? The function could then be compiled and <em>optimized</em>! The compiled function would then be linked dynamically by the compiler (like a plugin), and be used anytime the function \"fun\" is used.<br>\nThis would mean that the function \"fun\" would be almost pre-compiled (only missing LLVM optimization passes), and should reduce the burden of using it greatly for the compiler. </p>\n<p>I apologize if this is not entirely clear, it is hard to explain in few words (I wrote more stuff about this but for C++).</p>",
        "id": 216736857,
        "sender_full_name": "Ogg",
        "timestamp": 1605370723
    },
    {
        "content": "<p>While this is an interesting proposal, I don't think it will help much for rustc. I don't think there are enough opportunities to specialize on the MIR of a generic function. In addition codegen uses queries for stuff like computing type layout. Queries can't ever be inlined as the actual code is always called through a function pointer. Also the LLVM optimizations passes are responsible for a lot of the compile time.</p>\n<p>Currently the goal is to reduce the amount of LLVM ir that needs to be generated by improving MIR optimizations, which run once before monomorphization. This is a lot easier than your proposal.</p>",
        "id": 216739384,
        "sender_full_name": "bjorn3",
        "timestamp": 1605373832
    },
    {
        "content": "<p>Thanks for the feedback!<br>\nObviously as you said some stuff could not be inlined/optimized since the types cannot be known in advance. Do you think it would represent a significant fraction of the code?<br>\nAlso, which fraction of the time is spent on LLVM optimization passes compared to MIR-&gt; LLVM IR? I tried finding some info on that but I came back empty ended.</p>\n<p>I think I still somewhat want to have a go at that and try to hack something together that does it for a specific generic function (or generic type) and see what happens.</p>",
        "id": 216749225,
        "sender_full_name": "Ogg",
        "timestamp": 1605386818
    },
    {
        "content": "<p>some LLVM module optimization is tracked in the self-profiling data under \"LLVM_module_optimize\". It's a superset of at least 2 other timed tasks so you'll also see these 2 in the measureme data (namely, actually running the functions and module optimization passes, the rest is setup for those, some LTO prep, and more). So on perf.rlo if you go to a perf run's result, and to its detailed query page, say <a href=\"https://perf.rust-lang.org/detailed-query.html?commit=75042566d1c90d912f22e4db43b6d3af98447986&amp;benchmark=style-servo-opt&amp;run_name=full\">https://perf.rust-lang.org/detailed-query.html?commit=75042566d1c90d912f22e4db43b6d3af98447986&amp;benchmark=style-servo-opt&amp;run_name=full</a> you'll see that the <code>LLVM_module_optimize_module_passes</code> subtask I mentioned is taking 28% of the time here and <code>LLVM_lto_optimize</code> another 25%</p>",
        "id": 216777561,
        "sender_full_name": "lqd",
        "timestamp": 1605426990
    },
    {
        "content": "<p>in general, you'll find all this data you need starting from those profiling results. Whenever you see a query/profile label that interests you, look for that in the code: you'll see calls to the self-profiler (to \"generic activities\", sometimes \"with args\", sometimes \"verbose\") with those event labels and go from there</p>",
        "id": 216777662,
        "sender_full_name": "lqd",
        "timestamp": 1605427196
    },
    {
        "content": "<p>That's really interseting thanks! So if I'm reading this correctly, about half of the time is spent on LLVM optimization passes.<br>\nIf I understand \"LLVM_module_codegen_emit_obj\" and \"LLVM_passes\" correctly, those also happens after the transformation from MIR to LLVM IR. <br>\nIf we sum it up, about 75% of the time is spent <em>after</em> the transformation from MIR to LLVM, which would mean that my proposal would only help for at most the remaining 25%, meaning not much.   I wonder if this figure changes for generic/macro heavy code?</p>",
        "id": 216793608,
        "sender_full_name": "Ogg",
        "timestamp": 1605448216
    },
    {
        "content": "<p>for generic heavy code, <em>more</em> time is spent in llvm</p>",
        "id": 216793670,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1605448318
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/issues/78925\">https://github.com/rust-lang/rust/issues/78925</a></p>",
        "id": 216793718,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1605448335
    },
    {
        "content": "<p>Intersting! is it the same for macro heavy code?</p>",
        "id": 216793751,
        "sender_full_name": "Ogg",
        "timestamp": 1605448382
    },
    {
        "content": "<p>well it depends what the macro generates</p>",
        "id": 216793828,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1605448457
    },
    {
        "content": "<p><a href=\"#narrow/stream/247081-t-compiler.2Fperformance/topic/Slow.20Builtin.20Derives\">https://rust-lang.zulipchat.com/#narrow/stream/247081-t-compiler.2Fperformance/topic/Slow.20Builtin.20Derives</a> might be interesting to read through</p>",
        "id": 216793845,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1605448479
    },
    {
        "content": "<p>what is the \"finish_ongoing_codegen\" task?</p>",
        "id": 216793854,
        "sender_full_name": "Ogg",
        "timestamp": 1605448484
    },
    {
        "content": "<p>I will have a look thanks :)</p>",
        "id": 216793898,
        "sender_full_name": "Ogg",
        "timestamp": 1605448550
    },
    {
        "content": "<p><code>finish_ongoing_codegen</code> waits for all background threads that are performing optimizations and the final emission of the object files.</p>",
        "id": 216794851,
        "sender_full_name": "bjorn3",
        "timestamp": 1605450007
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"364324\">@Ogg</span> ^</p>",
        "id": 216794865,
        "sender_full_name": "bjorn3",
        "timestamp": 1605450043
    },
    {
        "content": "<p>pretty confusing name, \"codegen\" usually means translation of MIR to LLVM IR</p>",
        "id": 216795303,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1605450647
    },
    {
        "content": "<p>Thanks! yeah I would never have guessed</p>",
        "id": 216796448,
        "sender_full_name": "Ogg",
        "timestamp": 1605451806
    }
]