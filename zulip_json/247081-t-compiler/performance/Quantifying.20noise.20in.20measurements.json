[
    {
        "content": "<p>I think the perf site does not currently do a great job at showing how noisy a benchmark is. </p>\n<p>What are your thoughts on adding a summary statistic like standard deviation or error bars in percentage besides each benchmark on the compare page? Implementation wise, we could extend the noise run to have more iterations (3-5) and then display the summary statistic of that population</p>",
        "id": 229713061,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1615399269
    },
    {
        "content": "<p>I'd personally love this. It's hard to know when to trust a benchmark and when not to.</p>",
        "id": 229714688,
        "sender_full_name": "rylev",
        "timestamp": 1615399921
    },
    {
        "content": "<p>which tools are there to integrate summary statistics into static code analysis?</p>",
        "id": 229715215,
        "sender_full_name": "oliver",
        "timestamp": 1615400120
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"281739\">@oliver</span> for what I'm imagining above, there's no static code analysis involved. It's just calculating the summary statistic based on 3-5 repeated runs of the benchmarks. It's pretty simple, but hopefully better than the current state</p>",
        "id": 229727317,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1615404473
    },
    {
        "content": "<p>but then those parts of the code can be collected and traced right, indexed pointing to where in the code the measurement came from</p>",
        "id": 229730788,
        "sender_full_name": "oliver",
        "timestamp": 1615405736
    },
    {
        "content": "<p>Right, okay you mean identifying <em>where</em> in the code the noise comes from. That could also be useful, and probably also more difficult</p>",
        "id": 229740331,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1615408896
    },
    {
        "content": "<p>it would be interesting to try to predict the perf outcome when changing a particular code section</p>",
        "id": 230083767,
        "sender_full_name": "oliver",
        "timestamp": 1615577027
    },
    {
        "content": "<p><a href=\"https://arxiv.org/abs/1810.05286\">https://arxiv.org/abs/1810.05286</a><br>\n\"While we cannot compute exactly the set of impacted tests for a particular change, we can approximate this computation by  learning  to  identify  which  tests  would  have  reported  a failure, based on historical data.\"</p>",
        "id": 230086510,
        "sender_full_name": "oliver",
        "timestamp": 1615578318
    },
    {
        "content": "<p>\"Change level features consist of:<br>\n•Change history for files is useful to identify active areas of development which are more prone to breakages. We thus use features indicating number of changes made to modified files in the last 3, 14, and 56 days.<br>\n•File cardinality, or number of files touched in a change.Large  changes  are  harder  to  review  and  we  assume  that probability of a test failure is lower for small changes.<br>\n•Target cardinality, i.e. number of test targets triggered by a change. If certain files are used in many projects then a small change in them might trigger unexpected behavior.<br>\n•Our projects use multiple programming languages, which have different breakage patterns. We use a fixed-size bit vector to identify extensions of files modified in a change.<br>\n•Number  of distinct  authors for  files  in  a  change  might indicate  common  code  that  is  used  in  multiple  project and requires extra attention.<br>\nTarget level features consist of:<br>\n•Historical failure rates of a target are a good baseline for the probability of failure. We include a vector of failure rates in the last 7, 14, 28 and 56 days as a feature.<br>\n•Project name is useful to identify an area the target covers and categorize breakage patterns based on a project.<br>\n•Number of tests in a target can be used as a proxy of the code area covered by it.<br>\nCross features are:<br>\n•Minimal  distance between  one  of  the  files  touched  in  a change  and  the  prediction  target.  The  feature  approximates  how  close  are  changes  to  a  given  target  and  the significance of the impact on it.<br>\n•Number  of common  tokens shared  by  paths  of  modified files  and  test  defines  lexical  distance  to  proxy  human perceived relevance.\"</p>",
        "id": 230086852,
        "sender_full_name": "oliver",
        "timestamp": 1615578467
    },
    {
        "content": "<p>Revisiting this thread. As I see it there are two types of noise:</p>\n<ul>\n<li>Noise over N runs of the same benchmark for the same commit. This is what <span class=\"user-mention\" data-user-id=\"139182\">@Simon Vandel Sillesen</span> mentions above</li>\n<li>Historic noise of a benchmark - i.e., std deviation over the last N runs of this benchmark </li>\n</ul>\n<p>Right now we could keep track of a rolling average of deviation for some number of commits in the past. For instance, when recording the result of a benchmark run, look at the results from the previous N runs of that benchmark, calculate that mean, and determine that std deviation from that mean for those historic runs. Record that std deviation as noisiness of that benchmark at the point that the new run happens.</p>",
        "id": 245339478,
        "sender_full_name": "rylev",
        "timestamp": 1625762798
    },
    {
        "content": "<p>It seems like the best time to do this is when recording the results of a perf run, but we probably could also do this after the fact if we're able to find the results that immediately proceeded the commit in question</p>",
        "id": 245339743,
        "sender_full_name": "rylev",
        "timestamp": 1625762934
    },
    {
        "content": "<p>I'm still not familiar with the details of how the collector collects results, but if everyone agrees that it would be useful to collect historical noise as measured by the std deviation of N previous runs of the benchmark, I could try to take a look at implementing it.</p>",
        "id": 245339993,
        "sender_full_name": "rylev",
        "timestamp": 1625763032
    },
    {
        "content": "<p>We run several times on the same benchmark</p>",
        "id": 245340566,
        "sender_full_name": "simulacrum",
        "timestamp": 1625763329
    },
    {
        "content": "<p>So there's likely at least 2 runs available to give a little measure of noise</p>",
        "id": 245340592,
        "sender_full_name": "simulacrum",
        "timestamp": 1625763344
    },
    {
        "content": "<p>But I think historical is likely to be more practical, or having dedicated \"noise measurements\" every few months</p>",
        "id": 245340676,
        "sender_full_name": "simulacrum",
        "timestamp": 1625763371
    },
    {
        "content": "<p>In theory one could also get more signal out of looking at all benchmarks in a run instead of individual benchmarks. But the resulting statement is more nebulous \"overall it looks like a small regression, but we can't point to any individual benchmark since they're all in their recent noise range\", so that's probably less useful.</p>",
        "id": 245345064,
        "sender_full_name": "The 8472",
        "timestamp": 1625765479
    },
    {
        "content": "<blockquote>\n<p>Right now we could keep track of a rolling average of deviation for some number of commits in the past.</p>\n</blockquote>\n<p>You do NOT want the rolling average. Averages are sensitive to big outliers. And when we're measuring performance deltas then real optimizations or regressions are the big changes. To get the noise we need something histogram-based so we can exclude the big changes.</p>",
        "id": 245375882,
        "sender_full_name": "The 8472",
        "timestamp": 1625780381
    },
    {
        "content": "<p>mm. I heard a recent argument that trimmed mean is a good way to go</p>",
        "id": 245455031,
        "sender_full_name": "pnkfelix",
        "timestamp": 1625843172
    },
    {
        "content": "<p>But to be fair that was more about metrics w.r.t. availability and latency</p>",
        "id": 245455112,
        "sender_full_name": "pnkfelix",
        "timestamp": 1625843216
    },
    {
        "content": "<p>(well, I guess compile-times <em>are</em> latency… heh.)</p>",
        "id": 245455140,
        "sender_full_name": "pnkfelix",
        "timestamp": 1625843228
    },
    {
        "content": "<p>There has to be good statistical methods for determining variance that takes into account outliers, no? Histograms are great, and we'll want those, but we'll also want a method for categorizing noise that automated systems can take advantage of, and histograms don't allow for that.</p>",
        "id": 245458691,
        "sender_full_name": "rylev",
        "timestamp": 1625844900
    },
    {
        "content": "<p>Well, since we'd be taking abs(delta) trimming only makes sense on one end of the distribution. And to do that kind of trimming you need to keep multiple samples in memory rather than just calculating a moving average. At that point you might as well build a histogram and later pick a percentile as your cutoff point based on bucket populations.</p>",
        "id": 245458718,
        "sender_full_name": "The 8472",
        "timestamp": 1625844917
    },
    {
        "content": "<p>How do histograms not let you cut off outliers? They're basically a discrete version of a CDF. So you can cut off at some percentile.</p>\n<p>Just to make sure we're talking about the same thing: I mean a histogram of deltas. With noise we expect a left-heavy distribution with the small buckets containing all the noise and the significant changes being the outliers on the right</p>\n<p>So we can say that most changes aren't performance-impacting (this is a heuristic based on looking at past charts, not an iron law) so we expect a large percentile of deltas to be noise.</p>",
        "id": 245459157,
        "sender_full_name": "The 8472",
        "timestamp": 1625845148
    },
    {
        "content": "<p>I see - I think we agree.</p>",
        "id": 245459176,
        "sender_full_name": "rylev",
        "timestamp": 1625845161
    },
    {
        "content": "<p>That'll ignore the small but significant changes which are within the noise floor of a single change and only become apparent over multiple samples. Getting those is where things start to get tricky.</p>",
        "id": 245459395,
        "sender_full_name": "The 8472",
        "timestamp": 1625845275
    },
    {
        "content": "<p>On a practical level, I'm wondering _when_ the histogram should be calculated. We could calculate inside the collector when recording the results of a run, we could try to look up previous runs that match the same description (i.e., the same \"pstat_series\"), but we could also do this on demand since the storing something new in the database would only be in order to cache the lookup.</p>",
        "id": 245463837,
        "sender_full_name": "rylev",
        "timestamp": 1625847480
    },
    {
        "content": "<p>One issue is is that we don't currently keep track of _when_ runs are done. Most likely we'll want to find N number of runs that happen most recently relative to the run in question (by the way, by \"run\" I mean what the DB refers to as \"collection\")</p>",
        "id": 245463989,
        "sender_full_name": "rylev",
        "timestamp": 1625847569
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> what do you think about adding a created_at field to the collection table that would allow us to know _when_ perf runs happened? This could be used to compare runs temporally.</p>",
        "id": 245464575,
        "sender_full_name": "rylev",
        "timestamp": 1625847899
    },
    {
        "content": "<p>Hm, I'm not sure what you would want to use it for. I think it seems fine to add if necessary, but I feel like recent by \"previous N commits\" is better</p>",
        "id": 245465052,
        "sender_full_name": "simulacrum",
        "timestamp": 1625848098
    },
    {
        "content": "<p>Two concerns with previous N commits:</p>\n<ul>\n<li>Try runs are from different branches and don't have a parent child relationship. If two try runs happen don't we want the results of one to factor into noise calculation of the other?</li>\n<li>Walking commit graphs sounds more expensive than a db lookup, but maybe not....</li>\n</ul>",
        "id": 245465336,
        "sender_full_name": "rylev",
        "timestamp": 1625848240
    },
    {
        "content": "<p>previous N commits in the 'parent commit' sense</p>",
        "id": 245466282,
        "sender_full_name": "simulacrum",
        "timestamp": 1625848700
    },
    {
        "content": "<p>we can store a vec for main, and try commits have one lookup in theory to get to parent and then you're on main</p>",
        "id": 245466346,
        "sender_full_name": "simulacrum",
        "timestamp": 1625848737
    },
    {
        "content": "<p>Ok cool we'll start with that. I might just prototype something next week.</p>",
        "id": 245466667,
        "sender_full_name": "rylev",
        "timestamp": 1625848916
    },
    {
        "content": "<p>So I have something working, but one question  I have is how do we handle benchmarks where a change has led to a new mean. The proposal put forth by <span class=\"user-mention\" data-user-id=\"330154\">@The 8472</span> works really well for benchmarks that maintain a steady mean over time. But some changes fundamentally shift the mean which introduces more variance that is <em>not</em> noise. A histogram approach won't necessarily help here as the mean is thrown off and <em>all</em> deltas will have more variance from the mean than they should.</p>",
        "id": 245605323,
        "sender_full_name": "rylev",
        "timestamp": 1626007391
    },
    {
        "content": "<p>Looks like step detection can help with this. I'll look into that.</p>",
        "id": 245605413,
        "sender_full_name": "rylev",
        "timestamp": 1626007556
    },
    {
        "content": "<p>Don't measure deltas relative to a mean, measure deltas between adjacent commits.</p>",
        "id": 245605487,
        "sender_full_name": "The 8472",
        "timestamp": 1626007653
    },
    {
        "content": "<p>Given recent commits <em>a, b, c, d</em> calculate deltas <em>abs(a-b), abs(b-c), abs(c-d)</em>. build a histogram of absolute deltas.<br>\nFor a new perf run <em>e</em> you calculate the delta <em>abs(d, e)</em> and then look into which bucket it falls on the histogram. the higher the percentile the more likely it's not noise.</p>",
        "id": 245605602,
        "sender_full_name": "The 8472",
        "timestamp": 1626007820
    },
    {
        "content": "<p>How would you define the buckets?</p>",
        "id": 245605610,
        "sender_full_name": "rylev",
        "timestamp": 1626007852
    },
    {
        "content": "<p>look at hdrhistogram. you configure it with a value range, a desired precision and a 3rd parameter and it figures out what's needed. you can query it for percentiles.</p>",
        "id": 245605663,
        "sender_full_name": "The 8472",
        "timestamp": 1626007927
    },
    {
        "content": "<p><a href=\"https://crates.io/crates/hdrhistogram\">https://crates.io/crates/hdrhistogram</a></p>",
        "id": 245605684,
        "sender_full_name": "The 8472",
        "timestamp": 1626007984
    },
    {
        "content": "<p>I suppose we could also just play it by ear since at first we only want the binary decision of \"is this noise\"</p>",
        "id": 245605751,
        "sender_full_name": "rylev",
        "timestamp": 1626008084
    },
    {
        "content": "<p>Oh, you meant how to choose which bucket to pick as threshold and not the bucket sizes?</p>",
        "id": 245605810,
        "sender_full_name": "The 8472",
        "timestamp": 1626008184
    },
    {
        "content": "<p>Yes</p>",
        "id": 245605817,
        "sender_full_name": "rylev",
        "timestamp": 1626008224
    },
    {
        "content": "<p>yeah, I'd just print a histogram for each benchmark and pick a threshold by hand so that whatever we manually deemed noteworth in the past is above it</p>",
        "id": 245605876,
        "sender_full_name": "The 8472",
        "timestamp": 1626008343
    },
    {
        "content": "<p>I guess it's a pretty high quantile since there are lots of commits each week but only few noteworthy items</p>",
        "id": 245605887,
        "sender_full_name": "The 8472",
        "timestamp": 1626008390
    },
    {
        "content": "<p>I also want to classify benchmarks as noisy or not. I think I should be able to measure the mean and std deviations of changes (i.e., the delta between two commits) and then calculate the coefficient of variance on that population (trimming outliers at the top which are more likely to be real changes).</p>",
        "id": 245605995,
        "sender_full_name": "rylev",
        "timestamp": 1626008514
    },
    {
        "content": "<p>If those deltas have a high coefficient of variance than the benchmark is noisy. And we can tweak that threshold over time.</p>",
        "id": 245606055,
        "sender_full_name": "rylev",
        "timestamp": 1626008592
    },
    {
        "content": "<p>You can also get that by looking at the histogram, just pick a lower percentile instead to figure out where the noise is, then get the ratio of a low-quantile delta (the noise) to the magnitude of a non-delta sample.</p>",
        "id": 245606066,
        "sender_full_name": "The 8472",
        "timestamp": 1626008631
    },
    {
        "content": "<p>I guess I just am having a harder time translating \"looking at the histogram\" into code</p>",
        "id": 245606134,
        "sender_full_name": "rylev",
        "timestamp": 1626008716
    },
    {
        "content": "<p>Yeah that's the heuristic part based on past experience, the one parameter we need to tune.</p>\n<p>If 50% of all rustc contributors were people that made big feature changes no matter the cost in performance and the other 50% then did crazy optimizations on the next commit to fix the regression then the charts would go up and down like a sawtooth. Then most changes would be significant and occupy a major fraction of the histogram and you'd only find noise in the bottom decile for example.</p>\n<p>But that's not what happens, big performance changes that we care about are rare, so the noise probably dominates more than 50% of it and the interesting changes must be somewhere in the top decile or maybe even higher.</p>\n<p>Looking at the graphs from 2021-06-02 onwards most benchmarks experienced at least one big step change and often several smaller ones that obviously rise above the noise.<br>\nBut a few are troublesome. E.g. <em>deep-vector-check</em> is noisy but clearly moves the mean over time, but often within the noise. The histogram approach will have trouble categorizing something like that from a single perf run, you either suffer false positives or negatives from single samples. But the histogram for something like that should look different. The easy ones should be positive-skewed while the difficult ones are flatter or even negative-skewed.</p>",
        "id": 245607103,
        "sender_full_name": "The 8472",
        "timestamp": 1626010029
    },
    {
        "content": "<p>I guess we don't really need a histogram. The sample count is small enough that one could calculate statistical measures directly on them.</p>",
        "id": 245607478,
        "sender_full_name": "The 8472",
        "timestamp": 1626010612
    },
    {
        "content": "<p>I think I'm settling for calculating commit deltas, ordering them from smallest to largest, removing the top 25% since we're not after large changes, measuring the mean of that subsection of deltas, and then dividing by the mean of the benchmark population to normalize.</p>",
        "id": 245607742,
        "sender_full_name": "rylev",
        "timestamp": 1626011103
    },
    {
        "content": "<p><a href=\"https://docs.rs/average/0.9.2/average/index.html\">https://docs.rs/average/0.9.2/average/index.html</a></p>",
        "id": 245607748,
        "sender_full_name": "The 8472",
        "timestamp": 1626011116
    },
    {
        "content": "<p>That should answer the question of how much non-significant change happens on average.</p>",
        "id": 245607753,
        "sender_full_name": "rylev",
        "timestamp": 1626011139
    },
    {
        "content": "<p>If it's above a certain percentage, we can label that noise.</p>",
        "id": 245607761,
        "sender_full_name": "rylev",
        "timestamp": 1626011153
    },
    {
        "content": "<blockquote>\n<p>removing the top 25% since we're not after large changes, measuring the mean of that subsection of deltas</p>\n</blockquote>\n<p>That smells like bad statistics. I think that would result in classifying <em>more</em> than 25% of all changes as non-noise.</p>\n<p>Taking a mean of an already oneside-trimmed sample set only means you're getting even value than whatever the highest one the 75th percentile was.</p>",
        "id": 245607982,
        "sender_full_name": "The 8472",
        "timestamp": 1626011441
    },
    {
        "content": "<p>I agree. But I think what we need is a measure of how many changes are above some arbitrary nose threshold. If a large percentage of changes are above that threshold than we can also classify the benchmark as highly variable. Highly variable differs from noisy in that highly variable benchmarks might actually be measuring real performance sensitivity. Ideally our benchmarks would only experience significant change some small percentage of time.</p>",
        "id": 245608240,
        "sender_full_name": "rylev",
        "timestamp": 1626011823
    },
    {
        "content": "<p>Right, e.g. the CGU partitioning stuff isn't really noise. ~highly responsive to perturbations</p>",
        "id": 245608331,
        "sender_full_name": "The 8472",
        "timestamp": 1626011988
    },
    {
        "content": "<p>The first PR of what is likely to be many is up: <a href=\"https://github.com/rust-lang/rustc-perf/pull/902\">https://github.com/rust-lang/rustc-perf/pull/902</a>.</p>",
        "id": 245673257,
        "sender_full_name": "rylev",
        "timestamp": 1626089149
    },
    {
        "content": "<p>fwiw the \"change detector\" i wrote for lolbench used kernel density estimation and deviations from the mean of a trailing window (30 nightlies or sth iirc) <a href=\"https://github.com/anp/lolbench/blob/main/src/analysis.rs#L356\">https://github.com/anp/lolbench/blob/main/src/analysis.rs#L356</a></p>",
        "id": 245735853,
        "sender_full_name": "anp",
        "timestamp": 1626118059
    },
    {
        "content": "<p>you can get good results with k-means clustering if you know the <code>k</code> but thats Hard if you have a bunch of time series</p>",
        "id": 245735959,
        "sender_full_name": "anp",
        "timestamp": 1626118102
    },
    {
        "content": "<p>i kept looking at time series analysis techniques and i think it might be a rabbit hole to try and treat it like a time series vs. just a sample from an unknown distribution</p>",
        "id": 245736223,
        "sender_full_name": "anp",
        "timestamp": 1626118231
    },
    {
        "content": "<p>It depends on whether you want to pull things out of the noise floor or not. If not then just looking at the distribution of deltas  should work well enough. If you want to find jumps that are only apparent over multiple samples you have to treat it as a time series.</p>",
        "id": 245738892,
        "sender_full_name": "The 8472",
        "timestamp": 1626119556
    },
    {
        "content": "<p>This is certainly pushing the ends of my statistics 101 course I took in uni so forgive me if I have some catching up to do. Looking into the KDE technique, I'm not sure if it buys us anything here which I believe is what <span class=\"user-mention\" data-user-id=\"330154\">@The 8472</span> is saying. I don't think we need to detect jumps if we're looking at deltas though it might eventually be interesting to detect large noise through time series analysis, but if we assume that noise is equally likely to happen on every perf run, we should detect noise by seeing too many \"significant\" changes.</p>",
        "id": 245800798,
        "sender_full_name": "rylev",
        "timestamp": 1626169579
    },
    {
        "content": "<p>mm yeah that makes sense, your analysis is only running pairwise, not on historical data?</p>",
        "id": 245866031,
        "sender_full_name": "anp",
        "timestamp": 1626200397
    },
    {
        "content": "<p>Currently the analysis is on historical data, but we're only looking at deltas between adjacent performance runs. We don't do any analysis on population as a whole (other than population analysis of the deltas)</p>",
        "id": 245867581,
        "sender_full_name": "rylev",
        "timestamp": 1626201063
    },
    {
        "content": "<blockquote>\n<p>Looking into the KDE technique, I'm not sure if it buys us anything here which I believe is what @The 8472 is saying.</p>\n</blockquote>\n<p>Partially. Let's say we have an artificial sample set of 0.4, 0.5, 0.4, 0.5, 0.4, 0.5, 0.4, 0.5, 0.4, 0.5, 0.41, 0.51, 0.41, 0.51, 0.41, 0.51, 0.41, 0.51, 0.41, 0.51... then obviously the mean jumped by 0.01. the delta approach will discard that change because for a single sample it's much smaller than what we qualified as noise, even though it is significant once you look at many samples. But trying to extract these from arbitrary timelines is hard and beyond my statistics knowledge. Clustering seems like it might find some of those, on good days :D</p>",
        "id": 245871125,
        "sender_full_name": "The 8472",
        "timestamp": 1626202735
    },
    {
        "content": "<p>yeah this makes sense!</p>",
        "id": 245871947,
        "sender_full_name": "anp",
        "timestamp": 1626203075
    },
    {
        "content": "<p>(Not statistically justified, but) Something inspired by \"Jenks natural breaks\" would call that out pretty easily. <br>\nSplit the history into N continuous chunks. Pick the change dates to minimize <code>sum(standard deviation(data within chunk))</code>.</p>",
        "id": 245969034,
        "sender_full_name": "Eh2406",
        "timestamp": 1626273586
    },
    {
        "content": "<p>And yes, this is a reformulation of one dimensional k-means clustering.</p>",
        "id": 245969277,
        "sender_full_name": "Eh2406",
        "timestamp": 1626273713
    },
    {
        "content": "<p>but this formulation supports efficient dynamic programming solutions.</p>",
        "id": 245969498,
        "sender_full_name": "Eh2406",
        "timestamp": 1626273833
    },
    {
        "content": "<p>my fear with that kind of approach is being able pick a good N for dozens/hundreds of different series that might have very different clustering. are there good ways to also iterate on number of clusters?</p>",
        "id": 246026366,
        "sender_full_name": "anp",
        "timestamp": 1626299394
    },
    {
        "content": "<p>machine learning? <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span>  (I recently saw some paper on clustering beating sota normal algorithms)</p>",
        "id": 246026629,
        "sender_full_name": "The 8472",
        "timestamp": 1626299526
    },
    {
        "content": "<p>I don't know of a statistically justified one. But I bet the <code>minimize(sum(standard deviation(data within chunk))</code> vs number of chunks graph will have a clear bend in it.<br>\nOr do the calculation for all N in 0..=data-points, and plot the histogram for etch day the number of times that day was a brake point.</p>",
        "id": 246027089,
        "sender_full_name": "Eh2406",
        "timestamp": 1626299785
    },
    {
        "content": "<p>you joke about ML but i was vaguely wondering if it would be worth it last time i spent a while trying to wrap my brain around this</p>",
        "id": 246028902,
        "sender_full_name": "anp",
        "timestamp": 1626300879
    },
    {
        "content": "<p>hm maybe it's worth trying to do clustering and pick the cluster number after all, that could be a pretty reasonable approach</p>",
        "id": 246028966,
        "sender_full_name": "anp",
        "timestamp": 1626300933
    },
    {
        "content": "<p>(I'd want to constrain the clusters to be continuous time ranges)</p>",
        "id": 246030098,
        "sender_full_name": "Eh2406",
        "timestamp": 1626301620
    },
    {
        "content": "<p>agreed</p>",
        "id": 246030181,
        "sender_full_name": "anp",
        "timestamp": 1626301678
    }
]