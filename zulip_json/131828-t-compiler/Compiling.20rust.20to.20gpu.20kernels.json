[
    {
        "content": "<p>Hello! I just wanted to showcase something ive been working on for a couple of months and get your thoughts.</p>\n<p>For some time now ive been writing a custom rustc backend which targets a closed source nvidia cuda library that uses a subset of llvm ir to compile code to gpu kernels that can be run on nvidia gpus using a library. I was tempted because the llvm ptx backend does not really work on windows, it doesnt have some closed source opts, and it makes broken ptx. </p>\n<p>Today i was finally able to compile all of core and compile a simple kernel!</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"k\">extern</span><span class=\"w\"> </span><span class=\"s\">\"C\"</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"cp\">#[link_name = </span><span class=\"s\">\"llvm.nvvm.read.ptx.sreg.tid.x\"</span><span class=\"cp\">]</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">fn</span> <span class=\"nf\">thread_id_x</span><span class=\"p\">()</span><span class=\"w\"> </span>-&gt; <span class=\"kt\">u32</span><span class=\"p\">;</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"cp\">#[no_mangle]</span><span class=\"w\"></span>\n<span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"k\">unsafe</span><span class=\"w\"> </span><span class=\"k\">fn</span> <span class=\"nf\">kernel</span><span class=\"p\">(</span><span class=\"n\">foo</span>: <span class=\"o\">*</span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"kt\">u32</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">idx</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">thread_id_x</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"kt\">isize</span><span class=\"p\">;</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">elem</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"p\">.</span><span class=\"n\">offset</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">).</span><span class=\"n\">as_mut</span><span class=\"p\">().</span><span class=\"n\">unwrap</span><span class=\"p\">();</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"o\">*</span><span class=\"n\">elem</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">elem</span><span class=\"p\">.</span><span class=\"n\">wrapping_add</span><span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n<p>was compiled into a ptx function: <a href=\"https://paste.rs/ge1\">https://paste.rs/ge1</a><br>\nwhich can then be run as a gpu kernel using a rust wrapper for the cuda driver api i forked from rustacuda. <br>\nThis is very exciting for me because i have been wanting to write gpu code in rust for a long time, but never could do so because i needed CUDA things and the llvm ptx backend didnt really work. <br>\nThis is just a POC right now, i have tons of housekeeping to do before its usable, but once it is, i will make it open source to hopefully get more people using rust for gpu computing since it seems to be one of rust's big weak points right now.</p>",
        "id": 251294593,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1630357345
    },
    {
        "content": "<p>skipping the as_mut makes it generate much better code, i need to investigate if nvvm wants you to run llvm passes before giving it the llvm ir, because it should have inlined the unwrap there </p>\n<div class=\"codehilite\"><pre><span></span><code>.visible .func kernel(\n    .param .b64 kernel_param_0\n)\n{\n    .reg .b32     %r&lt;4&gt;;\n    .reg .b64     %rd&lt;5&gt;;\n\n\n    ld.param.u64     %rd2, [kernel_param_0];\n    mov.u32     %r1, %tid.x;\n    bra.uni     $L__BB0_1;\n\n$L__BB0_1:\n    cvt.u64.u32     %rd3, %r1;\n    shl.b64     %rd4, %rd3, 2;\n    add.s64     %rd1, %rd2, %rd4;\n    bra.uni     $L__BB0_2;\n\n$L__BB0_2:\n    ld.u32     %r3, [%rd1];\n    add.s32     %r2, %r3, 6;\n    bra.uni     $L__BB0_3;\n\n$L__BB0_3:\n    st.u32     [%rd1], %r2;\n    ret;\n\n}\n</code></pre></div>",
        "id": 251296034,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1630358030
    },
    {
        "content": "<p>im also working on a crate for managing common operations as well as having a simple way of binding to nvvm ir things. This should make things much easier, for example, <code>thread::index()</code> automatically accounts for any dimension of grid/block/thread, so it will work with 3d blocks or 1d blocks just fine, while in cuda C++ youd have to do the calculation yourself and potentially mess it up and cause UB</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"cp\">#![no_std]</span><span class=\"w\"></span>\n\n<span class=\"k\">use</span><span class=\"w\"> </span><span class=\"n\">cuda_std</span>::<span class=\"n\">thread</span><span class=\"p\">;</span><span class=\"w\"></span>\n\n<span class=\"cp\">#[no_mangle]</span><span class=\"w\"></span>\n<span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"k\">unsafe</span><span class=\"w\"> </span><span class=\"k\">fn</span> <span class=\"nf\">kernel</span><span class=\"p\">(</span><span class=\"n\">foo</span>: <span class=\"o\">*</span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"kt\">u32</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">bar</span>: <span class=\"kp\">&amp;</span><span class=\"p\">[</span><span class=\"kt\">u32</span><span class=\"p\">])</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">idx</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">thread</span>::<span class=\"n\">index</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"kt\">isize</span><span class=\"p\">;</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"o\">&amp;</span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"n\">foo</span><span class=\"p\">.</span><span class=\"n\">offset</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"o\">*</span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">bar</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"kt\">usize</span><span class=\"p\">];</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n<p>this compiles just fine</p>",
        "id": 251339522,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1630392328
    },
    {
        "content": "<p>Replying to express interest. Interested to hear any other updates you have.</p>",
        "id": 253702351,
        "sender_full_name": "Ben Reeves",
        "timestamp": 1631863970
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"435059\">@Ben Reeves</span> Since then ive:</p>\n<ul>\n<li>Started adding CUDA graph support to the driver API wrapper</li>\n<li>Implemented lazily loading dependency modules, bringing the ptx down from ~12mb to a couple of kb (and hopefully optimizing better)</li>\n<li>Added raw intrinsics for __nv intrinsics from libdevice in cuda_std (i will wrap these in extension traits in cuda_std eventually)</li>\n<li>Added a <code>#[kernel]</code> attr for managing boilerplate and making sure kernel args are correct (as well as special handling inside the codegen)</li>\n<li>Started working on PTX lexing/parsing (for more dead code elimination in the final PTX as well as safety checking kernel launches)</li>\n<li>Enforced all kernels be extern \"C\" (the kernel attr does this automatically) so that i can override the C call conv in the codegen to pass everything by value, mitigating any weirdness from ScalarPair and hopefully increasing performance (since aggregates and unions are passed as byte arrays)</li>\n<li>Bothered <span class=\"user-mention\" data-user-id=\"232545\">@Joshua Nelson</span> way too much about my struggles with cuda's <em>v e r y</em> odd APIs</li>\n<li>Made a picture :O (relevant picture attached)</li>\n<li>Fixed alloc not working (i forgot to add the allocator module i codegen to the final result <span aria-label=\"face palm\" class=\"emoji emoji-1f926\" role=\"img\" title=\"face palm\">:face_palm:</span>)</li>\n<li>Made cuda_std build on non-nvptx (albeit it wont link if you actually use the functions, but gpu-specific functions go in functions with the kernel attr and kernel cfgs the function for nvptx)</li>\n<li>Wrote high level bindings to the ptx-compiler APIs</li>\n<li>Wrote high level bindings to linker APIs in the driver API, inside of the driver API wrapper</li>\n<li>Fixed i128 not working</li>\n<li>Fixed integers not <code>i1, i8, i16, i32, i64</code> segfaulting (i promise its not my fault ok, its nvvm :(</li>\n<li>Fixed llvm.used not working, and consequently </li>\n<li>Fixed lang items being compiled out (they are now put in llvm.used so that nvvm keeps them when lazy loading, same with no_mangle things)</li>\n<li>Added LLVM opts (turns out you <em>are</em> supposed to run llvm opts before), so the code is basically all inlined now</li>\n<li>Started writing an mdbook on how to use the codegen as well as how the codegen works.</li>\n<li>Panicking code now works (albeit with no nice message because i am still fighting cuda errors if i try vprintf-ing inside panic_handler)</li>\n</ul>",
        "id": 254949405,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1632679160
    },
    {
        "content": "<p>here is the glorious picture :O <a href=\"/user_uploads/4715/mLFlhUkYkzlvivZBy5vhfkrK/out.png\">out.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/mLFlhUkYkzlvivZBy5vhfkrK/out.png\" title=\"out.png\"><img src=\"/user_uploads/4715/mLFlhUkYkzlvivZBy5vhfkrK/out.png\"></a></div>",
        "id": 254949432,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1632679197
    },
    {
        "content": "<p>I still have a lot to do, i need to kinda polish everything, it <em>works</em> currently but it needs a really long command for building a crate and you need llvm 7 installed. So i need to set up something to download prebuilt llvm 7 libraries. As well as... a lot more things i want to finish</p>",
        "id": 254949698,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1632679342
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"276242\">@Riccardo D'Ambrosio</span> you told me but I forgot - is the llvm 7 limitation inherent because CUDA uses it? or could it be fixed at some point?</p>",
        "id": 254949725,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1632679378
    },
    {
        "content": "<p>Its because libnvvm expects llvm 7 bitcode, trying to feed it llvm 13 bitcode does not work (ive tried)</p>",
        "id": 254949747,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1632679404
    },
    {
        "content": "<p>nvidia would need to update libnvvm to use a newer llvm and push a new libnvvm version</p>",
        "id": 254949761,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1632679430
    },
    {
        "content": "<p>If anyone is wondering, yes, existing CUDA tools work with rust kernels :D<br>\nAlthough they do not have the original source because i havent done debug info yet <a href=\"/user_uploads/4715/l6UDrx8gkoZRFhlUKsZuDInQ/Screenshot_371.png\">Screenshot_371.png</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/l6UDrx8gkoZRFhlUKsZuDInQ/Screenshot_371.png\" title=\"Screenshot_371.png\"><img src=\"/user_uploads/4715/l6UDrx8gkoZRFhlUKsZuDInQ/Screenshot_371.png\"></a></div><p>(This is nsight compute)</p>",
        "id": 254949909,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1632679557
    },
    {
        "content": "<p>surprisingly, cuda is almost fully frontend-agnostic, internally it works off of PTX and cubin (the step after ptx), so basically any tool that works with CUDA C/C++ will work with Rust, which is great.</p>",
        "id": 254950036,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1632679669
    },
    {
        "content": "<p>I was also bikeshedding something which i think could be greatly useful for ensuring more safety inside of kernel launches (which are insanely dangerous).</p>\n<p>As background, one of the main issues in CUDA is that you use thread and block indices to index into an array. This is dangerous because oftentimes people implicitly expect a 1d configuration (where y and z indices are always 0), but the kernel is launched with an invalid configuration. Which yields data races. </p>\n<p>Currently i kind of solve this by providing a <code>thread::index()</code> function which provides a globally unique index which accounts for any configuration. This is fast because it is just a couple math ops and reading special ptx registers, but id rather avoid useless math ops when possible. So, i was thinking of adding a way to hint to the codegen and to LLVM/NVVM that the launch configuration is always 1d/2d/3d:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"cp\">#[kernel(grid_dim = 2d, block_dim = 1d)]</span><span class=\"w\"></span>\n</code></pre></div>\n<p>This is going to do a couple of things:</p>\n<ul>\n<li>Add a special hint in the final PTX as a comment (this is part of a larger thing im calling ptx validation) which will be checked during launch by the driver api wrapper.</li>\n<li>Insert assertions at the top of the kernel that ensure that the configuration is correct, and abort (well, call __assertfail which returns an AssertionFailure to the caller).</li>\n<li>Get recognized by custom lints (which im planning to add for checking other things)</li>\n</ul>\n<p>This would add basically zero overhead, i will bench it, i can probably only run the assertions once on the first thread, but due to the way GPUs work, it would not add overhead to branch like that (it isnt even a branch on the gpu).</p>",
        "id": 255078984,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1632760560
    }
]