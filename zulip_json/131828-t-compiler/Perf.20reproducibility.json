[
    {
        "content": "<p>Hello!<br>\nFor a work related project, we want to achieve something very similar to <a href=\"http://perf.rust-lang.org\" title=\"http://perf.rust-lang.org\">perf.rust-lang.org</a> where we can see performance improvements / regressions over time.<br>\nBut i wonder how did you manage to have a setup the give reproducible results?</p>",
        "id": 194673591,
        "sender_full_name": "marmeladema",
        "timestamp": 1587388931
    },
    {
        "content": "<p>like overcome frequency scaling properly etc<br>\nbecause from my own experiences, its very hard to have something that you compare from run to run</p>",
        "id": 194673686,
        "sender_full_name": "marmeladema",
        "timestamp": 1587388987
    },
    {
        "content": "<p>perf is not very reproducible :)</p>\n<p>It's been an ongoing project to reduce variability -- currently, we have ASLR (both kernel and user space) disabled and that's pretty much it</p>\n<p>I have core pinning, frequency scaling, etc. planned as things to look at</p>",
        "id": 194673742,
        "sender_full_name": "simulacrum",
        "timestamp": 1587389017
    },
    {
        "content": "<p>but for the most part we just go for instruction counts, which are mostly reliable</p>",
        "id": 194673764,
        "sender_full_name": "simulacrum",
        "timestamp": 1587389035
    },
    {
        "content": "<p>Ok! Locally (ie: on my laptop) i've isolated 1 physical core (2 threads) and i've removed as much irq as i can etc</p>",
        "id": 194677113,
        "sender_full_name": "marmeladema",
        "timestamp": 1587390574
    },
    {
        "content": "<p>But even with that, instruction cache can make things very noisy</p>",
        "id": 194677168,
        "sender_full_name": "marmeladema",
        "timestamp": 1587390599
    },
    {
        "content": "<p>Also laptop cpus are totally unpredictable</p>",
        "id": 194677253,
        "sender_full_name": "marmeladema",
        "timestamp": 1587390618
    },
    {
        "content": "<p>Can I ask on which kind of machine do you run those tests? And in which environment? Like containers etc?</p>",
        "id": 194677320,
        "sender_full_name": "marmeladema",
        "timestamp": 1587390647
    },
    {
        "content": "<p>no containers, this is on a cloud provided machine (AMD Ryzen 5 3600 6-Core Processor)</p>",
        "id": 194677890,
        "sender_full_name": "simulacrum",
        "timestamp": 1587390934
    },
    {
        "content": "<p>in theory the machine is not shared though</p>",
        "id": 194677928,
        "sender_full_name": "simulacrum",
        "timestamp": 1587390951
    },
    {
        "content": "<p>Yeah ok that was my next question. So in theory, this machine is meant to run only rusct perf tests</p>",
        "id": 194678061,
        "sender_full_name": "marmeladema",
        "timestamp": 1587390997
    },
    {
        "content": "<p>that's the only thing <em>we</em> use it for (and IIRC, the cloud provider claims we have dedicated hardware, but I forget)</p>",
        "id": 194678193,
        "sender_full_name": "simulacrum",
        "timestamp": 1587391061
    },
    {
        "content": "<p>yep, hetzner claims it's a dedicated server</p>",
        "id": 194678457,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1587391194
    },
    {
        "content": "<p>I run <code>echo \"performance\" | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor </code> to ensure that the cpu frequency changes as little as possible. You may want to set it to <code>powersave</code> afterwards.</p>",
        "id": 194678517,
        "sender_full_name": "bjorn3",
        "timestamp": 1587391206
    },
    {
        "content": "<p>Did disabling ASLR really made a difference? For reproducibility?</p>",
        "id": 194678684,
        "sender_full_name": "marmeladema",
        "timestamp": 1587391287
    },
    {
        "content": "<p>Also is there some kind of normalization step of your metrics / numbers? Or, if you change hardware, then all the data becomes irrelevant?</p>",
        "id": 194678747,
        "sender_full_name": "marmeladema",
        "timestamp": 1587391320
    },
    {
        "content": "<p>I believe all data was removed when the hardware changed the last time.</p>",
        "id": 194678883,
        "sender_full_name": "bjorn3",
        "timestamp": 1587391365
    },
    {
        "content": "<p>Note that that due to numerous microarchitectural features in modern CPUs, wall clock time (and other metrics correlated with it) can vary greatly depending on details of the overall code and data layout (e.g. sizes as well as relative and absolute addresses of individual code sections and heap/stack allocations) that are almost guaranteed to be perturbed as side effect of essentially any code change. So it's questionable if \"stable\" performance measurements across code changes that should be performance-neutral are even possible to achieve.</p>",
        "id": 194679108,
        "sender_full_name": "Hanna Kruppe",
        "timestamp": 1587391467
    },
    {
        "content": "<p>yeah, we're mostly aiming for stable instruction counts (or as much as possible)</p>",
        "id": 194679231,
        "sender_full_name": "simulacrum",
        "timestamp": 1587391535
    },
    {
        "content": "<p>ah, yes, we also set scaling governor to performance, forgot about that</p>",
        "id": 194679275,
        "sender_full_name": "simulacrum",
        "timestamp": 1587391558
    },
    {
        "content": "<p>yeah I don't think anything we've done on the machine itself has had appreciable impact on reproducibility -- there's been some changes to the compiler itself or how we build it, but not beyond that</p>",
        "id": 194679368,
        "sender_full_name": "simulacrum",
        "timestamp": 1587391593
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"124289\">@Hanna Kruppe</span> yep  I understand, but still I need to monitor perf for critical projects, and i wondered how people do it</p>",
        "id": 194679428,
        "sender_full_name": "marmeladema",
        "timestamp": 1587391624
    },
    {
        "content": "<p>And ideally i'd like to understand if a change has an impact or not</p>",
        "id": 194679566,
        "sender_full_name": "marmeladema",
        "timestamp": 1587391683
    },
    {
        "content": "<p>Well, the answer is they mostly don't address this problem :) Either side-stepping it by looking at more stable but limited measures like instruction count or something higher-level, or trying to remove as much variance as possible (some techniques discussed above) and then trying to account for the remaining \"noise\" with rules of thumb, often with questionable results IMO. The only rigorous approach I know of it stabilizer <br>\n(&lt;<a href=\"https://people.cs.umass.edu/~emery/pubs/stabilizer-asplos13.pdf\" title=\"https://people.cs.umass.edu/~emery/pubs/stabilizer-asplos13.pdf\">https://people.cs.umass.edu/~emery/pubs/stabilizer-asplos13.pdf</a>&gt;) but as far as I can tell it hasn't achieved much adoption.</p>",
        "id": 194680057,
        "sender_full_name": "Hanna Kruppe",
        "timestamp": 1587391931
    },
    {
        "content": "<p>Thank you for input and to the paper, i'll read and share that :)</p>",
        "id": 194680445,
        "sender_full_name": "marmeladema",
        "timestamp": 1587392133
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"124289\">@Hanna Kruppe</span> i gave a quick read to the paper, its interesting. I lack the knowledge in statistiscs though^^ Its sad that <a href=\"https://github.com/ccurtsinger/stabilizer\" title=\"https://github.com/ccurtsinger/stabilizer\">https://github.com/ccurtsinger/stabilizer</a> is unmaintained :(</p>",
        "id": 194708101,
        "sender_full_name": "marmeladema",
        "timestamp": 1587403960
    }
]