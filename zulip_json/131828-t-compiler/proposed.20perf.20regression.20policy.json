[
    {
        "content": "<p>Hey <span class=\"user-mention\" data-user-id=\"224872\">@rylev</span>, <span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span>, <span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span>, <span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span> and anyone else who is interested:</p>\n<p>I was thinking about our policy around compilation time regressions or lack thereof. I think it'd be useful if we had some criteria for when we deem regressions acceptable and a simple policy to make people aware when we are landing a known regression and why. Here is my first attempt at a quick write-up:</p>\n<p><a href=\"https://hackmd.io/pMbSMx-PTnCmKlhY3X-Wqg\">https://hackmd.io/pMbSMx-PTnCmKlhY3X-Wqg</a></p>\n<p>I don't have the time to carry this over the finish line, but I thought maybe one of you might like to take a look and turn it into an MCP and (eventually) an edit to forge.</p>\n<p>I know I would appreciate having a simple policy like this! It's always rather hard to decide when it's ok to r+ such a case.</p>",
        "id": 240442028,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1622109028
    },
    {
        "content": "<p>Generally seems like a good start. I think building in explicit check point of <em>some</em> positive feedback is good, and one strategy might be to have the PR i-nominated - anything that regresses would then be discussed. We shouldn't be landing regressions often enough to make that a problem, I think.</p>",
        "id": 240446439,
        "sender_full_name": "simulacrum",
        "timestamp": 1622111758
    },
    {
        "content": "<p>Interesting \"4th\" reason worth talking about: \"improving rustc compilation time\" - this one likely wanting a cost/benefit with the runtime regressions.</p>",
        "id": 240446572,
        "sender_full_name": "simulacrum",
        "timestamp": 1622111833
    },
    {
        "content": "<p>Looks good - I'm a little worried about the 10 day wait period for the general reason of not wanting to discourage people from running the perf test in the first place. I don't want us to get into a situation where people (even subconsciously) avoid running the perf suite because they don't want to risk having their changes delayed.</p>",
        "id": 240450509,
        "sender_full_name": "rylev",
        "timestamp": 1622114258
    },
    {
        "content": "<p>I think we can easily make the proposed process here work well with any changes that we make to the process automation (which we discussed a few weeks back but which has been implemented yet due to lack of time)</p>",
        "id": 240450600,
        "sender_full_name": "rylev",
        "timestamp": 1622114305
    },
    {
        "content": "<p>The risk is already there though, isn't it?</p>",
        "id": 240451306,
        "sender_full_name": "nagisa",
        "timestamp": 1622114818
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224872\">rylev</span> <a href=\"#narrow/stream/131828-t-compiler/topic/proposed.20perf.20regression.20policy/near/240450509\">said</a>:</p>\n<blockquote>\n<p>Looks good - I'm a little worried about the 10 day wait period for the general reason of not wanting to discourage people from running the perf test in the first place. I don't want us to get into a situation where people (even subconsciously) avoid running the perf suite because they don't want to risk having their changes delayed.</p>\n</blockquote>\n<p>I tend to thnk the 10 day was too much</p>",
        "id": 240464601,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1622121159
    },
    {
        "content": "<p>but maybe if we have some way to \"affirmatively\" decide faster, that's good</p>",
        "id": 240464636,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1622121182
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/131828-t-compiler/topic/proposed.20perf.20regression.20policy/near/240446572\">said</a>:</p>\n<blockquote>\n<p>Interesting \"4th\" reason worth talking about: \"improving rustc compilation time\" - this one likely wanting a cost/benefit with the runtime regressions.</p>\n</blockquote>\n<p>i.e., sometimes improvng rustc time may harm other benchmarks?</p>",
        "id": 240464694,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1622121220
    },
    {
        "content": "<p>we can probably generalize that to:</p>",
        "id": 240464699,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1622121223
    },
    {
        "content": "<ul>\n<li>uneven impact</li>\n</ul>",
        "id": 240464708,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1622121227
    },
    {
        "content": "<p>Yeah, I think the interesting aspect of uneven impact is we don't have a good weighting strategy</p>",
        "id": 240466716,
        "sender_full_name": "simulacrum",
        "timestamp": 1622122057
    },
    {
        "content": "<p>Wait, the first sentence of this doc is \"We try to avoid regressing compilation time.” This whole doc is solely about performance regressions <em>to</em> compilation time</p>",
        "id": 240468865,
        "sender_full_name": "pnkfelix",
        "timestamp": 1622122922
    },
    {
        "content": "<p>right?</p>",
        "id": 240468881,
        "sender_full_name": "pnkfelix",
        "timestamp": 1622122923
    },
    {
        "content": "<p>As in, we don’t even properly keep running metrics on the performance of the code we generate</p>",
        "id": 240468959,
        "sender_full_name": "pnkfelix",
        "timestamp": 1622122953
    },
    {
        "content": "<p>(so its a little weird to add it as a trade-off that we are in a good position to weigh. I wish we <em>were</em> better able to weigh such trade-offs)</p>",
        "id": 240469063,
        "sender_full_name": "pnkfelix",
        "timestamp": 1622122993
    },
    {
        "content": "<p>I was referring to compile time (of third party code) as different from rustc itself</p>",
        "id": 240470762,
        "sender_full_name": "simulacrum",
        "timestamp": 1622123701
    },
    {
        "content": "<p>I think the \"uneven impact\" question is one of the hardest to answer for me personally. If a PR is all green, it's very easy to r+ or if it's all red, it's very easy to say \"we can't land this\" and often fairly easy to say \"we can land because of <code>x</code> reason\" but when a PR is mixed it's very hard for me to tell if we should be approving it. This is especially true in cases where the purpose of the PR is to improve performance. I often personally weigh the \"real world\" crate benchmarks higher than the synthetic ones but even then, you can still have uneven results.</p>",
        "id": 240471643,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1622124049
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/131828-t-compiler/topic/proposed.20perf.20regression.20policy/near/240468959\">said</a>:</p>\n<blockquote>\n<p>As in, we don’t even properly keep running metrics on the performance of the code we generate</p>\n</blockquote>\n<p>I've been occasionally concerned that the fact we can only measure compile time and not runtime performance means that we could be substantially regressing the other side for a 1% compile-time speedup.  This is less true for check builds, but it's not hard for a change to a library something to expose more optimization opportunities but get reported as a regression because all of a sudden LLVM is doing more work, making compilation slower.</p>",
        "id": 240496198,
        "sender_full_name": "scottmcm",
        "timestamp": 1622133565
    },
    {
        "content": "<p>I think that's a good example of where we should absolutely allow \"regressions\": additional optimizations.</p>",
        "id": 240497697,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1622134246
    },
    {
        "content": "<p>In particular, if a regression only applies to release builds, and not to build with optimization disabled, and it has a runtime improvement that seems worth the cost, that seems entirely fine.</p>",
        "id": 240497804,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1622134300
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116009\">nikomatsakis</span> <a href=\"#narrow/stream/131828-t-compiler/topic/proposed.20perf.20regression.20policy/near/240442028\">said</a>:</p>\n<blockquote>\n<p>I was thinking about our policy around compilation time regressions or lack thereof. I think it'd be useful if we had some criteria for when we deem regressions acceptable and a simple policy to make people aware when we are landing a known regression and why.</p>\n</blockquote>\n<p>My recent <code>?</code> PR might make an interesting case study.  It was green on some, but red on more, and I'm not sure it was a perfect match for any of the listed categories.  (I wouldn't call it a soundness fix, an improvement to the compiler architecture, or a simplification.)  But also the change actually generated <em>less</em> MIR, so there wasn't an obvious place to improve anything -- it would have needed a perf improvement to trait solving or something, but that also doesn't feel like it should be the same PR.</p>\n<p>I do think it could have made sense to wait if it was, say, a 25% regression, but I don't know what to do about a maybe 0.8% regression like that.  Because it's certainly also true that we don't want to just allow a whole bunch of individually-tiny regressions to add up...</p>",
        "id": 240497809,
        "sender_full_name": "scottmcm",
        "timestamp": 1622134303
    },
    {
        "content": "<p>Since I'm hitting this again, as an occasional compiler-PR-maker it'd be nice to have a more obvious \"this counts as a regression\" marker for when one would need to follow the policy. My <a href=\"https://perf.rust-lang.org/compare.html?start=bff138dbd95cec763f4def6b91bb465a26aaad9f&amp;end=4fc737828e7a2f978c30875c76c93b2f0e5d6562\">current PR results</a> have some red, some green.  None of the numbers are huge.  Is it a regression?  I don't know.</p>",
        "id": 240772352,
        "sender_full_name": "scottmcm",
        "timestamp": 1622406300
    },
    {
        "content": "<p>I don't think that's quite the right question - it does seem pretty clearly a regression, noise on those benchmarks is usually much less than 0.1% so these are likely significant changes. Whether it's acceptable is a separate question.</p>",
        "id": 240784004,
        "sender_full_name": "simulacrum",
        "timestamp": 1622426080
    },
    {
        "content": "<p>I guess then the other side is asking for is more guidance on how to look into small regressions, then.  For example, <code>many-assoc-items</code> shows 0.0% in the noise run, and 0.3% worse in my run, so ok, that sounds like maybe it's real.  And certainly if I'd been touching name resolution, or some visitor over items, or something then I'd have a place to start.  But the PR I'm doing only affects (AFAIK) equality for arrays of primitive types, and <a href=\"https://github.com/rust-lang/rustc-perf/blob/master/collector/benchmarks/many-assoc-items/src/lib.rs\">that test</a> never uses arrays nor <code>==</code>.  So coupled with other builds being reported as slightly faster, it makes it subjectively <em>feel</em> like noise.</p>",
        "id": 240787797,
        "sender_full_name": "scottmcm",
        "timestamp": 1622432267
    },
    {
        "content": "<p>We're testing compiler performance, though. Usually you can identify causes of regressions or improvements on a particular benchmark with cachegrind.</p>",
        "id": 240823847,
        "sender_full_name": "simulacrum",
        "timestamp": 1622462014
    }
]