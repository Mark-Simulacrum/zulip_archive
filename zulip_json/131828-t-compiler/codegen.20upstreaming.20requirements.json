[
    {
        "content": "<p>This isn't a question of whether rustc can be legally combined with it; it's a question of policy for targets merged into the upstream compiler.</p>",
        "id": 258325792,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634715913
    },
    {
        "content": "<p>And that policy is based on the ability to maintain, debug, and otherwise work with the target.</p>",
        "id": 258325822,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634715935
    },
    {
        "content": "<p>gutting good support for a target used by tens of thousands of developers, companies, and projects in multiple ecosystems for potential concern over unmaintainability (which is a valid concern) seems unwise, especially considering libnvvm is already battle tested, receptive to bugs, and open to temporary workarounds for things that dont work. Besides, by this proposal, people would be able to freely switch to something like the LLVM PTX backend if thats a concern, although it will probably never be as reliable as libnvvm or as well supported as a codegen which is designed around it. At some point i think the risk is worth taking considering absolutely nobody is currently seriously using rust for nvptx, because again, its completely broken</p>",
        "id": 258326468,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634716320
    },
    {
        "content": "<p>Choosing to support a proprietary target is a tradeoff. You're listing the upsides.</p>",
        "id": 258326573,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716367
    },
    {
        "content": "<p>And if people continue ignoring the LLVM ptx target and deciding that any serious work has to use libnvvm, that's a self-fulfilling prophecy.</p>",
        "id": 258326626,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716408
    },
    {
        "content": "<p>The biggest downside of supporting such a target isn't just the maintenance of that target itself.</p>",
        "id": 258326775,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716506
    },
    {
        "content": "<p>The biggest downside is opening the floodgates for <em>other</em> such targets. For instance, \"oh, if it's OK to have a proprietary codegen backend, then here's my target for a new CPU, that requires a custom fork of LLVM that's only shipped in binary form, we don't provide any open toolchain for targeting this CPU\".</p>",
        "id": 258326903,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716571
    },
    {
        "content": "<p>I do not disagree with you in saying its a risk, a risk that we may not be able to fix certain bugs, but i think you are disregarding that:</p>\n<ul>\n<li>there are ways of working around bugs, and thats already done a LOT, just ripgrep for it in cg_llvm, i dont see why libnvvm should be held to a higher standard, its not like rustc devs go directly to llvm to fix a bug themselves if they find it.</li>\n<li>The upsides of making rust a great language for gpu computing outweigh the negatives, there are plenty of companies and individuals interested in such a thing, i am currently working with a VFX supervisor on adding OptiX support for this exact reason</li>\n<li>libnvvm is not a library used by nobody, its used in nvcc, numba, julia GPU, etc, it is battle tested. Thats not to say it doesn't have bugs, but its very unlikely to have a miscompilation, and nvidia is (in my experience) pretty receptive to bugs in it</li>\n</ul>",
        "id": 258327007,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634716649
    },
    {
        "content": "<p>That's the primary reason for the written requirement that we don't support targets requiring proprietary bits in order to do code generation.</p>",
        "id": 258327008,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716651
    },
    {
        "content": "<p>We wouldn't be holding libnvvm to a higher standard, we'd be holding it to the <em>same</em> standard: must be open.</p>",
        "id": 258327134,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716706
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258326903\">said</a>:</p>\n<blockquote>\n<p>The biggest downside is opening the floodgates for <em>other</em> such targets. For instance, \"oh, if it's OK to have a proprietary codegen backend, then here's my target for a new CPU, that requires a custom fork of LLVM that's only shipped in binary form, we don't provide any open toolchain for targeting this CPU\".</p>\n</blockquote>\n<p>It doesnt have to be black and white, adding a target used by a couple of projects/companies is far less important than adding support for a gigantic ecosystem</p>",
        "id": 258327139,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634716710
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258327139\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258326903\">said</a>:</p>\n<blockquote>\n<p>The biggest downside is opening the floodgates for <em>other</em> such targets. For instance, \"oh, if it's OK to have a proprietary codegen backend, then here's my target for a new CPU, that requires a custom fork of LLVM that's only shipped in binary form, we don't provide any open toolchain for targeting this CPU\".</p>\n</blockquote>\n<p>It doesnt have to be black and white, adding a target used by a couple of projects/companies is far less important than adding support for a gigantic ecosystem</p>\n</blockquote>\n<p>Right, and the hypothetical CPU target might be an even larger ecosystem. Suppose it were the Apple M1, for instance.</p>",
        "id": 258327210,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716753
    },
    {
        "content": "<p>rust can already compile to m1 just fine AFAIK, what it can't do is any kind of serious gpu computing</p>",
        "id": 258327349,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634716812
    },
    {
        "content": "<p>I think you skipped over the point there. I'm saying, suppose a new CPU came along, something as large as the Apple M1 (there are lots of custom CPUs being developed now), and the CPU vendor said the only supported way to build for their CPU was a proprietary fork of LLVM.</p>",
        "id": 258327459,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716861
    },
    {
        "content": "<p>Down that path lies a lack of any open compilers at all.</p>",
        "id": 258327476,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716873
    },
    {
        "content": "<p>I think this is an unrealistic slippery slope</p>",
        "id": 258327527,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634716905
    },
    {
        "content": "<p>I think it's an <em>extremely</em> realistic scenario that has happened in the past and will happen again.</p>",
        "id": 258327584,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716923
    },
    {
        "content": "<p>CPU vendors already push proprietary compilers with custom optimizations that they don't share. The only reason it's not a more widespread problem is that there aren't many completely custom architectures out there (e.g. Apple M1 is ARM with custom extensions, not a completely custom architecture), so it's possible to support them even without the CPU vendor's assistance.</p>",
        "id": 258327691,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634716988
    },
    {
        "content": "<p>In the past, when far more CPU architectures existed, CPU vendors were even more protective of the ability to build compilers for \"their\" platform.</p>",
        "id": 258327730,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717014
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258327007\">said</a>:</p>\n<blockquote>\n<p>I do not disagree with you in saying its a risk, a risk that we may not be able to fix certain bugs, but i think you are disregarding that:</p>\n<ul>\n<li>there are ways of working around bugs, and thats already done a LOT, just ripgrep for it in cg_llvm, i dont see why libnvvm should be held to a higher standard, its not like rustc devs go directly to llvm to fix a bug themselves if they find it.</li>\n</ul>\n</blockquote>\n<p>They do. Look at these for example: <a href=\"https://reviews.llvm.org/D111276\">https://reviews.llvm.org/D111276</a> <a href=\"https://reviews.llvm.org/D111681\">https://reviews.llvm.org/D111681</a> <a href=\"https://reviews.llvm.org/D111493\">https://reviews.llvm.org/D111493</a></p>",
        "id": 258327825,
        "sender_full_name": "Urgau",
        "timestamp": 1634717048
    },
    {
        "content": "<p>rustc is not copyleft, and nothing stops you from building a fork of it that links to proprietary code generation libraries. This is a question of what policy we should apply for accepting and supporting new targets upstream.</p>",
        "id": 258327826,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717048
    },
    {
        "content": "<p>Building a fork of rustc would just reinvent nvcc and make the problem even worse</p>",
        "id": 258327900,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717090
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258327900\">said</a>:</p>\n<blockquote>\n<p>Building a fork of rustc would just reinvent nvcc and make the problem even worse</p>\n</blockquote>\n<p>I agree entirely. I just said \"nothing stops you\"; I wasn't suggesting you <em>should</em>, or that it'd be a good idea.</p>",
        "id": 258327954,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717120
    },
    {
        "content": "<p>I mean yeah i agree that slippery slope <em>is</em> a problem, but i don't think it is such a big problem that it should stop rust from expanding into big fields like gpu computing</p>",
        "id": 258328050,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717165
    },
    {
        "content": "<p>From my perspective, \"rust has great support for non-proprietary GPUs, and poor support for nvidia GPUs\" is a perfectly acceptable outcome.</p>",
        "id": 258328083,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717184
    },
    {
        "content": "<p>And one way to fix that would be to contribute to the LLVM backend, or write a new one.</p>",
        "id": 258328109,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717199
    },
    {
        "content": "<p>besides, the codegen <em>could</em> potentially in the future expand to just be an llvm-gpu codegen that targets generic gpu-friendly LLVM IR, although this is unlikely</p>",
        "id": 258328118,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717207
    },
    {
        "content": "<p>You made the comment elsewhere that reworking the LLVM ptx backend to be good enough would require turning it into a clone of libnvvm, and you seemed to see that as an argument for just using libnvvm. But an Open Source clone of libnvvm would be a <em>good</em> outcome.</p>",
        "id": 258328185,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717246
    },
    {
        "content": "<p>Well the truth of the matter is AMD is absolutely horrific to target through LLVM, the only reason i was able to do this was that libnvvm is a thing that works</p>",
        "id": 258328216,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717262
    },
    {
        "content": "<p>Well i also learned a lot of things since then, and i still think that using libnvvm is 100x less work than making llvm work and be on the same level of optimization and reliability as libnvvm</p>",
        "id": 258328313,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717302
    },
    {
        "content": "<p>I'm not a GPU expert; I can't speak to the quality of either the AMD or NVIDIA backends for LLVM. I know I've seen people use the AMD backend successfully, and I know it's being actively worked on and used by things like Mesa.</p>",
        "id": 258328317,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717304
    },
    {
        "content": "<p>There is a difference between using in small examples that are painful to build and serious projects</p>",
        "id": 258328346,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717325
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258328313\">said</a>:</p>\n<blockquote>\n<p>Well i also learned a lot of things since then, and i still think that using libnvvm is 100x less work than making llvm work and be on the same level of optimization and reliability as libnvvm</p>\n</blockquote>\n<p>Does that 100x take into account being able to upstream one backend and having to maintain a fork for the other?</p>",
        "id": 258328350,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717330
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258328346\">said</a>:</p>\n<blockquote>\n<p>There is a difference between using in small examples that are painful to build and serious projects</p>\n</blockquote>\n<p>I would consider Mesa a serious project.</p>",
        "id": 258328391,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717354
    },
    {
        "content": "<p>CUDA is not just libnvvm and some PTX, cuda is a gigantic ecosystem of libraries and driver api features</p>",
        "id": 258328401,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717361
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258328401\">said</a>:</p>\n<blockquote>\n<p>CUDA is not just libnvvm and some PTX, cuda is a gigantic ecosystem of libraries and driver api features</p>\n</blockquote>\n<p>I'm aware, and I consider that unfortunate and disappointing. I'll continue to cheer on the competing GPU vendors trying to build open standards. But regardless, a rustc backend that targets CUDA is entirely possible, and even upstreamable, as long as it doesn't use a proprietary codegen library.</p>",
        "id": 258328532,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717417
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258328350\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258328313\">said</a>:</p>\n<blockquote>\n<p>Well i also learned a lot of things since then, and i still think that using libnvvm is 100x less work than making llvm work and be on the same level of optimization and reliability as libnvvm</p>\n</blockquote>\n<p>Does that 100x take into account being able to upstream one backend and having to maintain a fork for the other?</p>\n</blockquote>\n<p>Libnvvm has very rarely been the bottleneck in trying to get my project to work, its actually usually been either LLVM or rustc itself. I would consider libnvvm mostly easy to work with, good luck trying to get the LLVM PTX backend to build on windows</p>",
        "id": 258328604,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717458
    },
    {
        "content": "<p>There is also a reason that almost all projects use nvcc and not clang for building their CUDA code, libnvvm is simply more reliable, its been battle tested for years, and it can keep compatability for things like compute capabilities much better than something which is mostly guessing</p>",
        "id": 258328855,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717585
    },
    {
        "content": "<p>I dont like libnvvm/cuda not being open just as much as the next person, but cuda is simply the best tool we have for GPU computing, and i think support for it that matches and exceeds CUDA C++ is a great next step for rust</p>",
        "id": 258328988,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717651
    },
    {
        "content": "<p>That sounds like a very good argument for making the case to either improve LLVM's CUDA support or convince NVIDIA to release/upstream more of their tooling.</p>",
        "id": 258329044,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717697
    },
    {
        "content": "<p>Well that hasn't really worked out for many projects, if it did, then i wouldn't have needed to start my project in the first place, or other projects like numba having to use libnvvm</p>",
        "id": 258329201,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717777
    },
    {
        "content": "<p>I'm <em>much</em> more concerned with Rust's sustainability and long-term prospects than I am about its popularity in one specific field.</p>",
        "id": 258329267,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717822
    },
    {
        "content": "<p>And the target policy was written with that in mind.</p>",
        "id": 258329288,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717831
    },
    {
        "content": "<p>What is preventing rust from long term swapping out libnvvm for the LLVM PTX backend if it turns out to work on the same level? There is nothing saying we are permanently stuck with libnvvm if we adopt this</p>",
        "id": 258329353,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717873
    },
    {
        "content": "<p>What is preventing it from using LLVM in the first place?</p>",
        "id": 258329402,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717888
    },
    {
        "content": "<p>\"Oh, we could use the Open Source backend if it got good enough\" sounds like a great recipe for never investing anything into making it good enough.</p>",
        "id": 258329424,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717905
    },
    {
        "content": "<p>If it can be made good enough, then it can be made good enough before upstreaming support for it. If it can't, then let's not pretend it can as a way of getting a different backend upstream.</p>",
        "id": 258329522,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634717967
    },
    {
        "content": "<p>optimizations not on the same level, compatability for compute capabilities not matching, not as reliable as libnvvm, lack of features like lazy loading (which is massively helpful on the gpu)</p>",
        "id": 258329537,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634717976
    },
    {
        "content": "<p>That sounds like a clear list of differences. It doesn't address any of the actual reasons for this policy.</p>",
        "id": 258329646,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718018
    },
    {
        "content": "<p>The amount of people who:</p>\n<ul>\n<li>are deeply familiar with llvm</li>\n<li>dont like closed source stuff enough to not use libnvvm</li>\n<li>are deeply familiar with cuda internals</li>\n</ul>\n<p>is basically next to zero</p>",
        "id": 258329716,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718071
    },
    {
        "content": "<p>The relevant number of people is \"Rust users and developers\", not \"CUDA developers who are not yet Rust users\".</p>",
        "id": 258329742,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718093
    },
    {
        "content": "<p>Rust users are Rust users, not LLVM C++ users</p>",
        "id": 258329770,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718108
    },
    {
        "content": "<p>You're not going to be successful arguing against a policy without taking into account the reasons for the policy. Whether you care about those reasons or not, other people do, and the policy was written for a reason.</p>",
        "id": 258329843,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718130
    },
    {
        "content": "<p>I am taking into account those reasons, i think the concerns are valid but outweighed by the benefits</p>",
        "id": 258329890,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718157
    },
    {
        "content": "<p>And every single person arguing for a proprietary backend will always feel the benefits of their new backend outweigh the problems.</p>",
        "id": 258329943,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718194
    },
    {
        "content": "<p>And even if that <em>were</em> true for that one backend, which may or may not be the case, part of the problem is that there'd be dozens more right behind it citing the precedent.</p>",
        "id": 258329984,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718220
    },
    {
        "content": "<p>To get rust to be reliable in gpu computing using the LLVM PTX backend would mean youd need to get someone who is wiling to do this and is deeply familiar with every part of the stack, rustc, llvm, and PTX. That is well, i think thats literally nobody</p>",
        "id": 258330076,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718263
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258330076\">said</a>:</p>\n<blockquote>\n<p>To get rust to be reliable in gpu computing using the LLVM PTX backend would mean youd need to get someone who is wiling to do this and is deeply familiar with every part of the stack, rustc, llvm, and PTX. That is well, i think thats literally nobody</p>\n</blockquote>\n<p>You seem to be taking it as a given that there should be an \"and therefore we should compromise in order to get a CUDA backend\" after that.</p>",
        "id": 258330123,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718292
    },
    {
        "content": "<p>Rather than \"and that's why we don't support CUDA\".</p>",
        "id": 258330140,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718299
    },
    {
        "content": "<p>As far as I can tell, you have all the relevant skills except for \"willing to do this\". ;)</p>",
        "id": 258330197,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718333
    },
    {
        "content": "<p>No i am explaining why my project exists and other projects that try to do it with the LLVM PTX backend don't</p>",
        "id": 258330202,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718337
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258330197\">said</a>:</p>\n<blockquote>\n<p>As far as I can tell, you have all the relevant skills except for \"willing to do this\". ;)</p>\n</blockquote>\n<p>I dont think i could be paid enough to debug issues in C++ tools, especially LLVM. I am familiar with rustc and the PTX ISA to a decent degree but basically nothing in how LLVM works</p>",
        "id": 258330316,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718385
    },
    {
        "content": "<blockquote>\n<p>I dont think i could be paid enough to debug issues in C++ tools, especially LLVM.</p>\n</blockquote>\n<p>You have my complete and total sympathy on that front.</p>",
        "id": 258330353,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718404
    },
    {
        "content": "<p>I'd still rather debug a C++ tool than a binary I have no source for, though. ;)</p>",
        "id": 258330370,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718416
    },
    {
        "content": "<p>But all else being equal, I'll happily avoid both.</p>",
        "id": 258330390,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718430
    },
    {
        "content": "<p>You could always add support to one of the Rust code generation projects, like cranelift; then you wouldn't have to touch C++ at all.</p>",
        "id": 258330436,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718461
    },
    {
        "content": "<p>I have encountered 2 issues in libnvvm (that dont result in the verifier catching it):</p>\n<ul>\n<li>libnvvm doesnt like integers that are \"irregular\", i.e. not i1, i8, i16, i32, or i64. This was easily patched by using vector types and bitcasts.</li>\n<li>libnvvm segfaults if you provide more than one module with debug info. This is well documented in the NVVM IR standard and it was my fault.</li>\n</ul>",
        "id": 258330538,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718504
    },
    {
        "content": "<p>Compared to the issues ive had to go through with llvm or rustc? libnvvm is an absolute pleasure to work with</p>",
        "id": 258330585,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718535
    },
    {
        "content": "<p>\"it's easy to work with\" is still entirely orthogonal to the issue.</p>",
        "id": 258330650,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718570
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258330436\">said</a>:</p>\n<blockquote>\n<p>You could always add support to one of the Rust code generation projects, like cranelift; then you wouldn't have to touch C++ at all.</p>\n</blockquote>\n<p>Optimizations matter even more on the gpu, if we didn't have that we would just get laughed at for our kernels being 100x slower. This is __Especially__ important because noalias is extremely helpful for GPU code</p>",
        "id": 258330695,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718597
    },
    {
        "content": "<p>/me personally hopes that one day MIR optimizations cover everything LLVM currently does.</p>",
        "id": 258330797,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718634
    },
    {
        "content": "<p>(Though, of course, they're not to that point yet.)</p>",
        "id": 258330811,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718644
    },
    {
        "content": "<p>FWIW, I'm not trying to be <em>gratuitously</em> difficult here. I'm trying to be clear on why we have this policy, what it's protecting against, and ways to potentially address it.</p>",
        "id": 258330817,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718649
    },
    {
        "content": "<p>Yeah and i totally understand, but i just dont think they are as big of an issue as you think they are</p>",
        "id": 258330865,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718675
    },
    {
        "content": "<p>I understand the case you're making. And I appreciate the frustration. I <em>have</em> been in the position of \"there's no technical reason against this, why can't the policy change\" before.</p>",
        "id": 258331007,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718743
    },
    {
        "content": "<p>Based on the discussions that happened when the policy was written, \"but this backend/target is really important\" is not likely to be a successful argument.</p>",
        "id": 258331013,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718749
    },
    {
        "content": "<p>The policy was written the way it was <em>because</em> we expected arguments of the form \"but this backend/target is really important\".</p>",
        "id": 258331048,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718776
    },
    {
        "content": "<p>But i don't think we will ever find someone who would be wiling to sink hundreds of hours of work into the LLVM PTX backend to make it match libnvvm AND be wiling to make a usable rustc backend AND be wiling to make gpu-side crates that work with the backend</p>",
        "id": 258331069,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718788
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258331069\">said</a>:</p>\n<blockquote>\n<p>But i don't think we will ever find someone who would be wiling to sink hundreds of hours of work into the LLVM PTX backend to make it match libnvvm AND be wiling to make a usable rustc backend AND be wiling to make gpu-side crates that work with the backend</p>\n</blockquote>\n<p>If that's the case, then it sounds like we won't have upstream CUDA support.</p>",
        "id": 258331108,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718816
    },
    {
        "content": "<p>Aren't some of those pieces independent and could be attacked by separate developers?</p>",
        "id": 258331189,
        "sender_full_name": "The 8472",
        "timestamp": 1634718851
    },
    {
        "content": "<p>Yes but the pool of people who are wiling to work on the LLVM PTX backend is low enough already</p>",
        "id": 258331249,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718892
    },
    {
        "content": "<p>That's still a weaker argument than saying that it takes a single person which doesn't exist.</p>",
        "id": 258331286,
        "sender_full_name": "The 8472",
        "timestamp": 1634718912
    },
    {
        "content": "<p>As an aside, there seems to be a GCC PTX backend as well.</p>",
        "id": 258331315,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718928
    },
    {
        "content": "<p>I don't know its quality, either.</p>",
        "id": 258331331,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634718938
    },
    {
        "content": "<p>You need a rustc backend that can make gpu friendly IR, targetting a ptx backend isnt enough</p>",
        "id": 258331353,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718957
    },
    {
        "content": "<p>you can start with a bad one and improve it</p>",
        "id": 258331418,
        "sender_full_name": "The 8472",
        "timestamp": 1634718972
    },
    {
        "content": "<p>especially since a lot of cuda things need explicit handling in the codegen</p>",
        "id": 258331460,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634718999
    },
    {
        "content": "<p>I mean, as someone who has done it, its a tremendous amount of work, and it would probably be even worse with gcc because of the issues that cg_gcc is going through with cg_ssa</p>",
        "id": 258331545,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634719048
    },
    {
        "content": "<p>and debugging who's fault (rustc or gcc) it is that code is miscompiling would be a nightmare</p>",
        "id": 258331621,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634719082
    },
    {
        "content": "<p>FWIW, I think you've made a <em>very</em> effective case that there should be a GPU-specific backend, and potentially even a CUDA-specific backend if a general GPU backend can't easily be made to target various GPU architectures.</p>",
        "id": 258331659,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634719101
    },
    {
        "content": "<p>Oh absolutely, and ive effectively done that, it just uses libnvvm because its the best option</p>",
        "id": 258331698,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634719128
    },
    {
        "content": "<p>but it wouldnt be impossible to make it abstract over the llvm ptx backend or whatever in the future</p>",
        "id": 258331729,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634719145
    },
    {
        "content": "<p>but the thing is, in my experience, every single backend will have a phase of \"oh my god why is this thing {segfaulting|miscompiling|asserting}\", and that is a solid couple months of debugging to get it to be kind of usable</p>",
        "id": 258331827,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634719186
    },
    {
        "content": "<p>and this doesn't even factor in miscompilation or errors inside of the backend itself, i was lucky because libnvvm doesnt really miscompile much, but i know things like amdgpu have so many odd things that cause them to break</p>",
        "id": 258331974,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634719261
    },
    {
        "content": "<p>and NVVM IR and llvm are not the same thing, NVVM doesnt like some things, but the LLVM PTX backend will miscompile if you use some things (instead of documenting what is and isnt allowed), which is why libnvvm again is so good, because NVVM IR is formalized</p>",
        "id": 258332207,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634719378
    },
    {
        "content": "<p>... and llvm doesnt have certain features like lazy loading which are essential for rust so you don't end up with 12mb ptx files</p>",
        "id": 258332436,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634719490
    },
    {
        "content": "<p>which is also why libnvvm doesnt need LTO, because it loads everything into a single module and optimizes it, so i can just lazy-load dependencies and end up with a relatively tiny, hyper-optimized PTX file that doesnt even need LTO</p>",
        "id": 258332539,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634719549
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> After looking at the LLVM PTX backend, it seems like they explicitly designed it so that its compatible with NVVM IR, so it may possibly work, but the concern i have is that i'd still like to have an option for users to use libnvvm over the LLVM PTX backend assuming that the codegen was upstreamed. Would you be wiling to upstream a backend which by default uses the LLVM PTX backend but also has an option to use libnvvm?</p>",
        "id": 258408863,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634750881
    },
    {
        "content": "<p>especially since it seems to be lacking some intrinsics which i provide already, so i think id have to gate those off for just libnvvm somehow</p>",
        "id": 258409019,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634750920
    },
    {
        "content": "<p>If libnvvm can't be upstreamed at all, then at least having the GPU LLVM IR part upstreamed would help a lot, since it would mean i don't need to do as much work in terms of keeping track of breakage in cg_ssa every week. And the GPU LLVM IR backend could then be leveraged by an LLVM PTX codegen backend, and my libnvvm backend. So the hard part still gets upstreamed, we have an upstreamed way of generating ptx code (that hopefully works), and we still have the option for users to use libnvvm (albeit not upstreamed so prob uglier to use)</p>",
        "id": 258410968,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634751588
    },
    {
        "content": "<p>So essentially:</p>\n<ul>\n<li>(upstream) rustc_codegen_gpu: just acts as a middleman generating LLVM IR that works on CUDA GPUs and offers interfaces to crates like cuda_std for implementing stuff like shared mem and stuff, also handles things like marking functions as gpu kernels.</li>\n<li>(upstream) rustc_codegen_llvmptx: outputs PTX using the gpu codegen and the LLVM PTX backend</li>\n<li>rustc_codegen_nvvm: outputs PTX using libnvvm, lowering the LLVM 13 IR given by the gpu codegen</li>\n</ul>",
        "id": 258411668,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634751842
    },
    {
        "content": "<p>then maybe in the future if the llvm ptx backend turns out to be just as good in basically all cases, the nvvm backend will be deprecated</p>",
        "id": 258411895,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634751915
    },
    {
        "content": "<p>That path seems potentially viable. (Nit: if it's called <code>rustc_codegen_gpu</code> it should be a generic layer for all kinds of GPUs, similar to rust-gpu; if it only generates code for CUDA then it should be <code>rustc_codegen_cuda</code> or similar.)</p>",
        "id": 258413280,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634752437
    },
    {
        "content": "<p>Yeah i couldnt think of a good name <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span></p>",
        "id": 258413706,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634752583
    },
    {
        "content": "<p>That trio of projects otherwise seems fine.</p>",
        "id": 258413723,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634752586
    },
    {
        "content": "<p>To answer your previous question, I'd be <em>nervous</em> about the idea of a \"default LLVM, optional libnvvm\" backend. I can imagine it working, but it'd need to have careful constraints like \"must always pass the testsuite with LLVM, needs to be usefully viable with LLVM\".</p>",
        "id": 258413859,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634752640
    },
    {
        "content": "<p>great, after i release my project ill prob experiment with the llvm ptx backend a bit, id prob need to reimplement lazy loading because without it its kind of impossible to match libnvvm</p>",
        "id": 258413871,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634752644
    },
    {
        "content": "<p>Wym be usefully viable with llvm?</p>",
        "id": 258414039,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634752700
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258414039\">said</a>:</p>\n<blockquote>\n<p>Wym be usefully viable with llvm?</p>\n</blockquote>\n<p>I mean that I'd be nervous about whether a backend that is designed for both LLVM and NVVM would have the LLVM portion languish from disuse or otherwise break without people noticing. The LLVM backend might not be optimal, but it would need to be <em>useful</em>.</p>",
        "id": 258414707,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634752898
    },
    {
        "content": "<p>I would just like to have a way to fall back to libnvvm (which should be the reference of what the ptx should look like since its the most battle tested one) in case we find a weird miscompilation or bug in the llvm ptx backend we cant immediately solve</p>",
        "id": 258414775,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634752918
    },
    {
        "content": "<p>(In other words, it'd need to be useful enough to justify the target by itself, because the NVVM alternative doesn't add weight to the target's justification.)</p>",
        "id": 258414848,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634752933
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258414775\">said</a>:</p>\n<blockquote>\n<p>I would just like to have a way to fall back to libnvvm (which should be the reference of what the ptx should look like since its the most battle tested one) in case we find a weird miscompilation or bug in the llvm ptx backend we cant immediately solve</p>\n</blockquote>\n<p>Sure, I get that. And I can even see that being useful for people doing LLVM development, so that they can compare and optimize.</p>",
        "id": 258414919,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634752956
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258414707\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/258414039\">said</a>:</p>\n<blockquote>\n<p>Wym be usefully viable with llvm?</p>\n</blockquote>\n<p>I mean that I'd be nervous about whether a backend that is designed for both LLVM and NVVM would have the LLVM portion languish from disuse or otherwise break without people noticing. The LLVM backend might not be optimal, but it would need to be <em>useful</em>.</p>\n</blockquote>\n<p>There is only a single part of my codegen that is libnvvm-specific, and that is the linking step, but it only does a couple things:</p>\n<ul>\n<li>collects llvm bitcode from rlibs</li>\n<li>lazy loads dependencies in order</li>\n<li>loads libdevice</li>\n<li>calls libnvvm to output ptx</li>\n</ul>\n<p>But the core of the work is not in libnvvm its in the actual LLVM IR (well, bitcode) generation, which would be shared</p>",
        "id": 258415031,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634752997
    },
    {
        "content": "<p>Ideally we would run a shared test suite which pits both backends against eachother, if one makes valid ptx and the other doesn't, then well, both teams (llvm or nvidia) have something to work off of, and we could also make benchmarks to check for performance disparities</p>",
        "id": 258415243,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634753076
    },
    {
        "content": "<p>(I'd also want to know if the NVVM license has any prohibition on reverse-engineering, and if they'd consider such heads-up testing to be reverse-engineering...)</p>",
        "id": 258415327,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1634753110
    },
    {
        "content": "<p>I am not sure honestly, but formats cannot be copyrighted, so just looking at what ptx libnvvm makes and guessing what its doing should be fine</p>",
        "id": 258415465,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634753162
    },
    {
        "content": "<p>this is the CUDA EULA it is under: <a href=\"https://docs.nvidia.com/cuda/eula/index.html\">https://docs.nvidia.com/cuda/eula/index.html</a></p>",
        "id": 258415503,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634753170
    },
    {
        "content": "<p>At the very least we would have a default nvptx target that isnt completely broken, it would be very helpful to have users use the nvptx(64) target without libnvvm so that they can catch anything that breaks. If their project doesnt compile under the ptx backend, then well, they can easily swap back to using libnvvm. This is a great way to catch any bugs inside of the llvm ptx backend, but still have a way to fall back to something which is pretty much guaranteed to work</p>",
        "id": 258416346,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634753504
    },
    {
        "content": "<p>However, the issue with it not linking on windows because of dylib stuff needs to be resolved, otherwise it wont work at all</p>",
        "id": 258416477,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634753539
    },
    {
        "content": "<p>IIRC the only way that it worked is if you compiled rustc with static and not dynamic LLVM, which was a gigantic pain to do</p>",
        "id": 258416620,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1634753603
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> I was thinking, per the target tier policy:</p>\n<blockquote>\n<p>Targets should not require proprietary (non-FOSS) components to link a functional binary or library.</p>\n</blockquote>\n<p>Doesn't this mean it would be perfectly fine for me to try to use libnvvm by default then fall back to the LLVM PTX backend if not found? This would pass just fine for that sort of policy because it isn't strictly requiring libnvvm since it can fall back to a non-proprietary solution.</p>",
        "id": 262912369,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638079904
    },
    {
        "content": "<p>Other than this policy, libnvvm imposes no licensing requirements, my entire project is MIT OR Apache-2.0, and the PTX ISA (the end \"binary\") generated in the end is open and well-specified, so it imposes zero requirements</p>",
        "id": 262912409,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638079962
    },
    {
        "content": "<p>So, as mentioned previously, that may theoretically pass the policy, but my primary concern is whether the ptx backend would actually work in practice, and work well enough to actually use. Given how broken you've described that target as, and the lack of interest you've expressed in changing that, what happens if that fallback breaks?</p>",
        "id": 262912472,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080087
    },
    {
        "content": "<p>Well if it breaks then we are just back to normal rustc lol, anyways, after doing more research, i think the issue is cg_llvm/rustc, not the ptx backend.</p>",
        "id": 262912487,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080142
    },
    {
        "content": "<p>This would always be a better situation than what rustc currently has</p>",
        "id": 262912527,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080183
    },
    {
        "content": "<p>(That's useful information! Does that mean with your GPU backend ptx would be more functional?)</p>",
        "id": 262912533,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080198
    },
    {
        "content": "<p>Absolutely, im hopeful it should work mostly very well because some projects use clang to compile their cuda code instead of nvcc</p>",
        "id": 262912541,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080226
    },
    {
        "content": "<p>And no, if it breaks then we potentially have a target with a working proprietary backend that people use and a broken Open Source that they can't, and a lot of people who would be upset if the target was removed as a result.</p>",
        "id": 262912550,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080276
    },
    {
        "content": "<p>Well nvptx64-nvidia-cuda has been utterly broken for ages and it has been a tier 2 target for ages</p>",
        "id": 262912592,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080304
    },
    {
        "content": "<p>what im saying is that either ways, we are at a better spot than we were for a long time</p>",
        "id": 262912603,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080348
    },
    {
        "content": "<p>because then we have concrete IR that can be reported to nvidia or LLVM as not working. Without the issues of rustc producing horrific gpu-unfriendly IR</p>",
        "id": 262912609,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080385
    },
    {
        "content": "<p>I know you don't agree with this perspective, but: all else being equal, adding a proprietary backend in-tree makes us strictly worse off than leaving it out of tree.</p>",
        "id": 262912657,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080409
    },
    {
        "content": "<p>Hence my concerns about adding a switchable backend if in practice the maintainers if it don't care about the open alternative.</p>",
        "id": 262912666,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080446
    },
    {
        "content": "<p>If it is only proprietary, if it has a fallback then no, we are just back to square one if the proprietary thing or the FOSS thing doesnt work</p>",
        "id": 262912667,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080448
    },
    {
        "content": "<p>the LLVM PTX backend is explicitly designed to be mostly equal to what libnvvm expects</p>",
        "id": 262912673,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080475
    },
    {
        "content": "<p>there is basically zero libnvvm-specific stuff in my code at this point, other than calling libnvvm</p>",
        "id": 262912678,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080489
    },
    {
        "content": "<p>previously it was quite libnvvm specific because it used lazy-loading, now it doesn't, it uses LTO-like DCE which makes a single llvm ir file that can be given to anything that can consume it</p>",
        "id": 262912717,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080526
    },
    {
        "content": "<p>So, a question: once you've generated GPU-friendly IR, to what extent are you calling LLVM/NVVM functions in a non-trivial way, and to what extent do you pretty much just call them and get output out the other end?</p>",
        "id": 262912723,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080576
    },
    {
        "content": "<p><a href=\"https://github.com/Rust-GPU/Rust-CUDA/blob/dce/crates/rustc_codegen_nvvm/src/nvvm.rs#L52\">https://github.com/Rust-GPU/Rust-CUDA/blob/dce/crates/rustc_codegen_nvvm/src/nvvm.rs#L52</a></p>",
        "id": 262912731,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080605
    },
    {
        "content": "<p>That function is the only function that calls libnvvm</p>",
        "id": 262912735,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080618
    },
    {
        "content": "<p>it could be replaced with about an equal amount of llvm calls for the nvptx backend</p>",
        "id": 262912738,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080630
    },
    {
        "content": "<p>Then in that case, I have a possible idea that might work here.</p>",
        "id": 262912778,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080663
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262912592\">said</a>:</p>\n<blockquote>\n<p>Well nvptx64-nvidia-cuda has been utterly broken for ages and it has been a tier 2 target for ages</p>\n</blockquote>\n<p>tier 2 targets have a very low functionality requirement.</p>",
        "id": 262912779,
        "sender_full_name": "Jubilee",
        "timestamp": 1638080665
    },
    {
        "content": "<p>...with some additional <a href=\"http://build.rs\">build.rs</a> logic for only <em>optionally</em> linking in libnvvm, but thats a secondary thing</p>",
        "id": 262912780,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080666
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"281757\">Jubilee</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262912779\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262912592\">said</a>:</p>\n<blockquote>\n<p>Well nvptx64-nvidia-cuda has been utterly broken for ages and it has been a tier 2 target for ages</p>\n</blockquote>\n<p>tier 2 targets have a very low functionality requirement.</p>\n</blockquote>\n<p>Well yes but ideally theyd work at least a little bit, per the docs</p>",
        "id": 262912791,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080686
    },
    {
        "content": "<p>Weeeell mostly to build host tools on, apparently they aren't even smoke tested in practice. <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span></p>",
        "id": 262912801,
        "sender_full_name": "Jubilee",
        "timestamp": 1638080717
    },
    {
        "content": "<p>Theoretically, could you make a standalone tool that loads your GPU-friendly LLVM IR from a file and invokes NVVM on it?</p>",
        "id": 262912802,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080730
    },
    {
        "content": "<p>nvptx64-nvidia-cuda:</p>\n<ul>\n<li>doesnt work on windows</li>\n<li>when it works it either errors or makes invalid/miscompiled ptx</li>\n</ul>",
        "id": 262912804,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080733
    },
    {
        "content": "<p>(Assuming your backend to generate that IR was in rustc.)</p>",
        "id": 262912808,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080762
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262912802\">said</a>:</p>\n<blockquote>\n<p>Theoretically, could you make a standalone tool that loads your GPU-friendly LLVM IR from a file and invokes NVVM on it?</p>\n</blockquote>\n<p>Id rather not, what i could do is make a generic codegen for \"nvvm ir\"</p>",
        "id": 262912823,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080763
    },
    {
        "content": "<p>but not having the libnvvm stuff upstreamed would kinda defeat the point since it could not be used on stable</p>",
        "id": 262912849,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080788
    },
    {
        "content": "<p>I was asking what's possible, not what you would ideally prefer. ;)</p>",
        "id": 262912850,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080788
    },
    {
        "content": "<p>i mean its possible yes, but impractical</p>",
        "id": 262912853,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080798
    },
    {
        "content": "<p>I have an idea that might let it be used on stable.</p>",
        "id": 262912854,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080807
    },
    {
        "content": "<p>i think cuda has something for it too, cicc.exe or something like that</p>",
        "id": 262912856,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080810
    },
    {
        "content": "<p>Oooh, an existing tool would be even better.</p>",
        "id": 262912861,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080826
    },
    {
        "content": "<p>And yeah, I don't think that the nvptx64 target should be considered acceptable.<br>\nBut mostly, as far as tier 2 targets goes, it technically just <strong>barely</strong> meets the requirements and no one has asked to downgrade it.</p>",
        "id": 262912863,
        "sender_full_name": "Jubilee",
        "timestamp": 1638080832
    },
    {
        "content": "<p>(ive never been able to get it to actually work tho lol)</p>",
        "id": 262912865,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080845
    },
    {
        "content": "<p>but id rather not separate nvvm ir, libnvvm, and llvm ptx backend into like 3 different things, not only is it confusing, but its a maintenance hell</p>",
        "id": 262912894,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080884
    },
    {
        "content": "<p>Just two things, not three.</p>",
        "id": 262912911,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080903
    },
    {
        "content": "<p>I also agree w/ Josh that a target should work in principle without proprietary code.<br>\nIt's fine if it can link against/use a proprietary tool with an argument to its invocation.</p>",
        "id": 262912913,
        "sender_full_name": "Jubilee",
        "timestamp": 1638080916
    },
    {
        "content": "<p>besides, id <em>like</em> to be able to add libnvvm or llvm-ptx backend specific things, i don't want to 100% lock myself out of being able to do that</p>",
        "id": 262912914,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080920
    },
    {
        "content": "<p>but this <em>would</em> work without libnvvm</p>",
        "id": 262912918,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080934
    },
    {
        "content": "<p>i mean, it is linking dynamically</p>",
        "id": 262912924,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080961
    },
    {
        "content": "<p>So, theoretically, suppose we merged the GPU IR backend you've worked on, and made it run the IR through LLVM normally. But we added a configuration option, available in stable, that invoked a standalone tool in place of LLVM. Any tool the user specified, with specified command-line arguments.</p>",
        "id": 262912925,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638080970
    },
    {
        "content": "<p>i wouldn't strictly need libnvvm during building the codegen</p>",
        "id": 262912927,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638080974
    },
    {
        "content": "<p>It's not about whether it's linking dynamically, really.</p>",
        "id": 262912930,
        "sender_full_name": "Jubilee",
        "timestamp": 1638080978
    },
    {
        "content": "<p>But then i would just make it try to pass libnvvm by default from cuda_builder and then its the same thing except the libnvvm stuff is moved to cuda_builder, and its less flexible for the codegen</p>",
        "id": 262912988,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081028
    },
    {
        "content": "<p>It's about whether it is <strong>actually dynamically linked</strong>, as it were.<br>\n\"what, you just said\"<br>\nah ah<br>\na thing that links dynamically against one possible thing has a statically determined need for a specific library. :^)</p>",
        "id": 262912993,
        "sender_full_name": "Jubilee",
        "timestamp": 1638081049
    },
    {
        "content": "<p>With that approach, you could write any NVVM-specific code you wanted in a standalone tool, but your GPU-friendly backend could go upstream, and the whole combination works in stable.</p>",
        "id": 262912994,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081050
    },
    {
        "content": "<p>I mean nvvm-specific codegen, for example, if the llvm ptx backend doesn't break on i128, then i dont need to run my int replacement logic i have for libnvvm</p>",
        "id": 262913008,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081092
    },
    {
        "content": "<p>It's the same thing <em>except rustc upstream isn't maintaining nvvm code</em>.</p>",
        "id": 262913011,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081104
    },
    {
        "content": "<p>Well yes but i would like to be open to doing libnvvm-specific things <em>during</em> codegen if the codegen knows it will be using libnvvm</p>",
        "id": 262913052,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081143
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"276242\">@Riccardo D'Ambrosio</span> I don't think anyone would <em>block</em> merging changes to support making the IR friendlier for other tools processing it.</p>",
        "id": 262913054,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081148
    },
    {
        "content": "<p>if its just spitting out IR it has no idea</p>",
        "id": 262913056,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081148
    },
    {
        "content": "<p><code>-C no-i128</code></p>",
        "id": 262913059,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081172
    },
    {
        "content": "<p>Hmmm</p>",
        "id": 262913069,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081194
    },
    {
        "content": "<p>Id be wiling to compromise for something like <code>-C target_backend=libnvvm</code></p>",
        "id": 262913071,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081209
    },
    {
        "content": "<p>then rustc has no libnvvm but the codegen still has the control it needs</p>",
        "id": 262913076,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081233
    },
    {
        "content": "<p>I believe we're angling for a <code>codegen-args</code> aren't we?</p>",
        "id": 262913077,
        "sender_full_name": "Jubilee",
        "timestamp": 1638081233
    },
    {
        "content": "<p>yes pls no more \"<code>llvm-args</code> except its not llvm its actually codegen args\"</p>",
        "id": 262913125,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081277
    },
    {
        "content": "<p>kinda puts me in a weird position cause i have both special codegen args AND maybe llvm-args too in the future</p>",
        "id": 262913129,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081303
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"276242\">@Riccardo D'Ambrosio</span>  I would call the option for limiting the IR something like \"ir-backend-capabilities\", but if you're in general arguing for a named suite of features rather than individual codegen options that seems fine.</p>",
        "id": 262913140,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081329
    },
    {
        "content": "<p>I mean, i'll try to give it better names like <code>int-remapping</code></p>",
        "id": 262913144,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081360
    },
    {
        "content": "<p>I don't think this is a bad solution</p>",
        "id": 262913186,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081383
    },
    {
        "content": "<p>Sure, but I mean in general it seems fine to have a high-level codegen option that enables a whole suite of low-level options like that.</p>",
        "id": 262913191,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081410
    },
    {
        "content": "<p>Since all gpu code would still go through cuda_builder because of special gpu codegen things, the issue is that building gpu crates takes nightly</p>",
        "id": 262913197,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081424
    },
    {
        "content": "<p>although nvptx64-nvidia-cuda spitting out llvm bitcode would be a bit... weird</p>",
        "id": 262913208,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081449
    },
    {
        "content": "<p>Can you explain more of what you mean by that? Why does it need nightly once your backend is upstream?</p>",
        "id": 262913210,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081474
    },
    {
        "content": "<p>no i mean currently</p>",
        "id": 262913211,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081481
    },
    {
        "content": "<p>Ah.</p>",
        "id": 262913247,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081485
    },
    {
        "content": "<p>if it was upstreamed it would be golden for me, dont need to actively keep up with nightly changes and projects build fully in stable</p>",
        "id": 262913250,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081504
    },
    {
        "content": "<p>So, the main constraint on this solution is that the default needs to be to take the IR and feed it into LLVM, and that needs to work. And the state of the target for the purposes of the target tier policy would be evaluated based on that.</p>",
        "id": 262913257,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081550
    },
    {
        "content": "<p>Although the rust repo would still need some CUDA/libnvvm stuff for tests, in the future i would like to try to get nvptx close to or into tier 1 (not tier 1 with host tools)</p>",
        "id": 262913261,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081568
    },
    {
        "content": "<p>But then people can enable calling an alternate binary (kinda like a linker), and that seems fine.</p>",
        "id": 262913264,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081582
    },
    {
        "content": "<p>Im in contact with someone at nvidia who said they may sponsor/work on rustc CI for actually running GPU code for tests in the future</p>",
        "id": 262913310,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081622
    },
    {
        "content": "<p>The Rust repo could test the default version that uses LLVM ptx via your more GPU-friendly IR, and you could test the nvvm backend separately.</p>",
        "id": 262913322,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081686
    },
    {
        "content": "<p>CI on the GPU is... rough, because of how much it could be abused</p>",
        "id": 262913324,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081696
    },
    {
        "content": "<p>I think we could have runners on an AWS GPU-enabled instance, for instance.</p>",
        "id": 262913330,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081723
    },
    {
        "content": "<p>Id rather not, if we test the target we test both things, libnvvm AND the LLVM PTX backend, separating CI is just a whole lot of work</p>",
        "id": 262913367,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081734
    },
    {
        "content": "<p>i mean technically tier 1 would only count the LLVM PTX backend, but in reality it would be very unfair to not test libnvvm when in reality it will be the most common backend</p>",
        "id": 262913380,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081792
    },
    {
        "content": "<p>I'm trying to suggest that if this target is tier 1, it needs to pass tests with the default LLVM version.</p>",
        "id": 262913381,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081793
    },
    {
        "content": "<p>im not saying it doesnt, im saying it needs to pass both llvm AND libnvvm</p>",
        "id": 262913385,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081806
    },
    {
        "content": "<p>and that leaving out either one is not an option</p>",
        "id": 262913386,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081820
    },
    {
        "content": "<p>So, a hint: if you want to make this plan palatable, you need to stop suggesting that nvvm should be the most common backend.</p>",
        "id": 262913388,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081824
    },
    {
        "content": "<p>You just said that ptx would be much higher quality than it is today with your GPU IR generator.</p>",
        "id": 262913431,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081853
    },
    {
        "content": "<p>but that is simply the truth, just because it is proprietary doesn't mean it shouldn't be taken into account</p>",
        "id": 262913432,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081856
    },
    {
        "content": "<p>im not saying ill treat the LLVM PTX backend as a secondary thing</p>",
        "id": 262913434,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081878
    },
    {
        "content": "<p>You're not <em>saying</em> that, but you're kinda strongly implying that.</p>",
        "id": 262913438,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081898
    },
    {
        "content": "<p>no i am saying that generated ptx would not be a miscompiled mess</p>",
        "id": 262913439,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081898
    },
    {
        "content": "<p>well if i say the llvm ptx backend is the most common then id be implying i wont take libnvvm seriously, which would leave a bad taste in a lot of people's mouths</p>",
        "id": 262913456,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081951
    },
    {
        "content": "<p>I'm not at <em>all</em> asking you to not take it seriously.</p>",
        "id": 262913499,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638081968
    },
    {
        "content": "<p>The Rust project has ideological and also, er, legal commitments.</p>",
        "id": 262913509,
        "sender_full_name": "Jubilee",
        "timestamp": 1638081994
    },
    {
        "content": "<p>Then im saying libnvvm should be tested alongside LLVM PTX in rustc</p>",
        "id": 262913510,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638081995
    },
    {
        "content": "<p>I'm asking you to not effectively tell people nvvm is so much <em>more</em> important that they should immediately enable that configuration and never consider doing otherwise.</p>",
        "id": 262913516,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082036
    },
    {
        "content": "<p>It does, but neither CUDA nor libnvvm force rust to change licenses for anything</p>",
        "id": 262913517,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082036
    },
    {
        "content": "<p>users wont have to think about the used backend for most uses</p>",
        "id": 262913522,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082064
    },
    {
        "content": "<p>This isn't about license compatibility, it's about ecosystem health and project maintenance.</p>",
        "id": 262913524,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082067
    },
    {
        "content": "<p>Now <em>that</em> is the kind of statement that's much more promising!</p>",
        "id": 262913567,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082106
    },
    {
        "content": "<p>Saying im going to pick libnvvm first is basically the same as saying im going to pick llvm ptx first, either ways im implying i think one is the better choice</p>",
        "id": 262913568,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082110
    },
    {
        "content": "<p>So i dont think theres any winning here, ill leave the option widely open for anyone who wants to change it, but internally i will pick an arbitrary backend decided by the project</p>",
        "id": 262913587,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082171
    },
    {
        "content": "<p>I'm not telling you to treat ptx as better, I'm asking you to not systematically treat it as a formality that's only there so you can get upstream and tell people to avoid it. I'm not saying you would do so maliciously, just that some of your recommendations come across as though that's how you would position it.</p>",
        "id": 262913591,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082197
    },
    {
        "content": "<p>Yeah</p>",
        "id": 262913636,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082225
    },
    {
        "content": "<p>If you're comfortable treating it as a first-class supported choice, then I don't think there's an issue here. We can talk at length about the boundaries here so that you don't feel like anyone changes the goalposts on you later on. :)</p>",
        "id": 262913650,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082302
    },
    {
        "content": "<p>/shrug You will in practice pick one over the other in terms of time and energy, and everyone will notice your behavior and evaluate it using their own heuristics.<br>\nOur main consideration is that, whatever choice is made in practice, it doesn't add maintenance burden.</p>",
        "id": 262913651,
        "sender_full_name": "Jubilee",
        "timestamp": 1638082302
    },
    {
        "content": "<p>And you will have thousands of people trying to break the new toy.</p>",
        "id": 262913700,
        "sender_full_name": "Jubilee",
        "timestamp": 1638082343
    },
    {
        "content": "<p>I mean, i am wiling to treat LLVM PTX as a serious backend as long as LLVM is receptive to bug reports and it isn't <em>too</em> broken, meaning, i will feed it a lot of rust code and see if it breaks before considering it</p>",
        "id": 262913707,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082363
    },
    {
        "content": "<p>Ah, so you... haven't reported bugs upstream to LLVM before?</p>",
        "id": 262913715,
        "sender_full_name": "Jubilee",
        "timestamp": 1638082392
    },
    {
        "content": "<p>I haven't, that will be a <del>fun</del> experience</p>",
        "id": 262913721,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082406
    },
    {
        "content": "<p>/me cries in compiling with spaces in filenames</p>",
        "id": 262913728,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1638082434
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"276242\">@Riccardo D'Ambrosio</span>  That sounds perfectly reasonable. And you have a strong incentive to convince them to be receptive and responsive enough to make the backend high quality enough to support the Rust target.</p>",
        "id": 262913732,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082442
    },
    {
        "content": "<p>Also, because of that, i kind of expect that libnvvm will also be treated as a serious backend by rustc, meaning that for example, tests are ran with both libnvvm and llvm ptx to make sure they <em>both</em> work</p>",
        "id": 262913769,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082455
    },
    {
        "content": "<p>So, two separate thoughts on that:</p>",
        "id": 262913775,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082502
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232545\">Joshua Nelson</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262913728\">said</a>:</p>\n<blockquote>\n<p>/me cries in compiling with spaces in filenames</p>\n</blockquote>\n<p>You know what makes me MAD, nvidia loves files with spaces</p>",
        "id": 262913778,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082507
    },
    {
        "content": "<p>like... please <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5</code></p>",
        "id": 262913788,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082527
    },
    {
        "content": "<p>You... may or may not find LLVM is what you would consider \"receptive to bug reports\".</p>",
        "id": 262913795,
        "sender_full_name": "Jubilee",
        "timestamp": 1638082541
    },
    {
        "content": "<p>/me pain</p>",
        "id": 262913839,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082566
    },
    {
        "content": "<p>But yeah, i am not a stranger to talking to a brick wall in bug reports</p>",
        "id": 262913843,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082588
    },
    {
        "content": "<p>nvidia has been surprisingly receptive to bug reports, they seem to be actively fixing the i128 issues in libnvvm with CUDA 11.5's experimental 128 bit ints</p>",
        "id": 262913851,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082637
    },
    {
        "content": "<p>First, I do absolutely think it's fair to treat bugs reported by users using nvvm as important, and in particular I don't think Rust folks should dismiss them. (There's value  in distinguishing whether any particular bug is present with the default backend or only with a non-default backend, but we should still accept the bug reports, and handle code generation issues such as the one you mentioned of limiting the IR to match NVVM's capabilities.)</p>",
        "id": 262913857,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082651
    },
    {
        "content": "<p>I am not saying LLVM is <strong>not</strong> receptive to bug reports, I am merely saying they are a... different project with their own goals and ideas of how things should go, so! Sometimes a bug report is answered and addressed promptly, sometimes... not.</p>",
        "id": 262913859,
        "sender_full_name": "Jubilee",
        "timestamp": 1638082658
    },
    {
        "content": "<p>Yeah absolutely</p>",
        "id": 262913861,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082669
    },
    {
        "content": "<p>But if its a simple codegen bug i will probably bite the bullet and submit a pr myself</p>",
        "id": 262913901,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082698
    },
    {
        "content": "<p>But second, I think there may well be logistical issues with treating nvvm testing as part of Rust CI, and I don't think we could reasonably treat test failures in it the same way we would treat test failures in a tier 1 target (e.g. blocking Rust PRs).</p>",
        "id": 262913913,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082775
    },
    {
        "content": "<p>( this is in the theoretical future where this target gets far enough to consider applying for tier one status)</p>",
        "id": 262914014,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082824
    },
    {
        "content": "<p>Yeah, this will probably need to get run by whatever passes for legal around here.</p>",
        "id": 262914019,
        "sender_full_name": "Jubilee",
        "timestamp": 1638082836
    },
    {
        "content": "<p>Hmm yeah, nvptx may not be able to make it into tier 1, but i would at least like to get it to \"it will almost definitely work, and we test automatically that gpu code is not miscompiled and wrong\"</p>",
        "id": 262914027,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082853
    },
    {
        "content": "<p>sounds plausible.</p>",
        "id": 262914041,
        "sender_full_name": "Jubilee",
        "timestamp": 1638082895
    },
    {
        "content": "<p>Maybe a new subcategory of tier 1/tier 2?</p>",
        "id": 262914044,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082905
    },
    {
        "content": "<p>Well mostly we have makefile tests for that sort of thing and you could just have it run as a cross compilation test.</p>",
        "id": 262914083,
        "sender_full_name": "Jubilee",
        "timestamp": 1638082937
    },
    {
        "content": "<p>Let's entertain that discussion after you get to tier 2. ;)</p>",
        "id": 262914086,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082941
    },
    {
        "content": "<p>I am already at tier 2 <span aria-label=\"wink\" class=\"emoji emoji-1f609\" role=\"img\" title=\"wink\">:wink:</span></p>",
        "id": 262914088,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638082952
    },
    {
        "content": "<p>lul</p>",
        "id": 262914089,
        "sender_full_name": "Jubilee",
        "timestamp": 1638082959
    },
    {
        "content": "<p>Your new GPU-friendly cg isn't. ;)</p>",
        "id": 262914093,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638082982
    },
    {
        "content": "<p>But mine actually works ;)</p>",
        "id": 262914104,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083005
    },
    {
        "content": "<p>I mean there's another GPU backend that also works.</p>",
        "id": 262914105,
        "sender_full_name": "Jubilee",
        "timestamp": 1638083022
    },
    {
        "content": "<p>It's just not in-tree.</p>",
        "id": 262914109,
        "sender_full_name": "Jubilee",
        "timestamp": 1638083034
    },
    {
        "content": "<p>Anyways, blocking PRs may not be possible, but id at least like some way of being notified of \"hey someone just borked the thing somehow\"</p>",
        "id": 262914151,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083052
    },
    {
        "content": "<p>Which, speaking of!</p>",
        "id": 262914152,
        "sender_full_name": "Jubilee",
        "timestamp": 1638083054
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"276242\">@Riccardo D'Ambrosio</span> That seems easy enough, though bear in mind the text in the target tier policy about poking PR authors. It was written for that kind of situation.</p>",
        "id": 262914166,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083109
    },
    {
        "content": "<p>Yeah im kind of torn on that clause... on one hand yes, we may have issues where some change breaks something deep inside of libnvvm or LLVM PTX, but at the same time, having changes break the codegen, even if not a backend issue, i think that should at least make the PR get paused until a decision is made</p>",
        "id": 262914227,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083206
    },
    {
        "content": "<p>So kind of like an in-between of \"you are not responsible if it breaks, BUT, if it does break, the PR will be paused while a decision is made on whether to wait on a fix inside the codegen/backends, or whether to push it\"</p>",
        "id": 262914242,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083256
    },
    {
        "content": "<p>Breaking the codegen would be blocking, breaking the nvvm usage wouldn't be.</p>",
        "id": 262914243,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083276
    },
    {
        "content": "<p>breaking the codegen (not breaking the backends) would be blocking</p>",
        "id": 262914285,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083293
    },
    {
        "content": "<p>if it was like, breaking LLVM PTX, and the fix was small enough, and the llvm team merged it fast enough, then idk? should that be blocking?</p>",
        "id": 262914288,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083316
    },
    {
        "content": "<p>I mean im sure youve had this issue in the past</p>",
        "id": 262914293,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083337
    },
    {
        "content": "<p>Argh, mobile UI treated that as an edit to a previous message rather than a new message.</p>",
        "id": 262914311,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083395
    },
    {
        "content": "<p>Lol</p>",
        "id": 262914312,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083401
    },
    {
        "content": "<p>mobile zulip is something else</p>",
        "id": 262914351,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083411
    },
    {
        "content": "<p>Breaking the codegen would absolutely block a PR, once your codegen gets to an appropriate tier for that.</p>",
        "id": 262914359,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083458
    },
    {
        "content": "<p>I mean, i think tier 1 is achievable, because in theory if something truly breaks badly in something like libnvvm, we can just switch to defaulting to LLVM PTX while we wait on a codegen workaround and/or a fix from nvidia.</p>",
        "id": 262914372,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083489
    },
    {
        "content": "<p>I think you may have missed that this will need to default to LLVM upstream.</p>",
        "id": 262914414,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083527
    },
    {
        "content": "<p>Yeah</p>",
        "id": 262914422,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083546
    },
    {
        "content": "<p>With an option to call an external program to do something else with the IR.</p>",
        "id": 262914426,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083561
    },
    {
        "content": "<p>Yeah this would not be a concern for rustc itself, but it would be for the project as a whole because after all, \"direct\" uses of the target should be rare</p>",
        "id": 262914436,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083599
    },
    {
        "content": "<p>I think we can absolutely get to a well-supported state you're reasonably satisfied by. It may have some portions supported by rustc, and other portions supported by an external project you maintain and Nvidia sponsors.</p>",
        "id": 262914491,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083676
    },
    {
        "content": "<p>Yeah, im just concerned of cases like, if libnvvm breaks (but it can be worked around), but llvm ptx works, then the PR won't be blocked because libnvvm is \"external\"</p>",
        "id": 262914532,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083763
    },
    {
        "content": "<p>Yes, that's exactly what's going to happen.</p>",
        "id": 262914556,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083777
    },
    {
        "content": "<p>Which i mean, is not the end of the world but not the best either</p>",
        "id": 262914561,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083792
    },
    {
        "content": "<p>I get that. Nvidia's GPU support being proprietary is not the best, either. :)</p>",
        "id": 262914565,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083819
    },
    {
        "content": "<p>Until AMD starts putting in effort into documenting the most minimal things about amdgpu i will stick with targeting nvidia, besides, to target something like HIP we would need a good nvidia codegen anyways</p>",
        "id": 262914572,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083865
    },
    {
        "content": "<p>I'm personally hopeful that your GPU-friendly IR codegen will be useful for a variety of such purposes. :)</p>",
        "id": 262914617,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083911
    },
    {
        "content": "<p>I mean, if amd starts actually documenting the amdgpu target, im open to trying to target it with some changes, but until then, i cant be expected to put in effort when a multibillion dollar company wont put in any</p>",
        "id": 262914631,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638083965
    },
    {
        "content": "<p>I wouldn't expect you to.</p>",
        "id": 262914635,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083979
    },
    {
        "content": "<p>Someone else might want to work on that though, and get you to merge their changes. :)</p>",
        "id": 262914639,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638083999
    },
    {
        "content": "<p>In theory if we can target both PTX and whatever amd uses, and amd makes a version of HIP that is language-agnostic, then we can have cross platform stuff</p>",
        "id": 262914680,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084010
    },
    {
        "content": "<p>but so far the only one putting in effort for language-agnostic stuff is nvidia, so, thats kind of the state of everything</p>",
        "id": 262914689,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084032
    },
    {
        "content": "<p>I mean this is just funny and sad <a href=\"/user_uploads/4715/BZD3ouu06aAwg1TKURZ58Giq/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/BZD3ouu06aAwg1TKURZ58Giq/image.png\" title=\"image.png\"><img src=\"/user_uploads/4715/BZD3ouu06aAwg1TKURZ58Giq/image.png\"></a></div>",
        "id": 262914698,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084083
    },
    {
        "content": "<p>numba even dropped rocm support because it was just such a hassle to work with, so undocumented, and so buggy. So its not as much about \"i just like nvidia more and dont like amd\", its \"nvidia is the only viable option\" if you get what i mean</p>",
        "id": 262914758,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084156
    },
    {
        "content": "<p>I'm not asking you to argue that point.</p>",
        "id": 262914770,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638084196
    },
    {
        "content": "<p>I want to confirm something about the plan we've talked about here to make sure we're on the same page.</p>",
        "id": 262914780,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638084223
    },
    {
        "content": "<p>Yeah it sounds good for now</p>",
        "id": 262914798,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084243
    },
    {
        "content": "<p>I will probably make some sort of MCP once i can optionally target llvm ptx and ive found its not too buggy</p>",
        "id": 262914841,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084290
    },
    {
        "content": "<p>You have a cg that generates GPU-friendly LLVM IR. You're going to feed that into LLVM PTX by default. You'll add a codegen option that sets an external IR tool, and either make a small command-line wrapper for nvvm or use an existing one. Does that match your takeaway from this conversation too?</p>",
        "id": 262914908,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638084420
    },
    {
        "content": "<p>We're talking about one target with some new options, not two targets, right?</p>",
        "id": 262914916,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638084459
    },
    {
        "content": "<p>yes</p>",
        "id": 262914917,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084481
    },
    {
        "content": "<p>Although one weird thing is that if the project wants to switch, say, libnvvm for llvm ptx by default, then it would need to somehow sync with stable versions</p>",
        "id": 262914959,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084516
    },
    {
        "content": "<p>like, if a release of the codegen happens in 1.50 that breaks libnvvm, then it should switch to llvm ptx until hopefully 1.51 when it is fixed</p>",
        "id": 262914962,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084543
    },
    {
        "content": "<p>I mean, thats probably possible by checking dates, but that seems... weird, its kind of an odd case</p>",
        "id": 262914966,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084569
    },
    {
        "content": "<p>So<br>\nyou support all of the \"platform-intrinsics\"<br>\nright?</p>",
        "id": 262915019,
        "sender_full_name": "Jubilee",
        "timestamp": 1638084633
    },
    {
        "content": "<p>Is there a list of those somewhere?</p>",
        "id": 262915023,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084657
    },
    {
        "content": "<p>Im not sure if u mean like the codegen-handled intrinsics like trap, unreachable, bswap, etc</p>",
        "id": 262915032,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084677
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"276242\">@Riccardo D'Ambrosio</span> </p>\n<blockquote>\n<p>Although one weird thing is that if the project wants to switch, say, libnvvm for llvm ptx by default, then it would need to somehow sync with stable versions</p>\n</blockquote>\n<p>That isn't going to be an issue. The project will start out defaulting to LLVM, and stay that way, and if we ever had a situation in which that became non-viable we would be talking about removing the target.</p>",
        "id": 262915040,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638084707
    },
    {
        "content": "<p>I mean also the list of <code>simd_*</code> symbols. <a href=\"https://github.com/rust-lang/rust/blob/master/compiler/rustc_codegen_llvm/src/intrinsic.rs\">https://github.com/rust-lang/rust/blob/master/compiler/rustc_codegen_llvm/src/intrinsic.rs</a></p>",
        "id": 262915082,
        "sender_full_name": "Jubilee",
        "timestamp": 1638084743
    },
    {
        "content": "<p>simd does not make sense on the gpu, it has some kinda weird specific simd things but thats it: <a href=\"https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__SIMD.html\">https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__SIMD.html</a></p>",
        "id": 262915092,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084779
    },
    {
        "content": "<p>It doesn't actually matter if you lower them to actual SIMD instructions FYI.</p>",
        "id": 262915100,
        "sender_full_name": "Jubilee",
        "timestamp": 1638084819
    },
    {
        "content": "<p>Yeah but emulating those will be... yikes</p>",
        "id": 262915102,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084835
    },
    {
        "content": "<p>I hope to get an emulation library somewhere in the compiler eventually, though I am still doing cleanup on the frontend (i.e. the actual library-library).</p>",
        "id": 262915149,
        "sender_full_name": "Jubilee",
        "timestamp": 1638084872
    },
    {
        "content": "<p>Other than that, i don't support a lot of advanced i128 intrinsics. ill need to emulate those too</p>",
        "id": 262915159,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084898
    },
    {
        "content": "<p>Don't worry, I am inconveniencing <strong>everyone</strong>. :D</p>",
        "id": 262915160,
        "sender_full_name": "Jubilee",
        "timestamp": 1638084900
    },
    {
        "content": "<p>The price of good simd</p>",
        "id": 262915173,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084927
    },
    {
        "content": "<p>now stabilize it thanks :)</p>",
        "id": 262915175,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638084934
    },
    {
        "content": "<p>But yes just \"whatever you would do looped on an array\" is (generically speaking, ignoring permutation instructions and the like) a valid lowering.</p>",
        "id": 262915222,
        "sender_full_name": "Jubilee",
        "timestamp": 1638084968
    },
    {
        "content": "<p>Although i think if ur trying to use simd on the gpu you probably have bigger issues in your kernels</p>",
        "id": 262915253,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638085069
    },
    {
        "content": "<p>because yknow, the whole point is to not use simd and to use threads</p>",
        "id": 262915270,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638085084
    },
    {
        "content": "<p>It's more that people will be using libraries.</p>",
        "id": 262915299,
        "sender_full_name": "Jubilee",
        "timestamp": 1638085096
    },
    {
        "content": "<p>warps are just really w i d e simd execution units</p>",
        "id": 262915300,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638085099
    },
    {
        "content": "<p>So while it's not a huge thing <strong>right now</strong>, eventually someone will call a library function that for some godforsaken reason reprs something as SIMD somewhere and then... and then...! <strong>boom</strong></p>",
        "id": 262915382,
        "sender_full_name": "Jubilee",
        "timestamp": 1638085266
    },
    {
        "content": "<p>Yeah</p>",
        "id": 262915387,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638085282
    },
    {
        "content": "<p>I mean, i can lower certain simd things to those special cuda instructions, but the rest needs to be emulated</p>",
        "id": 262915395,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638085319
    },
    {
        "content": "<p>Oh yeah these are like i16x2 packed in a u32? Yeah Arm does the same thing.</p>",
        "id": 262915563,
        "sender_full_name": "Jubilee",
        "timestamp": 1638085583
    },
    {
        "content": "<p>I would <em>love</em> it if instead of just</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"k\">fn</span> <span class=\"nf\">unchecked_mul</span><span class=\"o\">&lt;</span><span class=\"n\">T</span>: <span class=\"nb\">Copy</span><span class=\"o\">&gt;</span><span class=\"p\">(</span><span class=\"n\">x</span>: <span class=\"nc\">T</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">y</span>: <span class=\"nc\">T</span><span class=\"p\">)</span><span class=\"w\"> </span>-&gt; <span class=\"nc\">T</span><span class=\"p\">;</span><span class=\"w\"></span>\n</code></pre></div>\n<p>the intrinsics could be defined like</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"k\">fn</span> <span class=\"nf\">unchecked_mul</span><span class=\"o\">&lt;</span><span class=\"n\">T</span>: <span class=\"nb\">Copy</span><span class=\"o\">&gt;</span><span class=\"p\">(</span><span class=\"n\">x</span>: <span class=\"nc\">T</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">y</span>: <span class=\"nc\">T</span><span class=\"p\">)</span><span class=\"w\"> </span>-&gt; <span class=\"nc\">T</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"n\">wrapping_mul</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"p\">)</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n<p>so the backend could just use that MIR if it didn't have a native implementation.</p>\n<p>That would make adding intrinsics <em>so</em> much easier in our upcoming \"at least 4 common backends\" world...</p>",
        "id": 262915898,
        "sender_full_name": "scottmcm",
        "timestamp": 1638086193
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> Actually, after a lot of thought, and a lot of consulting other people who have a stake in targets that use proprietary libraries, i think i will go the route of proposing changes to the target tier policy. Not only for CUDA, but for future targets in which the best way to target them is a proprietary library.</p>",
        "id": 262944059,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638127509
    },
    {
        "content": "<p>Sigh. I'd love it if you picked a path that <em>doesn't</em> require me to spend a while attempting to prevent such changes, because I'm a little busy with other things right now.</p>",
        "id": 262944070,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638127541
    },
    {
        "content": "<p>In short: yes, I want to stop those other targets too.</p>",
        "id": 262944074,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638127550
    },
    {
        "content": "<p>Every proposal will have people who don't agree, i just think the amount of people who have a stake in such targets is much higher than those who oppose such changes <span aria-label=\"shrug\" class=\"emoji emoji-1f937\" role=\"img\" title=\"shrug\">:shrug:</span></p>",
        "id": 262944130,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638127622
    },
    {
        "content": "<p>I think it's a fundamentally bad idea, I wrote the policy specifically to prevent it, and people across three teams signed off on that policy. I started with a toned-down version and people asked me to make it <em>stricter</em>.</p>",
        "id": 262944134,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638127626
    },
    {
        "content": "<p>Yes but a lot has changed since then, nowadays rustc is starting to expand into tons of other custom backends</p>",
        "id": 262944145,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638127656
    },
    {
        "content": "<p>That's quite recent, and the policy was written with other backends in mind.</p>",
        "id": 262944147,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638127672
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262944130\">said</a>:</p>\n<blockquote>\n<p>Every proposal will have people who don't agree, i just think the amount of people who have a stake in such targets is much higher than those who oppose such changes <span aria-label=\"shrug\" class=\"emoji emoji-1f937\" role=\"img\" title=\"shrug\">:shrug:</span></p>\n</blockquote>\n<p>Also, I'm going to ask you to <em>please</em> not try to make this a numbers game. This is not a vote.</p>",
        "id": 262944194,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638127712
    },
    {
        "content": "<p>I know its not, but the more people you can show that have a stake in rust supporting such things, the more you can show the usefulness of overriding such a policy</p>",
        "id": 262944209,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638127775
    },
    {
        "content": "<p>That's really, really not how it works.</p>",
        "id": 262944211,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638127791
    },
    {
        "content": "<p>This is \"cost/benefit\" or \"harm vs help\" not \"number of interested people\".</p>",
        "id": 262944260,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638127813
    },
    {
        "content": "<p>I realize that you don't actually believe this would do harm to Rust, but take it as a given that those who <em>do</em> oppose proprietary in-tree targets do believe it would do substantial harm to Rust. If you take that to be true, then you're attempting to argue \"there are enough people who care about this target that it outweighs the harm that would be done to Rust\".</p>",
        "id": 262944289,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638127875
    },
    {
        "content": "<p>Yes, exactly, the more people interested in using rust for such domains, the more it would help rust. It would be kind of useless to propose such a change without showing what the change could do for rust itself</p>",
        "id": 262944297,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638127886
    },
    {
        "content": "<p>I do know the downsides of allowing this, but i also recognize the great benefits this could bring for rust</p>",
        "id": 262944302,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638127905
    },
    {
        "content": "<p>The argument of \"you want your compiler to support my special target more than you want to keep your compiler open\" is a very old one. The compilers that heard and ignored that argument are still around. The targets aren't.</p>",
        "id": 262944366,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638127990
    },
    {
        "content": "<p>If rustc does not allow itself to expand into such domains, in the long term we may have compilers and tools that <em>do</em> target such domains, leading to rustc having a hard time keeping up</p>",
        "id": 262944377,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638128001
    },
    {
        "content": "<p>Or we may have people who care enough about using Rust in those domains that they build open tools for targeting those domains.</p>",
        "id": 262944387,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638128019
    },
    {
        "content": "<p>Or we may have people using something other than Rust for those domains, and that's OK too.</p>",
        "id": 262944392,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638128028
    },
    {
        "content": "<p>I care about Rust enduring for the long term and being non-proprietary for <em>other</em> targets much much more than I care about any one target.</p>",
        "id": 262944439,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638128051
    },
    {
        "content": "<p>To put it another way: the same argument you're using to support NVIDIA is one that CPU vendors would jump on in a <em>heartbeat</em> if they hear \"oooh, we can keep optimizations for our CPU secret!\".</p>",
        "id": 262944445,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638128076
    },
    {
        "content": "<p>And ten years from now, I don't want the primary backends for x86 and ARM to be proprietary.</p>",
        "id": 262944449,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638128100
    },
    {
        "content": "<p>Nor do I want Rust folks trying to figure out how to support them while not being able to debug them unless they work for those companies.</p>",
        "id": 262944463,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638128141
    },
    {
        "content": "<p>If people are so eager to keep their stuff closed they can always implement a wasm compiler and take wasm output from rust, with all the penalties that implies <span aria-label=\"shrug\" class=\"emoji emoji-1f937\" role=\"img\" title=\"shrug\">:shrug:</span></p>",
        "id": 262944905,
        "sender_full_name": "The 8472",
        "timestamp": 1638128878
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"330154\">@The 8472</span> I don't think we have to go <em>that</em> far. I think it's fine for folks to use a decent IR.</p>",
        "id": 262944974,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638128973
    },
    {
        "content": "<p>(And I don't think wasm is a good fit for GPUs, as far as I know.)</p>",
        "id": 262944980,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638128990
    },
    {
        "content": "<p>Sure, substitute wasm with something equivalent.</p>",
        "id": 262945039,
        "sender_full_name": "The 8472",
        "timestamp": 1638129040
    },
    {
        "content": "<p>FWIW, <span class=\"user-mention\" data-user-id=\"276242\">@Riccardo D'Ambrosio</span> and I talked this through in detail, and came up with the following rough sketch of a plan that should allow merging a CUDA target upstream:</p>\n<ul>\n<li>Add the new GPU-friendly CUDA codegen layer to Rust, <code>cg_cuda</code>, which generates a specified subset of LLVM IR that's compatible with both LLVM PTX and libnvvm.</li>\n<li>Modify the existing <code>nvptx64-nvidia-cuda</code> target to use <code>cg_cuda</code>. (We don't have to drop support for using <code>cg_llvm</code> with that target, but we almost certainly want to default to using <code>cg_cuda</code> once it's merged; it's likely to produce much better code and make LLVM PTX behave better, in addition to supporting libnvvm. And we may want to drop <code>cg_llvm</code> support for this if it turns out to not be worth supporting.)</li>\n<li>Add a new rustc codegen option to use the external <code>cicc</code> program (which uses libnvvm) from the CUDA SDK to turn the IR into PTX. (We should document exactly what interface we expect from <code>cicc</code> so that it's possible for people to use that interface for some other tool, such as experimenting with an open equivalent.)</li>\n<li>Plumb that codegen option through cargo so that people can easily pass it in their projects without using <code>RUSTFLAGS</code>.</li>\n</ul>",
        "id": 262949518,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638135303
    },
    {
        "content": "<p>Does nvidia provide a software emulation library to run cuda on the CPU so we could run tests for that in CI without needing expensive GPU runners?</p>",
        "id": 262949626,
        "sender_full_name": "The 8472",
        "timestamp": 1638135468
    },
    {
        "content": "<p>I dont think so</p>",
        "id": 262949687,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638135515
    },
    {
        "content": "<p>There is gpu ocelot but thats a third party thing and probably doesnt always work</p>",
        "id": 262949693,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638135529
    },
    {
        "content": "<p>Besides, i dont think its wise to rely on third party tools for tests, because a lot of tests will be testing semantics of memory ordering and things like that</p>",
        "id": 262949712,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638135566
    },
    {
        "content": "<p>And we will need CUDA CI anyways for benchmarking libnvvm and llvm ptx against eachother</p>",
        "id": 262949722,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638135583
    },
    {
        "content": "<p>We may also want to run tests through <code>cuda-memcheck</code>, just to make sure we dont make anything unsound. CUDA is especially good at hiding unsoundness</p>",
        "id": 262950232,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638136208
    },
    {
        "content": "<p>Though for some reason cuda-memcheck, at least on my machine is soooooooo slow in startup and stop, it takes like 2 microseconds to process a kernel and 2 minutes to yield the result <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span></p>",
        "id": 262950278,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638136276
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262944209\">said</a>:</p>\n<blockquote>\n<p>I know its not, but the more people you can show that have a stake in rust supporting such things, the more you can show the usefulness of overriding such a policy</p>\n</blockquote>\n<p>Rust already supports proprietary targets to the extent it must (hi Windows) and it is HELL. Even Microsoft barely knows what the spec for their own debuginfo format is, and they certainly aren't really disclosing much to anyone else.</p>",
        "id": 262964557,
        "sender_full_name": "Jubilee",
        "timestamp": 1638157935
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"330154\">The 8472</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262945039\">said</a>:</p>\n<blockquote>\n<p>Sure, substitute wasm with something equivalent.</p>\n</blockquote>\n<p>The SPIRV backend, probably.</p>",
        "id": 262964563,
        "sender_full_name": "Jubilee",
        "timestamp": 1638157947
    },
    {
        "content": "<p>I suppose claiming Microsoft doesn't know their own debuginfo format could be seen as inflammatory, but I am open to being proven wrong by Microsoft publishing an actual spec instead of a barely-functional repo. :^)</p>",
        "id": 262964640,
        "sender_full_name": "Jubilee",
        "timestamp": 1638158057
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262944302\">said</a>:</p>\n<blockquote>\n<p>I do know the downsides of allowing this, but i also recognize the great benefits this could bring for rust</p>\n</blockquote>\n<p>Rust is nothing... ash and smoke in the wind... without the people backing it.<br>\nAnd I am not here for serving corporate interests, or supporting further enclosure of the commons.</p>",
        "id": 262965245,
        "sender_full_name": "Jubilee",
        "timestamp": 1638158795
    },
    {
        "content": "<p>I also think this counts as a crime on <del>humanity</del> rustc, i am overriding functions defined in libm with libdevice intrinsics for the purpose of code size and speed <a href=\"https://github.com/Rust-GPU/Rust-CUDA/blob/master/crates/rustc_codegen_nvvm/src/override_fns.rs\">https://github.com/Rust-GPU/Rust-CUDA/blob/master/crates/rustc_codegen_nvvm/src/override_fns.rs</a></p>",
        "id": 262970061,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638165835
    },
    {
        "content": "<p>anyways, <span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> would u be wiling to go with a dll interface instead of an exe interface? really the medium doesnt change, its just that keeping everything in memory is helpful for performance and flexibility</p>",
        "id": 262970112,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638165876
    },
    {
        "content": "<p>I would define a common interface thats expected, then make a dll that wraps the libnvvm dll with that common interface</p>",
        "id": 262970125,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638165906
    },
    {
        "content": "<p>that way, we don't get feature creep because the dll interface is always the same</p>",
        "id": 262970137,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638165920
    },
    {
        "content": "<p>That seems like a lot more trouble, but assuming the interface is roughly \"pass IR, get back generated code\", I don't see an issue with that.</p>\n<p>But wouldn't that mean users need to build something? You wanted to avoid that.</p>",
        "id": 262970192,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638165993
    },
    {
        "content": "<p>Well its because the bitcode may potentially be very big for large programs, and id like to add optional APIs for things like validation</p>",
        "id": 262970203,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166029
    },
    {
        "content": "<p>If possible id like to ship that dll <em>with</em> rustc</p>",
        "id": 262970207,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166044
    },
    {
        "content": "<p>So at that point you effectively are linking to nvvm directly and I don't see a meaningful difference.</p>",
        "id": 262970216,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166076
    },
    {
        "content": "<p>the dll is basically just a wrapper for the libnvvm dll, it doesn't statically link it, so there should be no issue</p>",
        "id": 262970217,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166078
    },
    {
        "content": "<p>not necessarily, im linking through a common interface to avoid contributors relying on the libnvvm interface directly</p>",
        "id": 262970260,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166104
    },
    {
        "content": "<p>which if im not mistaken, was the main reason for using an exe</p>",
        "id": 262970268,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166124
    },
    {
        "content": "<p>I don't know how many different ways to say \"static versus dynamic doesn't matter for licensing\". :)</p>",
        "id": 262970269,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166124
    },
    {
        "content": "<p>If you want to centralize the interface, I think you might as well use dlopen and just say \"do not dlsym any other functions\".</p>",
        "id": 262970344,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166218
    },
    {
        "content": "<p>Putting it in a separate crate would help for instance.</p>",
        "id": 262970357,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166245
    },
    {
        "content": "<p>Yeah</p>",
        "id": 262970361,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166253
    },
    {
        "content": "<p>I'm also wondering if cicc could take a pipe.</p>",
        "id": 262970384,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166278
    },
    {
        "content": "<p>No cicc only takes files i think</p>",
        "id": 262970391,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166291
    },
    {
        "content": "<p>(by the way, bitcode being big would normally be an argument for files, not memory)</p>",
        "id": 262970397,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166311
    },
    {
        "content": "<p>(in case it doesn't fit in memory)</p>",
        "id": 262970439,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166325
    },
    {
        "content": "<p>Well it <em>has to</em> because of the way linking works currently <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span></p>",
        "id": 262970449,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166339
    },
    {
        "content": "<p>i mean, it will be hard to make a bitcode file so big it doesnt fit into memory</p>",
        "id": 262970456,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166356
    },
    {
        "content": "<p>unless you are pulling in like half the ecosystem</p>",
        "id": 262970461,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166366
    },
    {
        "content": "<p>Would you consider trying the cicc interface at first, and see if it's actually a bottleneck? I would expect that since it only gets involved once per compile it wouldn't be a substantial overhead.</p>",
        "id": 262970474,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166393
    },
    {
        "content": "<p>If it's a bottleneck to the point that it makes the interface not usable, then let's talk about alternatives.</p>",
        "id": 262970506,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166426
    },
    {
        "content": "<p>Yeah i'll see once nvidia gets back to me about how to actually use it <span aria-label=\"stuck out tongue\" class=\"emoji emoji-1f61b\" role=\"img\" title=\"stuck out tongue\">:stuck_out_tongue:</span></p>",
        "id": 262970507,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166427
    },
    {
        "content": "<p>No --help?</p>",
        "id": 262970513,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166438
    },
    {
        "content": "<p>i also wanted to implement some way to load plugins, for tools like the enzyme autodiff</p>",
        "id": 262970515,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166441
    },
    {
        "content": "<p>No</p>",
        "id": 262970529,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166443
    },
    {
        "content": "<p>Plugins into what?</p>",
        "id": 262970571,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166459
    },
    {
        "content": "<p>into the generated bitcode module</p>",
        "id": 262970579,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166469
    },
    {
        "content": "<p>Ah. So on the rustc side before handing off the code?</p>",
        "id": 262970586,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166489
    },
    {
        "content": "<p>And i think doing Module -&gt; Bitcode -&gt; Module -&gt; Bitcode -&gt; Module -&gt; PTX would be a significant issue</p>",
        "id": 262970588,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166502
    },
    {
        "content": "<p>if it goes through files that is</p>",
        "id": 262970596,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166510
    },
    {
        "content": "<p>Yeah</p>",
        "id": 262970597,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166513
    },
    {
        "content": "<p>What do you mean by bitcode -&gt; module?</p>",
        "id": 262970606,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166534
    },
    {
        "content": "<p>enzyme works on the LLVM IR itself, you make extern calls, then enzyme generates defs for those functions using the LLVM IR</p>",
        "id": 262970607,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166535
    },
    {
        "content": "<p>Module is the in-memory repr</p>",
        "id": 262970613,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166543
    },
    {
        "content": "<p>its what the modules are stored in before giving it to libnvvm</p>",
        "id": 262970617,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166555
    },
    {
        "content": "<p>although libnvvm naturally reparses the in-memory bc to a module on its side</p>",
        "id": 262970658,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166574
    },
    {
        "content": "<p>So, one argument for translating to bitcode is that I would expect that to be more stable.</p>",
        "id": 262970663,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166593
    },
    {
        "content": "<p>Oh yeah 100%</p>",
        "id": 262970672,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166607
    },
    {
        "content": "<p>im not saying it wont be, im just saying with plugins and stuff the overhead with files might become unwanted</p>",
        "id": 262970676,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166621
    },
    {
        "content": "<p>and dlls are just an easier interface to work with</p>",
        "id": 262970688,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166632
    },
    {
        "content": "<p>I think supporting enzyme or similar is a perfectly reasonable thing to do, albeit not one I would expect to stabilize anytime soon.</p>",
        "id": 262970706,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166660
    },
    {
        "content": "<p>But I wouldn't want to tie that to nvvm support specifically.</p>",
        "id": 262970712,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166676
    },
    {
        "content": "<p>no enzyme doesnt have any libnvvm specific things</p>",
        "id": 262970757,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166692
    },
    {
        "content": "<p>It's just as useful for cg_llvm as well.</p>",
        "id": 262970758,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166698
    },
    {
        "content": "<p>i think it actually requires the llvm ptx backend currently because nvcc cant spit out intermediate nvvm ir</p>",
        "id": 262970761,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166713
    },
    {
        "content": "<p>Right, so no reason to emit files first for enzyme.</p>",
        "id": 262970765,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166720
    },
    {
        "content": "<p>Just for cicc.</p>",
        "id": 262970787,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638166748
    },
    {
        "content": "<p>Yeah</p>",
        "id": 262970797,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638166772
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262912925\">said</a>:</p>\n<blockquote>\n<p>So, theoretically, suppose we merged the GPU IR backend you've worked on, and made it run the IR through LLVM normally. But we added a configuration option, available in stable, that invoked a standalone tool in place of LLVM. Any tool the user specified, with specified command-line arguments.</p>\n</blockquote>\n<p>I think this is a really interesting idea. There are quite a few tools that read LLVM BC and then do <em>something</em> with it. Many of these tools are currently awkward to use in Rust since you can't really integrate them into the build via Cargo unless you create your own subcommand or something. So I think this would be useful even outside of the gpu/cuda target.</p>",
        "id": 263028670,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1638201968
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276242\">Riccardo D'Ambrosio</span> <a href=\"#narrow/stream/131828-t-compiler/topic/codegen.20upstreaming.20requirements/near/262970456\">said</a>:</p>\n<blockquote>\n<p>i mean, it will be hard to make a bitcode file so big it doesnt fit into memory</p>\n</blockquote>\n<p>Some people run rustc, even do significant development, on extremely resource-constrained machines.<br>\nThe compiler should be capable of reasonably gracefully handling being on a machine with only 1 GB RAM.</p>",
        "id": 263032981,
        "sender_full_name": "Jubilee",
        "timestamp": 1638203696
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"281757\">@Jubilee</span> This is difficult because of the strategy for linking that the codegen currently uses, it is essentially always fat LTO. At link time, i gather the bc of all the cgus in the rlibs, then i merge them into a single LLVM module, internalize anything not a kernel, then run global DCE. And finally give that to libnvvm. If i did not do that, we would have absolutely gigantic PTX files. This also reduces the amount of work libnvvm has to do, as well as give us LTO optimizations by default, which is important on the GPU. </p>\n<p>I used to lazily-load only what was needed by previous modules with dependency graphs, but becauseof no_mangle functions, it still made a large ptx file, this approach is much better and faster. Im not too worried about it because:</p>\n<ul>\n<li>If you are running CUDA you probably have at least 8 gb of ram, i dont think cuda can even run on 1gb of ram.</li>\n<li>GPU crates are unlikely to have tons of dependencies before or after DCE.</li>\n</ul>",
        "id": 263048539,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638210297
    },
    {
        "content": "<p>One thing that i want to figure out is how to get rustc to just not respect <code>#[inline]</code>, for us it is basically just bloat because inlining will happen anyways</p>",
        "id": 263048895,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638210464
    },
    {
        "content": "<p>No no,<br>\nPeople do not deploy on these machines.<br>\nThey build locally and then upload it to a server that runs it.</p>",
        "id": 263049821,
        "sender_full_name": "Jubilee",
        "timestamp": 1638210899
    },
    {
        "content": "<p>I've run rust on 1, 4, and 8gb devices many times, and it does work, but gosh does it take a long time sometimes. Not like \"get the coffee\" long, more like \"make and eat an entire sandwich\" long.</p>",
        "id": 263050397,
        "sender_full_name": "Lokathor",
        "timestamp": 1638211173
    },
    {
        "content": "<p>that's... not too terrible. compared to building llvm and rust on a laptop. last time I did that it ran for more than an hour. Granted, that was before download-ci-llvm.</p>",
        "id": 263052249,
        "sender_full_name": "The 8472",
        "timestamp": 1638211985
    },
    {
        "content": "<p>Yeah, I am really basically thinking<br>\n\"okay, yes, the user has 8 GB, but 6 of it has already been eaten by Chrome.\"</p>",
        "id": 263053744,
        "sender_full_name": "Jubilee",
        "timestamp": 1638212747
    },
    {
        "content": "<p>Mind, virtual memory, i.e. using a disk as memory, is an option. But Rustup has specific mitigations to make sure it installs properly on 32 bit machines that don't have virtual memory available. It's also fine if there's no <strong>specific</strong> mitigation, I think, that hard-guarantees it will work, but just attempts to reduce the working set, so I think \"convincing rustc to inline less for this backend because I will take care of that\" is a valid response.</p>",
        "id": 263054158,
        "sender_full_name": "Jubilee",
        "timestamp": 1638212918
    },
    {
        "content": "<p>It's also fine if the happy path is under 2 minutes and the sad path is 24 hours.</p>",
        "id": 263054250,
        "sender_full_name": "Jubilee",
        "timestamp": 1638212998
    },
    {
        "content": "<p>I think most of the bloat when linking will actually just be inline functions</p>",
        "id": 263055860,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638213825
    },
    {
        "content": "<p>so many functions that are needlessly there</p>",
        "id": 263055870,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638213837
    },
    {
        "content": "<p>So itd be great if there was a way to tell rustc to just... not</p>",
        "id": 263055932,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638213847
    },
    {
        "content": "<p>Cool.<br>\nThen I look forward to someone fixing that so I can compile CUDA in WASI on the browser.</p>",
        "id": 263056334,
        "sender_full_name": "Jubilee",
        "timestamp": 1638214051
    },
    {
        "content": "<p>You know just because you can doesnt mean you should</p>",
        "id": 263056377,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638214082
    },
    {
        "content": "<p><span aria-label=\"smiling imp\" class=\"emoji emoji-1f608\" role=\"img\" title=\"smiling imp\">:smiling_imp:</span> When has that incredibly sensible advice ever stopped humanity?</p>",
        "id": 263056549,
        "sender_full_name": "Jubilee",
        "timestamp": 1638214152
    },
    {
        "content": "<p>sadly not many times</p>",
        "id": 263056563,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638214159
    },
    {
        "content": "<p>someone will find a way to get us WebCUDA, then we will truly be evolved</p>",
        "id": 263056638,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638214205
    },
    {
        "content": "<p>As far as I know, I think we're getting close to that with WebGPU.</p>",
        "id": 263059873,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638215705
    },
    {
        "content": "<p>Not quite there, but getting there.</p>",
        "id": 263059883,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638215712
    },
    {
        "content": "<p>finally, websites can now mine bitcoin for me, ads werent enough <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span></p>",
        "id": 263059940,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638215740
    },
    {
        "content": "<p>They already do <span aria-label=\"rofl\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rofl\">:rofl:</span></p>",
        "id": 263059978,
        "sender_full_name": "Connor Horman",
        "timestamp": 1638215761
    },
    {
        "content": "<p>afaik there already are some web miners, but they use coins that are designed to be GPU/ASIC-hostile</p>",
        "id": 263060059,
        "sender_full_name": "The 8472",
        "timestamp": 1638215791
    },
    {
        "content": "<p>And AFAIK even a GPU isn't enough for competitive bitcoin mining these days. So until there's a WebFPGA...</p>",
        "id": 263066953,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638219537
    },
    {
        "content": "<p>Yeah but they still yoink all of the valuable silicon i could be using to turn my pc into a space heater <span aria-label=\"rage\" class=\"emoji emoji-1f621\" role=\"img\" title=\"rage\">:rage:</span></p>",
        "id": 263067316,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638219692
    },
    {
        "content": "<p>There are ASIC-resistant protocols that use GPUs, so.</p>",
        "id": 263072517,
        "sender_full_name": "Jubilee",
        "timestamp": 1638222742
    },
    {
        "content": "<p>I was thinking, would it be beneficial to actually demote nvptx64-nvidia-cuda to tier 3 once the codegen is upstreamed and is the default handler for it? Then its easier to track testing progress and milestones in general. Such as it being promoted to tier 2 once we have tests for \"this compiles\", then tier 1 once there are proper tests for \"this actually runs\"?</p>",
        "id": 263105018,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638252267
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"276242\">@Riccardo D'Ambrosio</span> That seems rather reasonable to me.</p>",
        "id": 263107310,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638255241
    },
    {
        "content": "<p>Also how do stability guarantees work in terms of targets? Is it breaking to demote or promote a target? what about substituting a codegen? I would presume the latter would only be breaking if the target was tier 1</p>",
        "id": 263237666,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638326166
    },
    {
        "content": "<p>the nvptx64-nvidia-cuda target spec is wrong too, it should state the minimum atomic width is 32 bits, cuda has nothing for less than 32 bit</p>",
        "id": 263237831,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638326379
    },
    {
        "content": "<p>But fixing it would break that target... which is why im asking about this</p>",
        "id": 263237913,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638326458
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"276242\">@Riccardo D'Ambrosio</span> So, a few different aspects of that...</p>",
        "id": 263252705,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638343274
    },
    {
        "content": "<p>The target tier policy specifically states that demoting a target is something we can do, but shouldn't do <em>lightly</em>, and lays out a process for it.</p>",
        "id": 263252721,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638343297
    },
    {
        "content": "<p>It's not breaking at all to <em>promote</em> a target, it just commits us to more.</p>",
        "id": 263252732,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638343319
    },
    {
        "content": "<p>Substituting a codegen is only breaking if it actually breaks things. If it causes things to stop working that worked before, then that'd have to be carefully evaluated.</p>",
        "id": 263252796,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638343351
    },
    {
        "content": "<p>As for changing something like \"minimum atomic width\", I think that's likely to fall under a rule we <em>often</em> apply for breaking changes:</p>",
        "id": 263252820,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638343385
    },
    {
        "content": "<p>It's only broken if it breaks real code that wasn't already broken.</p>",
        "id": 263252846,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638343421
    },
    {
        "content": "<p>If atomics less than 32 bits never actually <em>worked</em>, then code using them doesn't work, so that wouldn't be a blocker.</p>",
        "id": 263252911,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638343454
    },
    {
        "content": "<p>(There's also the question of whether anyone is using a relevant <code>cfg</code>, but again, that seems unlikely to crop up.)</p>",
        "id": 263252937,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1638343474
    },
    {
        "content": "<p>Types available, like AtomicU8, should be changed with great loathing...<br>\n...but functionally it is never actually breaking to fix a simply wrong type.</p>",
        "id": 263253668,
        "sender_full_name": "Jubilee",
        "timestamp": 1638344214
    },
    {
        "content": "<p>just a lot going on in that \"simply\" wrong.</p>",
        "id": 263253769,
        "sender_full_name": "Jubilee",
        "timestamp": 1638344293
    },
    {
        "content": "<p>I see</p>",
        "id": 263308942,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638373161
    },
    {
        "content": "<p>In theory there shouldn't be many things that will stop working, i am skeptic that atomics ever worked with the ptx backend + rustc, but i will need to try it. If it <em>did</em> work, it should definitely not work with atomics less than 32 bits unless the llvm ptx backend emulates them</p>",
        "id": 263311284,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638374068
    },
    {
        "content": "<p>Isn't it legal for the compiler to lower small atomics to larger ones? load + cas isn't destructive for neighboring bytes.</p>",
        "id": 263311305,
        "sender_full_name": "The 8472",
        "timestamp": 1638374079
    },
    {
        "content": "<p>Yes but in practice that would be a lot of work and less efficient than just using native</p>",
        "id": 263311351,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638374102
    },
    {
        "content": "<p>And i dont think we should lie to the user about what atomics are natively supported without emulation</p>",
        "id": 263311404,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638374128
    },
    {
        "content": "<p>sure, but if it doesn't support small ones then there still isn't a minimum, AtomicU8 would just be less efficient than AtomicUsize</p>",
        "id": 263311421,
        "sender_full_name": "The 8472",
        "timestamp": 1638374134
    },
    {
        "content": "<p>CUDA also does not have fetch_nand</p>",
        "id": 263311440,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638374142
    },
    {
        "content": "<p>but that can be emulated i guess, its not a common operation</p>",
        "id": 263311470,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638374152
    },
    {
        "content": "<p>yeah, most operations are emulated as long as CAS or similar are available</p>",
        "id": 263311571,
        "sender_full_name": "The 8472",
        "timestamp": 1638374175
    },
    {
        "content": "<p>theres always CAS for 32 bit, i think 64 bit anything needs compute capability 6.x+</p>",
        "id": 263311626,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638374197
    },
    {
        "content": "<p>but we default to 6.1</p>",
        "id": 263311656,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638374207
    },
    {
        "content": "<p>x86 doesn't have fetch_max either</p>",
        "id": 263311666,
        "sender_full_name": "The 8472",
        "timestamp": 1638374211
    },
    {
        "content": "<p>CUDA defaults to 5.2 but we default to 6.1 just in anticipation for cuda 12 deprecating 5.1</p>",
        "id": 263311701,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638374228
    },
    {
        "content": "<p>Hmm yeah but LLVM would not be doing the work for us, we would need to do it in codegen because nvvm ir does not support it</p>",
        "id": 263311989,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638374321
    },
    {
        "content": "<p>Although i probably will not be using NVVM IR intrinsics for it, i'll be using intrinsics implemented in cuda_std that use <code>atom</code> instructions on 7.x+ and \"emulate\" the ordering on &lt; 7.x</p>",
        "id": 263312274,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1638374412
    }
]