[
    {
        "content": "<p>I am trying to better understand ThinLTO and rust's use of it, but I haven't been able to find the answers I'm looking for on Google so here we are!</p>\n<ol>\n<li>My understanding is that ThinLTO runs many of the LLVM optimization passes at the link stage.  My testing seems to indicate that it does not do any optimizations within a module/CGU and instead only does optimizations across the modules/CGUs.  Is this correct?</li>\n<li>If ThinLTO does optimizations across modules, does this mean we can increase the codegen units on release builds to improve compile-time parallelism on high core count CPUs without taking a runtime performance hit?</li>\n<li>Does rustc have the ability to do incremental ThinLTO?  That is outlined here: <a href=\"https://clang.llvm.org/docs/ThinLTO.html#incremental\">https://clang.llvm.org/docs/ThinLTO.html#incremental</a></li>\n</ol>\n<p>Thanks!</p>",
        "id": 229160527,
        "sender_full_name": "andbleo",
        "timestamp": 1615096925
    },
    {
        "content": "<ol>\n<li>\n<p>Can you say more why it looks like you're not seeing the expected optimizations? My understanding is that the set of LLVM optimizations applied to a module are slightly different for ThinLTO, but shouldn't be substantially different. See <a href=\"https://github.com/rust-lang/rust/blob/66ec64ccf31883cd2c28d045912a76179c0c6ed2/compiler/rustc_codegen_llvm/src/back/lto.rs#L866-L876\">lto.rs</a> for more details (the Rust side is usually pretty well commented).</p>\n</li>\n<li>\n<p>CGUs on release is already 16 <em>per crate</em>. I'm not sure how many cores you have, but you can always experiment with larger values if you have a lot (like if you have a 96 core CPU), but I suspect it won't get much more parallelism. As with most optimization settings, you may get surprising behavior (it may generate better or worse code with more CGUs), and there may be many bottlenecks that get in the way. You may also run into memory issues with too many threads at once.</p>\n</li>\n<li>\n<p>I believe that if you enable rustc's incremental support, it will do some caching of incremental ThinLTO for the local crate (see <a href=\"https://github.com/rust-lang/rust/issues/53673\">#53673</a>), but not across crates. I'm not sure if it is possible to do more caching, or if there would be much benefit.</p>\n</li>\n</ol>",
        "id": 229203176,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615137682
    },
    {
        "content": "<ol>\n<li>I definitely do see optimizations being run during ThinLTO (-Z time-llvm-passes shows a LOT more passes being run on the final binary than in non-ThinLTO builds).  As I have thought about my reasoning for intramodule optimizations not happening, I now realize it didn't make any sense (sorry), so I won't rehash them here.  That being said, it would still be good to know if ThinLTO only optimizes across modules or if it also optimizes within modules.  If the latter, it might be possible to disable some of the non-ThinLTO pass manager optimizations since the ThinLTO pass manager will eventually run them, which could be a compile time improvement without any hit to runtime perf.</li>\n</ol>",
        "id": 229244275,
        "sender_full_name": "andbleo",
        "timestamp": 1615176190
    },
    {
        "content": "<p>I suspect you want to always do the definitely-worth-it transformations on all the CGUs before the ThinLTO step because then they can be run in parallel.  They may run again in ThinLTO, but ideally then they have to do the things that were exposed by the cross-unit availability, rather than <em>everything</em>.  Incremental probably wants the same -- if a CGU doesn't change, it's nicer to reuse the in-that-CGU optimizations rather than re-running them in ThinLTO every link.</p>",
        "id": 229246713,
        "sender_full_name": "scottmcm",
        "timestamp": 1615178415
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"322305\">@andbleo</span> ThinLTO (and LTO) has pre-link and post-link optimization pipelines, where the latter happen either after module merging (LTO) or cross import (ThinLTO). The pre-link phase is pretty similar to normal optimization, but drops passes at the end of pipeline like vectorization and full unrolling that would interfere with further optimization and cost modeling.</p>",
        "id": 229271233,
        "sender_full_name": "Nikita Popov",
        "timestamp": 1615196360
    },
    {
        "content": "<p>For (3) I believe rustc, with incremental enabled, will do all the incrementality described with clang. If I ever test with a release build I always test it with incremental turned on to speed up codegen times.</p>",
        "id": 229336597,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1615223204
    },
    {
        "content": "<p>For 1/2, ThinLTO itself literally only does cross-module inlining. There's then a standard pass manager you run post-inlining. If you mean the term \"ThinLTO\" to cover the inlining and everything afterwards then, yes, it effectively enables cross-module optimizations (due to inlining).</p>",
        "id": 229336730,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1615223271
    },
    {
        "content": "<p>Ah and when I say \"ThinLTO itself\" I mean all the infrastructure we've implemented for ThinLTO minus the pass manager bits which at least from an integration perspective was less interesting.</p>",
        "id": 229336861,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1615223310
    }
]