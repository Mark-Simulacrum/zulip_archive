[
    {
        "content": "<p>I'm currently looking at the performance of macro expansion on some crates where it's really expensive.</p>\n<ul>\n<li>async-std-1.10.0 (<a href=\"https://github.com/async-rs/async-std/blob/35f768166436112db97224e823b4ee610c81d6d6/src/utils.rs#L247\">macro</a>, <a href=\"https://github.com/async-rs/async-std/blob/b90c2cddd185c087d597922a39a56e038aea7f4e/src/stream/stream/mod.rs#L146\">example usage</a>)</li>\n<li>time-macros-0.2.3 (<a href=\"https://github.com/time-rs/time/blob/592ef915cf6275069f65483216ba7ccb8081b2bc/time-macros/src/quote.rs#L1\">macro</a>, <a href=\"https://github.com/time-rs/time/blob/159cedbf6e99213501d0a12a33b4e46d3291294f/time-macros/src/format_description/component.rs#L27\">example usage</a>)</li>\n<li>yansi-0.5.0 (<a href=\"https://github.com/SergioBenitez/yansi/blob/c5637db6599a72e3d5fd003f7d91f07f47fa3a97/src/docify.rs#L1\">macro</a>, <a href=\"https://github.com/SergioBenitez/yansi/blob/c5637db6599a72e3d5fd003f7d91f07f47fa3a97/src/style.rs#L164\">example usage</a>)</li>\n</ul>",
        "id": 273903757,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268303
    },
    {
        "content": "<p>These are all very complex macros with lots of rules and heavy <code>tt</code> metavariable usage.</p>",
        "id": 273903904,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268403
    },
    {
        "content": "<p>The expense is partly in macro expansion itself (i.e. <code>parse_tt</code>). That code is complex, maintains a lot of state, there's a lot of allocations because chunks of that state gets duplicated a lot (even though it uses <code>Lrc</code> heavily).</p>",
        "id": 273903965,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268458
    },
    {
        "content": "<p>And it's also partly in <code>parse_nonterminal</code>, which gets called a <em>lot</em>. E.g. 5.3 million times when compiling <code>async-std-1.10.0</code>.</p>",
        "id": 273904042,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268498
    },
    {
        "content": "<p>I've been looking closely at the <code>parse_tt</code> code, making lots of measurements. I can get some small to medium wins (e.g. 10%) with some moderate heroics relating to avoiding allocations. But it feels like there is some quadratic behaviour going on, and it might be worth trying to avoid that.</p>",
        "id": 273904128,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268562
    },
    {
        "content": "<p>For example, I printed out the value returned by every call to <code>parse_nonterminal()</code> and counted the frequencies. Here are the most common ones for <code>async-std</code>:</p>\n<div class=\"codehilite\"><pre><span></span><code>5312196 counts\n(  1)     4553 ( 0.1%,  0.1%): tt Token(Token { kind: Pound, span: src/stream/stream/mod.rs:238:9: 238:10 (#0) })\n(  2)     4551 ( 0.1%,  0.2%): tt Delimited(DelimSpan { open: src/stream/stream/mod.rs:238:10: 238:11 (#0), close: src/stream/stream/mod.rs:262:11: 262:12 (#0) }, Bracket, TokenStream([(Token(Token { kind: Ident(&quot;doc&quot;, false), span: src/stream/stream/mod.rs:238:11: 238:14 (#0) }), Alone), (Token(Token { kind: Eq, span: src/stream/stream/mod.rs:238:15: 238:16 (#0) }), Alone), (Token(Token { kind: Literal(Lit { kind: StrRaw(1), symbol: &quot;\\n            Advances the stream and returns the next value.\\n\\n            Returns [`None`] when iteration is finished. Individual stream implementations may\\n            choose to resume iteration, and so calling `next()` again may or may not eventually\\n            start returning more values.\\n\\n            [`None`]: https://doc.rust-lang.org/std/option/enum.Option.html#variant.None\\n\\n            # Examples\\n\\n            ```\\n            # fn main() { async_std::task::block_on(async {\\n            #\\n            use async_std::prelude::*;\\n            use async_std::stream;\\n\\n            let mut s = stream::once(7);\\n\\n            assert_eq!(s.next().await, Some(7));\\n            assert_eq!(s.next().await, None);\\n            #\\n            # }) }\\n            ```\\n        &quot;, suffix: None }), span: src/stream/stream/mod.rs:238:17: 262:11 (#0) }), Alone)]))\n(  3)     4549 ( 0.1%,  0.3%): tt Token(Token { kind: Ident(&quot;fn&quot;, false), span: src/stream/stream/mod.rs:263:9: 263:11 (#0) })\n(  4)     4547 ( 0.1%,  0.3%): tt Token(Token { kind: Ident(&quot;next&quot;, false), span: src/stream/stream/mod.rs:263:12: 263:16 (#0) })\n(  5)     4545 ( 0.1%,  0.4%): tt Delimited(DelimSpan { open: src/stream/stream/mod.rs:263:16: 263:17 (#0), close: src/stream/stream/mod.rs:263:26: 263:27 (#0) }, Paren, TokenStream([(Token(Token { kind: BinOp(And), span: src/stream/stream/mod.rs:263:17: 263:18 (#0) }), Alone), (Token(Token { kind: Ident(&quot;mut&quot;, false), span: src/stream/stream/mod.rs:263:18: 263:21 (#0) }), Alone), (Token(Token { kind: Ident(&quot;self&quot;, false), span: src/stream/stream/mod.rs:263:22: 263:26 (#0) }), Alone)]))\n(  6)     4543 ( 0.1%,  0.5%): tt Token(Token { kind: Ident(&quot;where&quot;, false), span: src/stream/stream/mod.rs:264:9: 264:14 (#0) })\n(  7)     4541 ( 0.1%,  0.6%): tt Token(Token { kind: Ident(&quot;Self&quot;, false), span: src/stream/stream/mod.rs:265:13: 265:17 (#0) })\n(  8)     4539 ( 0.1%,  0.7%): tt Token(Token { kind: Colon, span: src/stream/stream/mod.rs:265:17: 265:18 (#0) })\n(  9)     4538 ( 0.1%,  0.8%): tt Token(Token { kind: Interpolated(NtTy(..)), span: src/utils.rs:309:45: 309:47 (#456) })\n( 10)     4538 ( 0.1%,  0.9%): tt Token(Token { kind: RArrow, span: src/utils.rs:309:42: 309:44 (#456) })\n( 11)     4537 ( 0.1%,  0.9%): tt Token(Token { kind: Ident(&quot;Unpin&quot;, false), span: src/stream/stream/mod.rs:265:19: 265:24 (#0) })\n( 12)     4535 ( 0.1%,  1.0%): tt Token(Token { kind: Comma, span: src/stream/stream/mod.rs:265:24: 265:25 (#0) })\n</code></pre></div>",
        "id": 273904275,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268640
    },
    {
        "content": "<p>All the most common nonterminals are <code>tt</code>s. That first one was parsed 4,553 times. And you can see as you go down the list the frequencies gradually drop. E.g. here's the ones around the 1000th most common nonterminal:</p>\n<div class=\"codehilite\"><pre><span></span><code>(1000)     2601 ( 0.0%, 67.1%): tt Token(Token { kind: Gt, span: src/stream/stream/mod.rs:1790:17: 1790:18 (#0) })\n(1001)     2599 ( 0.0%, 67.2%): tt Delimited(DelimSpan { open: src/stream/stream/mod.rs:1790:18: 1790:19 (#0), close: src/stream/stream/mod.rs:1790:33: 1790:34 (#0) }, Paren, TokenStream([(Token(Token { kind: Ident(&quot;self&quot;, false), span: src/stream/stream/mod.rs:1790:19: 1790:23 (#0) }), Joint), (Token(Token { kind: Comma, span: src/stream/stream/mod.rs:1790:23: 1790:24 (#0) }), Alone), (Token(Token { kind: Ident(&quot;other&quot;, false), span: src/stream/stream/mod.rs:1790:25: 1790:30 (#0) }), Joint), (Token(Token { kind: Colon, span: src/stream/stream/mod.rs:1790:30: 1790:31 (#0) }), Alone), (Token(Token { kind: Ident(&quot;U&quot;, false), span: src/stream/stream/mod.rs:1790:32: 1790:33 (#0) }), Alone)]))\n(1002)     2597 ( 0.0%, 67.2%): tt Token(Token { kind: RArrow, span: src/stream/stream/mod.rs:1790:35: 1790:37 (#0) })\n(1003)     2595 ( 0.0%, 67.3%): tt Token(Token { kind: Ident(&quot;Zip&quot;, false), span: src/stream/stream/mod.rs:1790:38: 1790:41 (#0) })\n(1004)     2593 ( 0.0%, 67.3%): tt Token(Token { kind: Lt, span: src/stream/stream/mod.rs:1790:41: 1790:42 (#0) })\n(1005)     2591 ( 0.0%, 67.4%): tt Token(Token { kind: Ident(&quot;Self&quot;, false), span: src/stream/stream/mod.rs:1790:42: 1790:46 (#0) })\n(1006)     2589 ( 0.0%, 67.4%): tt Token(Token { kind: Comma, span: src/stream/stream/mod.rs:1790:46: 1790:47 (#0) })\n(1007)     2587 ( 0.0%, 67.5%): tt Token(Token { kind: Ident(&quot;U&quot;, false), span: src/stream/stream/mod.rs:1790:48: 1790:49 (#0) })\n(1008)     2585 ( 0.0%, 67.5%): tt Token(Token { kind: Gt, span: src/stream/stream/mod.rs:1790:49: 1790:50 (#0) })\n</code></pre></div>",
        "id": 273904456,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268750
    },
    {
        "content": "<p>Smells very quadratic.</p>",
        "id": 273904465,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268756
    },
    {
        "content": "<p>Despite looking at <code>parse_tt</code> a lot, I admit I still don't fully understand how it works. It's pretty complicated. So my primary question is: is this slowness avoidable?</p>",
        "id": 273904531,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268796
    },
    {
        "content": "<p>Are these macros inherently badly written, and the repetition is inherent? Could <code>parse_tt</code> do something smarter here? Some kind of memoization?</p>",
        "id": 273904579,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268829
    },
    {
        "content": "<p>Parsing the same token trees over and over, thousands of times... it's not good</p>",
        "id": 273904697,
        "sender_full_name": "nnethercote",
        "timestamp": 1646268895
    },
    {
        "content": "<p>Was this token really parsed that many times?</p>",
        "id": 273904935,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269062
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>(  1)     4553 ( 0.1%,  0.1%): tt Token(Token { kind: Pound, span: src/stream/stream/mod.rs:238:9: 238:10 (#0) })\n</code></pre></div>",
        "id": 273904940,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269067
    },
    {
        "content": "<p>Looking at the usage, there's only a single macro invocation: <a href=\"https://github.com/async-rs/async-std/blob/b90c2cddd185c087d597922a39a56e038aea7f4e/src/stream/stream/mod.rs#L238\">https://github.com/async-rs/async-std/blob/b90c2cddd185c087d597922a39a56e038aea7f4e/src/stream/stream/mod.rs#L238</a></p>",
        "id": 273905013,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269092
    },
    {
        "content": "<p>and it doesn't appear to do anything weird like expand to a new macro</p>",
        "id": 273905040,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269108
    },
    {
        "content": "<p>so I'm not sure how that token could be parsed so many times</p>",
        "id": 273905060,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269117
    },
    {
        "content": "<p>It really was</p>",
        "id": 273905122,
        "sender_full_name": "nnethercote",
        "timestamp": 1646269172
    },
    {
        "content": "<p>And the tokens nearby almost as many times (though the counts gradually dropped for the later tokens)</p>",
        "id": 273905221,
        "sender_full_name": "nnethercote",
        "timestamp": 1646269219
    },
    {
        "content": "<p>wow</p>",
        "id": 273905415,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269250
    },
    {
        "content": "<p><code>extension_trait</code> is a recursive macro, I've been assuming that's relevant</p>",
        "id": 273905455,
        "sender_full_name": "nnethercote",
        "timestamp": 1646269280
    },
    {
        "content": "<p>Oh, yeah, it's probably related to this: <a href=\"https://github.com/async-rs/async-std/blob/b90c2cddd185c087d597922a39a56e038aea7f4e/src/utils.rs#L304-L326\">https://github.com/async-rs/async-std/blob/b90c2cddd185c087d597922a39a56e038aea7f4e/src/utils.rs#L304-L326</a></p>",
        "id": 273905465,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269290
    },
    {
        "content": "<p>I think our hands may be partially tied by the design of macros like that</p>",
        "id": 273905558,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269352
    },
    {
        "content": "<p>I'm wondering about <a href=\"https://github.com/async-rs/async-std/blob/35f768166436112db97224e823b4ee610c81d6d6/src/utils.rs#L329-L330\">this</a></p>",
        "id": 273905575,
        "sender_full_name": "nnethercote",
        "timestamp": 1646269370
    },
    {
        "content": "<p>I was thinking more about things like this: <a href=\"https://github.com/async-rs/async-std/blob/b90c2cddd185c087d597922a39a56e038aea7f4e/src/utils.rs#L320-L326\">https://github.com/async-rs/async-std/blob/b90c2cddd185c087d597922a39a56e038aea7f4e/src/utils.rs#L320-L326</a></p>",
        "id": 273905615,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269411
    },
    {
        "content": "<p>where the macro recursively handles one token at a time</p>",
        "id": 273905632,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269427
    },
    {
        "content": "<p>In the past, I've wondered about arena-allocating tokens to replace the (frequent) token cloning by a pointer copy</p>",
        "id": 273905776,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269510
    },
    {
        "content": "<p>but I haven't gotten around to investing the time needed to do that level of refactoring</p>",
        "id": 273905804,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1646269531
    },
    {
        "content": "<p>I don't think that's such a big deal here</p>",
        "id": 273905868,
        "sender_full_name": "nnethercote",
        "timestamp": 1646269563
    },
    {
        "content": "<p>Those rules seem almost custom-designed to blow up macro expansion</p>",
        "id": 273905891,
        "sender_full_name": "nnethercote",
        "timestamp": 1646269577
    },
    {
        "content": "<p>They are laboriously shuffling one <code>tt</code> at a time within the parentheses, until there's little enough on the tail to match one of the earlier rules. If the input is thousands of <code>tt</code>s long, I can easily see how many of them will be parsed thousands of times. And also why the earlier ones gets re-parsed more times than the later ones.</p>",
        "id": 273910249,
        "sender_full_name": "nnethercote",
        "timestamp": 1646272741
    },
    {
        "content": "<p>Yikes!</p>",
        "id": 273911386,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646273580
    },
    {
        "content": "<p>Can this macro be written as a more standard tt muncher?</p>",
        "id": 273911423,
        "sender_full_name": "nnethercote",
        "timestamp": 1646273635
    },
    {
        "content": "<p>I'm also wondering if there's some way we could optimize that pattern and handle it without re-parsing untouched tokens.</p>",
        "id": 273911488,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646273681
    },
    {
        "content": "<p>I mean, I agree that if we can write the macro a better way in the first place we should.</p>",
        "id": 273911492,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646273694
    },
    {
        "content": "<p>But also, if we can catch an n^2 pattern and optimize it into a linear one, we should.</p>",
        "id": 273911503,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646273710
    },
    {
        "content": "<p>Oh look!:</p>\n<div class=\"codehilite\"><pre><span></span><code>src/lib.rs:#![recursion_limit = &quot;2048&quot;]\n</code></pre></div>",
        "id": 273911630,
        "sender_full_name": "nnethercote",
        "timestamp": 1646273854
    },
    {
        "content": "<p>In this context, it <em>seems</em> like we could roughly recognize three things:<br>\n1) There's a series of macros that are all matching the same prefix.<br>\n2) Those macros accumulate into the prefix.<br>\n3) The macros never actually parse anything <em>out</em> of the prefix, ever.</p>",
        "id": 273911637,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646273869
    },
    {
        "content": "<p>That last point seems like the key optimization: if we can recognize an accumulator that is only accumulated into and later emitted, never re-parsed, then we don't re-parse it, we just accumulate into it.</p>",
        "id": 273911731,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646273926
    },
    {
        "content": "<p>Right. There are numerous other crates that all seem to fit into the \"slow macro expansion\" bucket, I've just been looking at the top three offenders</p>",
        "id": 273911732,
        "sender_full_name": "nnethercote",
        "timestamp": 1646273928
    },
    {
        "content": "<p>Along the same lines, if we can recognize a \"source\" (like tail in this case) that's only ever parsed out of and never has things added to it, we could parse it once and then use it as a source repeatedly.</p>",
        "id": 273911837,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646274011
    },
    {
        "content": "<p>I'm not saying this is the right pattern, but the \"source\" case seems like a required part of parsing, and the \"accumulator\" could potentially be avoided by just producing macro output incrementally without passing it to the recursive call but hoisting that out still seems like a worthwhile optimization.</p>",
        "id": 273911872,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646274061
    },
    {
        "content": "<p>Also, the \"accumulator\" case is one that <em>could</em> make sense if you're going to feed it into something else at the end but only <em>once</em>, not repeatedly as you go.</p>",
        "id": 273911913,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646274105
    },
    {
        "content": "<p>Either way, the only \"state\" that syntax macros have and can use is their arguments, so recognizing when that's being used as variables would help.</p>",
        "id": 273911972,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646274133
    },
    {
        "content": "<p>I think these are all using \"push-down accumulation\", which the Little Book of Rust Macros explicitly <a href=\"https://veykril.github.io/tlborm/decl-macros/patterns/push-down-acc.html\">recommends</a>.</p>",
        "id": 273921588,
        "sender_full_name": "nnethercote",
        "timestamp": 1646282268
    },
    {
        "content": "<p>I noticed that the LBORM puts the accumulator at the end of each rule, rather than the start. I just tried changing the <code>@ext</code> rules in <code>async-std</code>'s <code> </code>extension_trait` so the accumulator is at the end, and instruction counts for a check build dropped from 21.3 billion to 14.8 billion(!)</p>",
        "id": 273921713,
        "sender_full_name": "nnethercote",
        "timestamp": 1646282367
    },
    {
        "content": "<p>Which makes sense, because putting the accumulator at the front means that on any accumulator-involving rule that fails, rustc will have re-processed the entire accumulator before hitting a post-accumulator token that doesn't match</p>",
        "id": 273921886,
        "sender_full_name": "nnethercote",
        "timestamp": 1646282496
    },
    {
        "content": "<p>Even with the reordering, we're still re-processing the accumulator on the successful rules</p>",
        "id": 273923355,
        "sender_full_name": "nnethercote",
        "timestamp": 1646283804
    },
    {
        "content": "<p>How feasible would it be to detect the push-down accumulation pattern?</p>",
        "id": 273925610,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1646285881
    },
    {
        "content": "<p>I don't understand the code well enough to answer that question. I'll have to re-read the code with all this in mind...</p>",
        "id": 273929669,
        "sender_full_name": "nnethercote",
        "timestamp": 1646289503
    },
    {
        "content": "<p>since expansion is 75% of <code>async-std</code>'s check time, I'll also look into ways to help in-crate</p>",
        "id": 273948858,
        "sender_full_name": "lqd",
        "timestamp": 1646301459
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116113\">@lqd</span> I'm not sure what you mean by \"in-crate\".</p>",
        "id": 273956745,
        "sender_full_name": "nnethercote",
        "timestamp": 1646305684
    },
    {
        "content": "<p>changing the macro</p>",
        "id": 273956806,
        "sender_full_name": "lqd",
        "timestamp": 1646305714
    },
    {
        "content": "<p>comparing with a proc-macro, minimizing it, finding other ways to achieve the rendered docs output, etc</p>",
        "id": 273956880,
        "sender_full_name": "lqd",
        "timestamp": 1646305755
    },
    {
        "content": "<p>As above, putting the accumulator at the end reduces compile time by about 1/3. But this is a common enough pattern that it would be good to find an in-compiler solution.</p>",
        "id": 273961987,
        "sender_full_name": "nnethercote",
        "timestamp": 1646308863
    },
    {
        "content": "<p>So skim in here as this seem roughly the right topic for this: <br>\nAnother macro that is really expensive to expand is diesel <code>__diesel_for_each_tuple!</code> macro. Especially this call <a href=\"https://github.com/rust-lang/rustc-perf/blob/e8c333e9a6d65bf47a90db6e62922c66ce1fcd33/collector/benchmarks/diesel/diesel/src/type_impls/tuples.rs#L502\">here</a>. This likely does not show up in the default runs as rust-pref \"only\" builds diesel with the default features and not with the <code>128-column-tables</code> flag enabled. That flag blows up the macro expansion time quite a lot. See the <a href=\"https://github.com/rust-lang/rustc-perf/pull/807#issuecomment-746005967\">PR</a> that added diesel to the rustc-pref infrastructure for more details.</p>",
        "id": 273964105,
        "sender_full_name": "weiznich",
        "timestamp": 1646310165
    },
    {
        "content": "<p>doesn't the <a href=\"https://github.com/async-rs/async-std/blob/b90c2cddd185c087d597922a39a56e038aea7f4e/src/utils.rs#L308\">first rule</a> cover the <a href=\"https://github.com/async-rs/async-std/blob/b90c2cddd185c087d597922a39a56e038aea7f4e/src/utils.rs#L316\">second rule</a> ? if expansion is very eager with this recursive second rule, that work will never be used, right ?</p>",
        "id": 273981579,
        "sender_full_name": "lqd",
        "timestamp": 1646318321
    },
    {
        "content": "<p>and if it would make sense to change the rules of what's allowed in order to allow algorithmic speedups in macro expansion, we do have the option of making breaking changes via macros 2.0 :P</p>",
        "id": 273996498,
        "sender_full_name": "bstrie",
        "timestamp": 1646324206
    },
    {
        "content": "<p>@lqd: that's what I thought, but when I removed the second of those <code>@doc</code> rules I got a small difference in the <code>cargo doc</code> output. I don't understand why. Remove the second <code>@ext</code> rule seemed to not affect anything, though.</p>",
        "id": 274034728,
        "sender_full_name": "nnethercote",
        "timestamp": 1646339297
    },
    {
        "content": "<p>I haven't tried the doc ones, but the ext one did improve my measurements</p>",
        "id": 274034855,
        "sender_full_name": "lqd",
        "timestamp": 1646339356
    },
    {
        "content": "<p>I'll try it again on cg soon enough</p>",
        "id": 274034991,
        "sender_full_name": "lqd",
        "timestamp": 1646339404
    },
    {
        "content": "<p>An update: the three crates I've been measuring all defined top-down accumulator macros. I just measured the next seven hottest crates. Six of them were dominated by <code>quote::quote!</code>, which ends up <a href=\"https://github.com/dtolnay/quote/blob/278057d0d2e59c7de95d79d42e4934783d013e83/src/lib.rs#L734\">here</a></p>",
        "id": 274345445,
        "sender_full_name": "nnethercote",
        "timestamp": 1646623529
    },
    {
        "content": "<p>The other one ended up <a href=\"https://github.com/bitflags/bitflags/blob/0bc11cb78b73522bc5dface26b3c30cefff94c3e/src/lib.rs#L434\">here</a> in the <code>bitflags</code> crate</p>",
        "id": 274345457,
        "sender_full_name": "nnethercote",
        "timestamp": 1646623551
    },
    {
        "content": "<p>The <code>quote</code> ones go through <a href=\"https://github.com/dtolnay/quote/blob/278057d0d2e59c7de95d79d42e4934783d013e83/src/lib.rs#L674\">here</a>, which is some pretty interesting macro abuse I admit I don't yet understand</p>",
        "id": 274345551,
        "sender_full_name": "nnethercote",
        "timestamp": 1646623653
    },
    {
        "content": "<p>I think the abuse there is allowing some form of look-ahead/look-back matching on tokens by offsetting the tt-captures by differing amounts.</p>",
        "id": 274486565,
        "sender_full_name": "Lukas Wirth",
        "timestamp": 1646705110
    },
    {
        "content": "<p>Yes. I don't know yet if this is for error messages or something else</p>",
        "id": 274487937,
        "sender_full_name": "nnethercote",
        "timestamp": 1646706714
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"215423\">weiznich</span> <a href=\"#narrow/stream/131828-t-compiler/topic/Macro.20expansion.20performance.20on.20complex.20macros/near/273964105\">said</a>:</p>\n<blockquote>\n<p>So skim in here as this seem roughly the right topic for this: <br>\nAnother macro that is really expensive to expand is diesel <code>__diesel_for_each_tuple!</code> macro. Especially this call <a href=\"https://github.com/rust-lang/rustc-perf/blob/e8c333e9a6d65bf47a90db6e62922c66ce1fcd33/collector/benchmarks/diesel/diesel/src/type_impls/tuples.rs#L502\">here</a>. This likely does not show up in the default runs as rust-pref \"only\" builds diesel with the default features and not with the <code>128-column-tables</code> flag enabled. That flag blows up the macro expansion time quite a lot. See the <a href=\"https://github.com/rust-lang/rustc-perf/pull/807#issuecomment-746005967\">PR</a> that added diesel to the rustc-pref infrastructure for more details.</p>\n</blockquote>\n<p>Aaaand this is why rust needs variadic tuples.</p>",
        "id": 274519779,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1646734372
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"120989\">nnethercote</span> <a href=\"#narrow/stream/131828-t-compiler/topic/Macro.20expansion.20performance.20on.20complex.20macros/near/274345551\">said</a>:</p>\n<blockquote>\n<p>The <code>quote</code> ones go through <a href=\"https://github.com/dtolnay/quote/blob/278057d0d2e59c7de95d79d42e4934783d013e83/src/lib.rs#L674\">here</a>, which is some pretty interesting macro abuse I admit I don't yet understand</p>\n</blockquote>\n<p>Yeah, it's a \"manual\" kind of lookahead. It's actually incredibly clever, if I understand it correctly.</p>",
        "id": 274558833,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1646754426
    },
    {
        "content": "<p>Basically, dtolnay is implementing something you'd normally implement with \"reduce\" by using only \"map\" and \"flatmap\", so to speak. This avoid the quadratic parsing problem mentioned in this thread.</p>\n<p>The quote macro is roughly equivalent to this pseudo-code:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"k\">fn</span> <span class=\"nf\">quote</span><span class=\"p\">(</span><span class=\"n\">tokens</span>: <span class=\"nc\">Tokens</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">tokens_b3</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"fm\">concat!</span><span class=\"p\">(</span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">tokens_b2</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"fm\">concat!</span><span class=\"p\">(</span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">tokens_b1</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"fm\">concat!</span><span class=\"p\">(</span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">tokens_curr</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"fm\">concat!</span><span class=\"p\">(</span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">tokens_a1</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"fm\">concat!</span><span class=\"p\">(</span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">tokens_a2</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"fm\">concat!</span><span class=\"p\">(</span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">tokens_a3</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"fm\">concat!</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'@'</span><span class=\"p\">);</span><span class=\"w\"></span>\n\n<span class=\"w\">    </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">b1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">b2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">b3</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">curr</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">a1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">a2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">a3</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"></span>\n<span class=\"w\">        </span><span class=\"n\">Iter</span>::<span class=\"n\">zip</span><span class=\"p\">(</span><span class=\"n\">tokens_b1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens_b2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens_b3</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">curr_token</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens_a1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens_a2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tokens_a3</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">        </span><span class=\"k\">match</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">b1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">b2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">b3</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">curr</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">a1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">a2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">a3</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n\n<span class=\"w\">            </span><span class=\"p\">(</span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'#'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">inner</span>:<span class=\"nc\">tt</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"sc\">'*'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'_'</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">=&gt;</span><span class=\"w\"> </span><span class=\"n\">do_interpolation</span><span class=\"p\">(</span><span class=\"n\">inner</span><span class=\"p\">),</span><span class=\"w\"></span>\n<span class=\"w\">            </span><span class=\"p\">(</span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'#'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">inner</span>:<span class=\"nc\">tt</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"sc\">'*'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'_'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'_'</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">=&gt;</span><span class=\"w\"> </span><span class=\"n\">skip</span><span class=\"p\">(),</span><span class=\"w\"></span>\n<span class=\"w\">            </span><span class=\"p\">(</span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'#'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">inner</span>:<span class=\"nc\">tt</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"sc\">'*'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'_'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'_'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"sc\">'_'</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">=&gt;</span><span class=\"w\"> </span><span class=\"n\">skip</span><span class=\"p\">(),</span><span class=\"w\"></span>\n\n<span class=\"w\">            </span><span class=\"c1\">// ... other interpolation cases</span>\n\n<span class=\"w\">            </span><span class=\"p\">(</span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">token</span>:<span class=\"nc\">tt</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">_</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">=&gt;</span><span class=\"w\"> </span><span class=\"n\">handle_regular_token</span><span class=\"p\">(</span><span class=\"n\">token</span><span class=\"p\">)</span><span class=\"w\"></span>\n<span class=\"w\">        </span><span class=\"p\">}</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"p\">}</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n<p>This basically \"parallelizes\" parsing: each token is parsed once, with three tokens of lookahead and three tokens of lookbehind. It handles the <code>#(stuff)*</code> syntax when parsing the <code>#</code> token, and compensates by emitting nothing when it parses the <code>(stuff)</code> / <code>*</code> tokens and detects <code>#</code> in lookback.</p>\n<p>This means the complexity of parsing a given <code>quote!</code> invocation O(token_count). This is actually pretty good, and I'm not sure there's any way to do better short of a compiler intrinsic.</p>",
        "id": 274564276,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1646756384
    },
    {
        "content": "<p>I wonder if the macros from async-std, time-macros and yansi could be optimized by following the same model.</p>",
        "id": 274564934,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1646756636
    },
    {
        "content": "<p>It might not even be worth adding caching optimizations or whatnot to the compilers, if macro parsing is only a serious part of build times in a few crates with extremely complex macros.</p>",
        "id": 274565371,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1646756811
    },
    {
        "content": "<p>I think the diesel case is quite different to the others I've been discussing</p>",
        "id": 274611753,
        "sender_full_name": "nnethercote",
        "timestamp": 1646775860
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"263609\">@Olivier FAURE</span> Thanks for the explanation. Even if quote is very clever and optimized, it's still showing up as hot in quite a few of these crates, so anything we can do in rustc (at moderate effort) to improve things would be worthwhile.</p>",
        "id": 274612034,
        "sender_full_name": "nnethercote",
        "timestamp": 1646775992
    },
    {
        "content": "<p>I have spent more time investigating this. I have managed to find one hacky little optimization in the macro expander that reduces compile times of <code>check</code> builds for <code>async-std</code>, <code>time-macros</code> and <code>yansi</code> by 15-20%. But it's a constant time improvement on a quadratic problem. And it's only a 2% win on <code>num-derive</code>, which uses <code>quote</code>.</p>",
        "id": 274787625,
        "sender_full_name": "nnethercote",
        "timestamp": 1646885717
    },
    {
        "content": "<p>Tweaking the macros in those first three crates would be easy and give bigger wins.</p>",
        "id": 274787643,
        "sender_full_name": "nnethercote",
        "timestamp": 1646885757
    },
    {
        "content": "<p>I'm not sure about <code>quote</code>, I still don't understand how it works.</p>",
        "id": 274787690,
        "sender_full_name": "nnethercote",
        "timestamp": 1646885774
    },
    {
        "content": "<p>Avoiding the quadratic-ness of the first three in rustc itself seems hard. The problem is that there are all these recursive invocations of the relevant macro, and each invocation is separate from those before and after it. This makes it hard to maintain state that might allow repetitive operations to be skipped.</p>",
        "id": 274787732,
        "sender_full_name": "nnethercote",
        "timestamp": 1646885850
    },
    {
        "content": "<p>The chapter on push-down accumulators in the Little Book of Rust Macros definitely needs some text about the performance pitfalls here.</p>",
        "id": 274787785,
        "sender_full_name": "nnethercote",
        "timestamp": 1646885889
    },
    {
        "content": "<p>I filed a PR for <code>async-std</code>: <a href=\"https://github.com/async-rs/async-std/pull/1005\">https://github.com/async-rs/async-std/pull/1005</a>. Reduces compile time for a <code>cargo check</code> build by 25%.</p>",
        "id": 274795191,
        "sender_full_name": "nnethercote",
        "timestamp": 1646893411
    },
    {
        "content": "<p>I also did remove the unused rules <a href=\"https://github.com/async-rs/async-std/pull/1004\">https://github.com/async-rs/async-std/pull/1004</a> but didn't do the pushdown change, I knew you'd do it  ^^</p>",
        "id": 274797986,
        "sender_full_name": "lqd",
        "timestamp": 1646896412
    },
    {
        "content": "<p>Something that might help, besides adding optimizations for common cases, is giving macro authors the tools to analyze how expansive their macros are.</p>",
        "id": 274834450,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1646919109
    },
    {
        "content": "<p>Maybe something like <a href=\"https://github.com/dtolnay/cargo-llvm-lines\">cargo-llvm-lines</a>, but instead of sorting generics by emitted line count, it sorts macros by how many intermediary passes they go through?</p>",
        "id": 274834823,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1646919272
    },
    {
        "content": "<p>sure, we’ve discussed this before in a more general context of being able to see the sources of slowness in one’s project (scheduling issues, missed parallelism, heavy monomorphizations, slow proc macros or build scripts, heavy dependencies, spurious rebuilds, etc). </p>\n<p>for macros, one may be able to see some info of these expansion passes via the trace_macros macro, but I’m not sure it contains everything an automated tool would need rn</p>",
        "id": 274835399,
        "sender_full_name": "lqd",
        "timestamp": 1646919563
    },
    {
        "content": "<p>worth looking at for sure</p>",
        "id": 274835533,
        "sender_full_name": "lqd",
        "timestamp": 1646919634
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"120989\">nnethercote</span> <a href=\"#narrow/stream/131828-t-compiler/topic/Macro.20expansion.20performance.20on.20complex.20macros/near/274787785\">말함</a>:</p>\n<blockquote>\n<p>The chapter on push-down accumulators in the Little Book of Rust Macros definitely needs some text about the performance pitfalls here.</p>\n</blockquote>\n<p>I added a small note to the top of the <a href=\"https://veykril.github.io/tlborm/decl-macros/patterns/push-down-acc.html\">page</a></p>",
        "id": 274852006,
        "sender_full_name": "Lukas Wirth",
        "timestamp": 1646926754
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"300586\">@Lukas Wirth</span> Thanks! I may end up adding a bit more detail, but that's an excellent start</p>",
        "id": 274882448,
        "sender_full_name": "nnethercote",
        "timestamp": 1646940025
    },
    {
        "content": "<p>You are more than welcome to <span aria-label=\"big smile\" class=\"emoji emoji-1f604\" role=\"img\" title=\"big smile\">:big_smile:</span></p>",
        "id": 274887848,
        "sender_full_name": "Lukas Wirth",
        "timestamp": 1646942519
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"211722\">@Yoshua Wuyts [he/they]</span> suggested getting rid of the <code>extension_trait</code> macro entirely in <code>async-std</code>, which I did here: <a href=\"https://github.com/async-rs/async-std/pull/1006\">https://github.com/async-rs/async-std/pull/1006</a>. With that, compile time for a non-incremental <code>cargo check</code> build is down by about 75% since I started working on it.</p>",
        "id": 274930578,
        "sender_full_name": "nnethercote",
        "timestamp": 1646972742
    },
    {
        "content": "<p>I also filed <a href=\"https://github.com/time-rs/time/pull/453\">https://github.com/time-rs/time/pull/453</a> for <code>time-macros</code>, roughly halving its compile time for the same scenario.</p>",
        "id": 274930654,
        "sender_full_name": "nnethercote",
        "timestamp": 1646972850
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"300586\">@Lukas Wirth</span> That's two crates now where I've got a big compile-time win by rewriting a push-down accumulator rule so that the accumulator is at the end. The LBORM does have examples with the accumulator at the end, but it would be worthwhile having some text briefly explaining this.</p>",
        "id": 274930723,
        "sender_full_name": "nnethercote",
        "timestamp": 1646972909
    },
    {
        "content": "<p>I also removed the <code>docify!</code> macro from <code>yansi</code>, but I see now that the repo hasn't been touched in almost three years <span aria-label=\"frown\" class=\"emoji emoji-1f641\" role=\"img\" title=\"frown\">:frown:</span></p>",
        "id": 274933797,
        "sender_full_name": "nnethercote",
        "timestamp": 1646977165
    },
    {
        "content": "<p>I filed <a href=\"https://github.com/SergioBenitez/yansi/pull/36\">https://github.com/SergioBenitez/yansi/pull/36</a> anyway, just in case it's ever resurrected</p>",
        "id": 274934010,
        "sender_full_name": "nnethercote",
        "timestamp": 1646977408
    },
    {
        "content": "<p>I'm now wondering: How do we enable ordinary users to get the kind of insights that are leading <span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> to do these experiments? Should the compiler, for example, be gathering time profiles attached to <em>specific macros</em>, so that people observe that there may be opportunities to improve their build times by changing <em>those macros</em> (or eliminating them entirely) ?</p>",
        "id": 274984112,
        "sender_full_name": "pnkfelix",
        "timestamp": 1647009938
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> It's a good question. Possibilities:</p>\n<ul>\n<li>If you run with <code>-Ztimings</code> and <code>expand_crate</code>/<code>macro_expand_crate</code> are big (e.g. <a href=\"https://github.com/lqd/rustc-benchmarking-data/blob/main/results/round-17-time-passes-check/Ztp-w-profiling-async-std-1.10.0-Check-Full.txt\">here</a>), that indicates macro expansion is a problem.</li>\n</ul>",
        "id": 275045495,
        "sender_full_name": "nnethercote",
        "timestamp": 1647039294
    },
    {
        "content": "<ul>\n<li>(Aside: presumably <code>macro_expand_crate</code> is a sub-step of <code>expand_crate</code>. <code>-Ztimings</code> used to use indentation to indicate sub-steps, but that disappeared at some point, which makes me sad.)</li>\n</ul>",
        "id": 275045538,
        "sender_full_name": "nnethercote",
        "timestamp": 1647039331
    },
    {
        "content": "<ul>\n<li><code>trace_macros</code> gives some insight, because you can see how many rule matching there are, but it won't make clear that there might be a linear cost within each rule matching, so the quadratic aspect isn't obvious.</li>\n</ul>",
        "id": 275045633,
        "sender_full_name": "nnethercote",
        "timestamp": 1647039381
    },
    {
        "content": "<ul>\n<li>If you have to adjust <code>recursion_limit</code> a lot, that's a sign that something bad might be happening. (E.g. <a href=\"https://github.com/async-rs/async-std/blob/35f768166436112db97224e823b4ee610c81d6d6/src/lib.rs#L285\">here</a>)</li>\n</ul>",
        "id": 275045851,
        "sender_full_name": "nnethercote",
        "timestamp": 1647039539
    },
    {
        "content": "<p>It's not a lot to work with! I'm not sure how better information would even be exposed to the user. Have some rustc flag or clippy mode that warns if code might cause compile times to be slow?</p>",
        "id": 275046157,
        "sender_full_name": "nnethercote",
        "timestamp": 1647039726
    },
    {
        "content": "<p>One idea would be to add on further timings around each macro invocation (using the macro name)</p>",
        "id": 275063190,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1647042339
    },
    {
        "content": "<p>and add an option to show just those timings</p>",
        "id": 275063201,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1647042353
    },
    {
        "content": "<p>Showing which macros are expensive in such a way would be already helpful, but it's still probably going to be hard to know <em>why</em> they are expensive (and fix them) without more metrics, e.g. the number of expansions, token tree sizes, success/failure stats about rule matching, etc</p>\n<p>If y'all also think adding timing information would be a good first step, I'll try to work on that. Recording and showing timings via <code>-Zself-profile-events</code> seems interesting. But I know nothing about the macro expansion code, so if <span class=\"user-mention\" data-user-id=\"125294\">@Aaron Hill</span> you would have the time to mentor me bit about it, and provide a few pointers, that would be greatly appreciated.</p>",
        "id": 275087102,
        "sender_full_name": "lqd",
        "timestamp": 1647077047
    },
    {
        "content": "<p>One useful measurement might be the number of times <code>parse_nonterminal</code> is called -- that's the best signal of quadratic-ness</p>",
        "id": 275309003,
        "sender_full_name": "nnethercote",
        "timestamp": 1647298610
    },
    {
        "content": "<p>In terms of other things that could be detected as possible causes of slow compile times, the other good possibility I can think of is if a function gets really large, e.g. lots of BBs or local variables.</p>",
        "id": 275328604,
        "sender_full_name": "nnethercote",
        "timestamp": 1647318306
    },
    {
        "content": "<p>i.e. \"Function <code>f</code> is very large, consider making it smaller\".</p>",
        "id": 275328614,
        "sender_full_name": "nnethercote",
        "timestamp": 1647318329
    },
    {
        "content": "<p>There are a few crates I know (e.g. <code>keccak</code> and <code>inflate</code> in <code>rustc-perf</code>, <code>http-0.2.6</code> on <a href=\"http://crates.io\">crates.io</a>) that would be hit by that.</p>",
        "id": 275328627,
        "sender_full_name": "nnethercote",
        "timestamp": 1647318362
    },
    {
        "content": "<p>clippy has a <code>perf</code> category, this could possibly be a new category, <code>compile-time</code> or something</p>",
        "id": 275328681,
        "sender_full_name": "nnethercote",
        "timestamp": 1647318395
    },
    {
        "content": "<p>Also, one difficulty I have is that any such command-line option is going to be unstable at first, maybe for years, which means you need to use a nightly compiler even if your current compiler has the option.</p>",
        "id": 275355056,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647340509
    },
    {
        "content": "<p>That's annoying because:</p>\n<ul>\n<li>You need to have a nightly toolchain installed and remember to use the <code>+nightly</code> flag.</li>\n<li>If you're used to compiling with a stable toolchain, switching to nightly will recompile all your program and your dependencies, which is kind of a big flow breaker.</li>\n</ul>",
        "id": 275355401,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647340707
    },
    {
        "content": "<p>I understand the rationale when this is about language features or cargo configs (they might be changed later), but in that case you're just trying to glean some information about your build process. There's almost no way using an experimental instrumentation flag in stable leads to breakage, unless the flag is used in CI and dropped, or parsed by a third-party tool and the format changes (and even those are a stretch).</p>",
        "id": 275355614,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647340827
    },
    {
        "content": "<p>I wish there was some kind of middle-ground for read-only unstable options, something like \"You're allowed to use them in stable, but only from a user console, not a pipe or a script. The option gives you a warning in a console and an error in a script\".</p>",
        "id": 275355716,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647340916
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"263609\">@Olivier FAURE</span> I now understand what <code>quote</code> is doing. The advantage of that approach is that the macro is no longer recursive, so you can process longer inputs without having to increase <code>recursion_limit</code>. But it's not  a speed win, it's still quadratic.</p>",
        "id": 275596964,
        "sender_full_name": "nnethercote",
        "timestamp": 1647479387
    },
    {
        "content": "<p>I tried converting <code>quote!</code> to a standard TT muncher and it was faster in some cases. But unfortunately standard TT munchers are <em>also</em> quadratic <span aria-label=\"frown\" class=\"emoji emoji-1f641\" role=\"img\" title=\"frown\">:frown:</span></p>",
        "id": 275597023,
        "sender_full_name": "nnethercote",
        "timestamp": 1647479413
    },
    {
        "content": "<p>Because they work like this: \"process N tokens\", \"process N-1 tokens\", ... \"process 1 token\"</p>",
        "id": 275597059,
        "sender_full_name": "nnethercote",
        "timestamp": 1647479490
    },
    {
        "content": "<p>Push-down accumulation macros are twice quadratic, because they have that behaviour for the input, and they're also building up an output whose size goes 1, 2, 3, ... N.</p>",
        "id": 275597117,
        "sender_full_name": "nnethercote",
        "timestamp": 1647479525
    },
    {
        "content": "<p>In summary, TT munchers are 0.5N^2, while push-down accumulators are N^2</p>",
        "id": 275597206,
        "sender_full_name": "nnethercote",
        "timestamp": 1647479648
    },
    {
        "content": "<p>I would expect that at least the output will not be inspected too much by a TT muncher, meaning that you are just copying pointers around. At least for some possible implementations of the macro engine it should be possible to make these patterns O(n log n) or O(n)</p>",
        "id": 275597928,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1647480449
    },
    {
        "content": "<p>You might expect that, but it's not what happens in the current compiler.</p>",
        "id": 275604410,
        "sender_full_name": "nnethercote",
        "timestamp": 1647488527
    },
    {
        "content": "<p>For a simple macro like this:</p>\n<div class=\"codehilite\"><pre><span></span><code>macro_rules! munch {\n    ($token:tt $($tail:tt)*) =&gt; {\n        let _x = 3;\n        munch!($($tail)*)\n    };\n    () =&gt; {};\n}\n</code></pre></div>",
        "id": 275604423,
        "sender_full_name": "nnethercote",
        "timestamp": 1647488555
    },
    {
        "content": "<p>The number of calls to <code>parse_nonterminal</code> is quadratic relative to the number of input tokens</p>",
        "id": 275604432,
        "sender_full_name": "nnethercote",
        "timestamp": 1647488577
    },
    {
        "content": "<blockquote>\n<p>But it's not  a speed win, it's still quadratic.</p>\n</blockquote>\n<p>What do you mean? It should be linear.</p>",
        "id": 275627205,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647510311
    },
    {
        "content": "<p>To be specific, if I'm understanding the macro correctly (I haven't tested it or anything):</p>\n<p>For every instance of <code>quote!</code> called with <code>N</code> tokens:</p>\n<ul>\n<li><code>quote_each_token!</code> is called once with <code>N</code> tokens</li>\n<li><code>quote_tokens_with_context!</code> is called once with (roughly) <code>7N + 36</code> tokens</li>\n<li><code>quote_token_with_context!</code> is called <code>N + 6</code> times with <code>7</code> tokens each.</li>\n<li><code>quote_token</code> is called <code>O(N)</code> times with a single token or so.</li>\n</ul>\n<p>Now, <code>quote_token!</code> is actually recursive, and calls <code>quote!</code> on group tokens eg <code>(), [], {}</code>. I'm not sure that's what you mean by quadratic?</p>",
        "id": 275636906,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647515792
    },
    {
        "content": "<p>If it is, it sounds like a problem with the macro engine, not the quote macro.</p>",
        "id": 275636947,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647515816
    },
    {
        "content": "<p>I think token matching is linear in input size. So if the macro is invoked with an input size linear in the original input size (so not say a single token each time) an amount of times linear in the input size the resulting runtime is quadratic (linear times linear).</p>",
        "id": 275637952,
        "sender_full_name": "bjorn3",
        "timestamp": 1647516426
    },
    {
        "content": "<p>Yeah, but that's not the case.</p>",
        "id": 275647995,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647522055
    },
    {
        "content": "<p>AFAICT, each sub-macro is either called <code>O(N)</code> times with <code>O(1)</code> tokens, or <code>O(1)</code> times with <code>O(N)</code> tokens</p>",
        "id": 275648246,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647522137
    },
    {
        "content": "<p>Like, that's the whole point of the weird parsing scheme.</p>",
        "id": 275648450,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647522196
    },
    {
        "content": "<p>(Paging <span class=\"user-mention\" data-user-id=\"119235\">@David Tolnay</span>, since we're discussing his lib)</p>",
        "id": 275648678,
        "sender_full_name": "Olivier FAURE",
        "timestamp": 1647522283
    },
    {
        "content": "<p>Right</p>",
        "id": 275652866,
        "sender_full_name": "bjorn3",
        "timestamp": 1647524179
    },
    {
        "content": "<p>Yeah I don't immediately understand how it would be quadratic, other than in nesting depth which is at most let's say 10 ever.</p>",
        "id": 275674067,
        "sender_full_name": "David Tolnay",
        "timestamp": 1647532005
    },
    {
        "content": "<p>True, my mistake</p>",
        "id": 275768506,
        "sender_full_name": "nnethercote",
        "timestamp": 1647589432
    }
]