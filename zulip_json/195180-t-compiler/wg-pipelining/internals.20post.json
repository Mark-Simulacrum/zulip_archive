[
    {
        "content": "<p>Cargo support is now in nightly! I'm drafting up an internals post and will post it here soon</p>",
        "id": 165913206,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558109327
    },
    {
        "content": "<p>Ok, <a href=\"https://internals.rust-lang.org/t/evaluating-pipelined-rustc-compilation/10199\" target=\"_blank\" title=\"https://internals.rust-lang.org/t/evaluating-pipelined-rustc-compilation/10199\">posted</a>!</p>",
        "id": 165913277,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558109379
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> rustup update only gets me 2019-05-16</p>",
        "id": 165914372,
        "sender_full_name": "Gankra",
        "timestamp": 1558110282
    },
    {
        "content": "<p>oh I should probably say that</p>",
        "id": 165914401,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558110319
    },
    {
        "content": "<p>I just guessed</p>",
        "id": 165914406,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558110323
    },
    {
        "content": "<p>? so 16 has pipelining?</p>",
        "id": 165914426,
        "sender_full_name": "Gankra",
        "timestamp": 1558110346
    },
    {
        "content": "<p><code>rustc 1.36.0-nightly (7d5aa4332 2019-05-16)</code> that has pipelining</p>",
        "id": 165914682,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558110528
    },
    {
        "content": "<p>I think it's <code>rustup update nightly-2019-05-17</code></p>",
        "id": 165914696,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558110539
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116009\">@nikomatsakis</span> with lark that just made pipelined compilation a sealed and done deal for me</p>",
        "id": 165916106,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558111651
    },
    {
        "content": "<p>those numbers are \"this is what rustc devs will see once we switch everything to rlibs\"</p>",
        "id": 165916124,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558111669
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> I was surprised</p>",
        "id": 165916126,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1558111671
    },
    {
        "content": "<p>I'm collecting all the results in a spreadsheet here - <a href=\"https://docs.google.com/spreadsheets/d/1CU7o3IocPtNAUevvrPsTI77z_AexsRloUgiusiXJWtg/edit?usp=sharing\" target=\"_blank\" title=\"https://docs.google.com/spreadsheets/d/1CU7o3IocPtNAUevvrPsTI77z_AexsRloUgiusiXJWtg/edit?usp=sharing\">https://docs.google.com/spreadsheets/d/1CU7o3IocPtNAUevvrPsTI77z_AexsRloUgiusiXJWtg/edit?usp=sharing</a></p>",
        "id": 165933186,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558124381
    },
    {
        "content": "<p>from the internals post</p>",
        "id": 165933188,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558124384
    },
    {
        "content": "<p>if you want you can make the google spreadsheet coloring be gradient which .. could be helpful? (i.e., from light green for low % wins to darker for high % wins)</p>",
        "id": 165933327,
        "sender_full_name": "simulacrum",
        "timestamp": 1558124486
    },
    {
        "content": "<p>oh nice</p>",
        "id": 165933439,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558124557
    },
    {
        "content": "<p>/me pokes</p>",
        "id": 165933444,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558124561
    },
    {
        "content": "<p>nice, that does look much better</p>",
        "id": 165933778,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558124849
    },
    {
        "content": "<p>So an interesting result I'm seeing is that this feels like some compelling data that we want to do the next step of pipelining</p>",
        "id": 165936641,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558127025
    },
    {
        "content": "<p>which is that cargo should be able to pipeline compliations that end in the linker</p>",
        "id": 165936649,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558127035
    },
    {
        "content": "<p>(e.g. compiling a binary or a proc-macro)</p>",
        "id": 165936667,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558127042
    },
    {
        "content": "<p>projects can always be reorganized to have a tiny crate which is the binary which basically just serves the purpose of calling the linker</p>",
        "id": 165936714,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558127060
    },
    {
        "content": "<p>but it's a bummer to have to do that manually</p>",
        "id": 165936721,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558127069
    },
    {
        "content": "<p>although this is also the same w/ sccache where rlibs are just inherently easier to work with than linked artifacts</p>",
        "id": 165936732,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558127086
    },
    {
        "content": "<p>fwiw I think there's already a general desire/feeling in the community to do the tiny binary split and big library backing</p>",
        "id": 165937173,
        "sender_full_name": "simulacrum",
        "timestamp": 1558127454
    },
    {
        "content": "<p>so that might be somewhat low priority -- I wonder how difficult it would be to get stats on that</p>",
        "id": 165937197,
        "sender_full_name": "simulacrum",
        "timestamp": 1558127476
    },
    {
        "content": "<p>maybe lines of code in <a href=\"http://main.rs\" target=\"_blank\" title=\"http://main.rs\">main.rs</a> after expansion (i.e., including modules) and <a href=\"http://lib.rs\" target=\"_blank\" title=\"http://lib.rs\">lib.rs</a> or something along those lines</p>",
        "id": 165937212,
        "sender_full_name": "simulacrum",
        "timestamp": 1558127503
    },
    {
        "content": "<p>that's true yeah, a bunch of stuff is already architected this way but I know of big stuff which isn't at least</p>",
        "id": 165938590,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558128760
    },
    {
        "content": "<p>e.g. unit tests can take awhile to compile, cargo's binary is relatively big, sccache's is huge,e tc</p>",
        "id": 165938595,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558128777
    },
    {
        "content": "<p>same w/ serde_derive</p>",
        "id": 165938604,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558128783
    },
    {
        "content": "<p>yeah, I think proc macros would probably benefit quite a bit from that layout</p>",
        "id": 165938998,
        "sender_full_name": "simulacrum",
        "timestamp": 1558129154
    },
    {
        "content": "<p>since generally they're written inline, somewhat unlike binary/library split since proc macro internals aren't really public, unlike lots of binary internals</p>",
        "id": 165939027,
        "sender_full_name": "simulacrum",
        "timestamp": 1558129201
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> you typoed the environment variable in one place in your post and it's causing chaos lol</p>",
        "id": 165988303,
        "sender_full_name": "Gankra",
        "timestamp": 1558208534
    },
    {
        "content": "<p>oopsie</p>",
        "id": 165989047,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558209806
    },
    {
        "content": "<p>So continuing to collate data, it looks like pipelining across the board is not a regression on any front, it averages a 10% reduction in build time, and we're seeing up to twice as fast builds</p>",
        "id": 166091074,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558363773
    },
    {
        "content": "<p>I've opened a <a href=\"https://github.com/rust-lang/rust/issues/60988\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/issues/60988\">dedicated tracking issue now</a> for stabilizing pipelined compilation given how compelling the data is</p>",
        "id": 166123896,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558390168
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> Thanks for the measurements and internals post. Initial results are promising, though the improvements are not as high/universal as I had hoped.</p>",
        "id": 166125908,
        "sender_full_name": "njn",
        "timestamp": 1558392101
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> But I can't manage to get the 2019-05-17 Cargo to test. <code>rustup update nightly</code> gives me 2019-05-15, as does <code>rustup update nightly-2019-05-17</code>. What am I missing?</p>",
        "id": 166125980,
        "sender_full_name": "njn",
        "timestamp": 1558392145
    },
    {
        "content": "<p>my rustc is 2019-05-19 after <code>rustup update nightly</code>, but the cargo is older</p>",
        "id": 166126025,
        "sender_full_name": "njn",
        "timestamp": 1558392203
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> Were all the non-results on small numbers of cores? I think its reasonable to expect that Cargo + codegen-units is pretty good at keeping a small number of cores busy. The main wins will be when there's currently a lot of idle cores.</p>",
        "id": 166126139,
        "sender_full_name": "Jeremy Fitzhardinge",
        "timestamp": 1558392312
    },
    {
        "content": "<p>I don't know. I have a 14-physical-core machine, I plan to test the full rustc-perf benchmark (~30 programs) once I can</p>",
        "id": 166126162,
        "sender_full_name": "njn",
        "timestamp": 1558392351
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> if you <code>rustup update nightly</code> it should be good enough</p>",
        "id": 166127231,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393419
    },
    {
        "content": "<p>the commit/date reported by <code>-V</code> is the commit date not the build date</p>",
        "id": 166127237,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393431
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> I agree it's not quite as good as I hoped, but still well within the range of \"worth the stabilization effort\"</p>",
        "id": 166127288,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393452
    },
    {
        "content": "<p>I think there's actually even more to be gained by going a step further and pipelining up to the linker</p>",
        "id": 166127296,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393465
    },
    {
        "content": "<p>but we'd want to gather more data first</p>",
        "id": 166127300,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393472
    },
    {
        "content": "<p>it's also worth keeping in mind what I mentioned in the first post on the thread that incremental builds are likely to see a much bigger benefit than whole crate builds</p>",
        "id": 166127322,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393509
    },
    {
        "content": "<p>but everyone's only been measuring whole crate builds</p>",
        "id": 166127330,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393514
    },
    {
        "content": "<p>which makes the most sense of course because it's the easiest thing to do</p>",
        "id": 166127333,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393520
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> oh, ok, thanks for the clarification. I definitely agree it's worth stabilizing, and there may be room for more improvement, e.g. by generating metadata in parallel with type-checking/borrow-checking.</p>",
        "id": 166127336,
        "sender_full_name": "njn",
        "timestamp": 1558393526
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> I'd sort of love to get to a world where Cargo can spawn literally all rustc instances for a crate graph immediately, and then rustc just queries cargo every now and then for \"wake me up when this is ready\" or \"this is ready\" and Cargo orchestrates the notifications</p>",
        "id": 166127512,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393717
    },
    {
        "content": "<p>like I could imagine rustc being a black box to Cargo and it just says \"needs Vec&lt;String&gt; or \"produced String\" and Cargo just wakes things up as necessary assuming all the strings are unique and all</p>",
        "id": 166127544,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393758
    },
    {
        "content": "<p>that way we could at least parse the whole crate graph in parallel</p>",
        "id": 166127557,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393784
    },
    {
        "content": "<p>would of course be much more difficult to implement :)</p>",
        "id": 166127560,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393792
    },
    {
        "content": "<p>I guess I'm also thinking that like if a crate requires a procedural macro it actually can progress largely through the whole resolution phase of the compiler until it finally needs the macro</p>",
        "id": 166127625,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393826
    },
    {
        "content": "<p>I dunno, these may all be small wins</p>",
        "id": 166127629,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558393833
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> All interesting ideas! Anyway, I should be pleased with the currently results, given that they are quite good for what is the absolute simplest implementation :)  I will do the rustc-perf measurements over the next couple of days, including incremental. I'll do Firefox as well.</p>",
        "id": 166127942,
        "sender_full_name": "njn",
        "timestamp": 1558394197
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> I've been thinking about a model where a graph orchestration engine can start a graph of compilers and then start feeding them incremental input at the keystroke level (ie, rather than linking cargo into rls, make it cheap enough for rls to invoke \"real\" builds). That would require lots of fine-grained control over what outputs you want from each compilation step, which perhaps changes dynamically.</p>",
        "id": 166128533,
        "sender_full_name": "Jeremy Fitzhardinge",
        "timestamp": 1558394751
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"213049\">@Jeremy Fitzhardinge</span> that does indeed sound like the dream :)</p>",
        "id": 166128600,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558394794
    },
    {
        "content": "<p>jturner was talking about that at the last all-hands</p>",
        "id": 166128613,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558394813
    },
    {
        "content": "<p>certainly an ambitious goal :)</p>",
        "id": 166128614,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558394819
    },
    {
        "content": "<p>I'm very interesting in continuing the conversation about how Buck and Cargo can work together. My project for the back half of the year will be trying to auto-generate buck build rules from Cargo.toml for <a href=\"http://crates.io\" target=\"_blank\" title=\"http://crates.io\">crates.io</a>, and there's a number of improvements to Cargo's model I'd like to discuss in that context.</p>",
        "id": 166128841,
        "sender_full_name": "Jeremy Fitzhardinge",
        "timestamp": 1558395009
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> wait, what is this pipelining up to if not the linker?</p>",
        "id": 166138354,
        "sender_full_name": "Gankra",
        "timestamp": 1558408128
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"137587\">@Gankro</span> If I understood you correctly, then we currently pipeline split after metadata (i.e., same output as you get from cargo check) is done but LLVM and linker has not yet run</p>",
        "id": 166138454,
        "sender_full_name": "simulacrum",
        "timestamp": 1558408264
    },
    {
        "content": "<p>so this is proposing splitting the llvm+link step into two parts? What could we possibly execute only once llvm was done, but before linking is done?</p>",
        "id": 166138516,
        "sender_full_name": "Gankra",
        "timestamp": 1558408367
    },
    {
        "content": "<p>er, not sure what you mean -- the stable/no flags cargo has no pipelining</p>",
        "id": 166138584,
        "sender_full_name": "simulacrum",
        "timestamp": 1558408498
    },
    {
        "content": "<p>(maybe I wasn't clear)</p>",
        "id": 166138590,
        "sender_full_name": "simulacrum",
        "timestamp": 1558408504
    },
    {
        "content": "<p>i.e., we always wait for LLVM and linking to finish if we're running it (debug/release builds)</p>",
        "id": 166138597,
        "sender_full_name": "simulacrum",
        "timestamp": 1558408522
    },
    {
        "content": "<p>alex said \"I think there's actually even more to be gained by going a step further and pipelining up to the linker\"</p>",
        "id": 166138599,
        "sender_full_name": "Gankra",
        "timestamp": 1558408534
    },
    {
        "content": "<p>I believe that might be referring to us not pipelining the final binary -- which can often be quite large</p>",
        "id": 166138603,
        "sender_full_name": "simulacrum",
        "timestamp": 1558408562
    },
    {
        "content": "<p>but not sure</p>",
        "id": 166138647,
        "sender_full_name": "simulacrum",
        "timestamp": 1558408566
    },
    {
        "content": "<p>I'm also .. not entirely sure what that would mean</p>",
        "id": 166138655,
        "sender_full_name": "simulacrum",
        "timestamp": 1558408606
    },
    {
        "content": "<p>oh right, the diagram does say we wait for all our libs to be fully codegen'd before even starting to compile the binary, which is odd yeah</p>",
        "id": 166138670,
        "sender_full_name": "Gankra",
        "timestamp": 1558408632
    },
    {
        "content": "<p>but I guess in theory we could run LLVM before dependency LLVMs finish?</p>",
        "id": 166138671,
        "sender_full_name": "simulacrum",
        "timestamp": 1558408634
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"137587\">@Gankro</span> yeah so the method of pipelining is pretty course and selective now, only when an rlib depends on another rlib can we pipeline those two compilations</p>",
        "id": 166173836,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558446675
    },
    {
        "content": "<p>we, for example, can't pipeline an executable depending on a bunch of other rlibs</p>",
        "id": 166173852,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558446688
    },
    {
        "content": "<p>(or any linked artifact for that matter)</p>",
        "id": 166173860,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558446695
    },
    {
        "content": "<p>similarly we can't pipeline anything depending on a build script or a procedural macro</p>",
        "id": 166173872,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558446706
    },
    {
        "content": "<p>so the general idea is just enabling more paralellism by having better synchronization between rustc/cargo</p>",
        "id": 166173886,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558446719
    },
    {
        "content": "<p>cool</p>",
        "id": 166174001,
        "sender_full_name": "Gankra",
        "timestamp": 1558446780
    },
    {
        "content": "<p>Why do the compiler invocations in <a href=\"https://github.com/rust-lang/compiler-team/blob/master/working-groups/pipelining/NOTES.md#step-2-work-with-only-metadata-as-input\" target=\"_blank\" title=\"https://github.com/rust-lang/compiler-team/blob/master/working-groups/pipelining/NOTES.md#step-2-work-with-only-metadata-as-input\">https://github.com/rust-lang/compiler-team/blob/master/working-groups/pipelining/NOTES.md#step-2-work-with-only-metadata-as-input</a> require <code>rustc libA.rs --emit metadata,link --crate-type lib</code> instead of <code>rustc libA.rs --emit metadata --crate-type lib</code>? <br>\nI can see the the <code>libA.rmeta</code>produced is different <em>sometimes</em>, but that's surprising.</p>\n<p>Fiddling with it locally and run into an ICE w/ the latter:</p>\n<div class=\"codehilite\"><pre><span></span>&gt;&gt;  rustc a.rs --emit=metadata --crate-type=lib\n&gt;&gt; rustc b.rs --emit=link --crate-type=rlib --extern=a=liba.rmeta\nerror: internal compiler error: src/librustc_mir/monomorphize/collector.rs:775: Cannot create local mono-item for DefId(13:12 ~ a[8787]::number[0])\n\nthread &#39;rustc&#39; panicked at &#39;Box&lt;Any&gt;&#39;, src/librustc_errors/lib.rs:637:9\nnote: Run with `RUST_BACKTRACE=1` environment variable to display a backtrace.\nerror: aborting due to previous error\n\n\nnote: the compiler unexpectedly panicked. this is a bug.\n\nnote: we would appreciate a bug report: https://github.com/rust-lang/rust/blob/master/CONTRIBUTING.md#bug-reports\n\nnote: rustc 1.36.0-nightly (37ff5d388 2019-05-22) running on x86_64-unknown-linux-gnu\n\nnote: compiler flags: --crate-type rlib\n</pre></div>",
        "id": 166337317,
        "sender_full_name": "Marco",
        "timestamp": 1558594496
    },
    {
        "content": "<p>I was looking to sketch the invoke-rustc-twice approach in Bazel, but ran into that immediately</p>",
        "id": 166337391,
        "sender_full_name": "Marco",
        "timestamp": 1558594579
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"222771\">@Marco</span> currently in rustc metadat is different if you emit just metadata vs if you emit metadata + a lib</p>",
        "id": 166378472,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558630052
    },
    {
        "content": "<p>we also want to use one rustc process to emit both</p>",
        "id": 166378478,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558630057
    },
    {
        "content": "<p>( so we don't need one rustc process for metadata and one for the lib)</p>",
        "id": 166378494,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558630073
    },
    {
        "content": "<p>Is it intentionally different? Does it make sense that it's non-deterministically different?</p>\n<p>I wanted to see if the redundant metadata generation work in doing 2 rustc invocations from bazel is still a benefit, since that is substantially simpler to implement.</p>",
        "id": 166382326,
        "sender_full_name": "Marco",
        "timestamp": 1558632971
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> I did some measurements: <a href=\"https://internals.rust-lang.org/t/evaluating-pipelined-rustc-compilation/10199/62?u=nnethercote\" target=\"_blank\" title=\"https://internals.rust-lang.org/t/evaluating-pipelined-rustc-compilation/10199/62?u=nnethercote\">https://internals.rust-lang.org/t/evaluating-pipelined-rustc-compilation/10199/62?u=nnethercote</a></p>",
        "id": 166421627,
        "sender_full_name": "njn",
        "timestamp": 1558671240
    },
    {
        "content": "<p>Wow <span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> that's quite comprehensive! Thanks for gathering all that!</p>",
        "id": 166456713,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558707468
    },
    {
        "content": "<p>FWIW rustc sees no benefit b/c it has no pipelining opportunities</p>",
        "id": 166456726,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558707477
    },
    {
        "content": "<p>and I think in general that's why pipelining isn't as great as we expected is that there's just fewere pipelining opportunities than we originally though</p>",
        "id": 166456796,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1558707496
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"222771\">@Marco</span> I also want to prototype pipelining with separate invocations, so it would be useful to be able to do <code>--emit metadata</code> which generates the same .rmetas as <code>--emit metadata,rlib</code>. I wonder if <code>-Zalways-encode-mir</code> makes up the difference?</p>",
        "id": 166490339,
        "sender_full_name": "Jeremy Fitzhardinge",
        "timestamp": 1558732070
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"213049\">@Jeremy Fitzhardinge</span> I don't think <code>-Zalways-encode-mir</code> will be enough (you can read more about that at <a href=\"https://github.com/rust-lang/rust/issues/58465#issuecomment-479032740\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/issues/58465#issuecomment-479032740\">https://github.com/rust-lang/rust/issues/58465#issuecomment-479032740</a>), unless it has been changed recently.  I also would expect running the compiler twice would be substantially slower, since the second invocation would need to repeat all the work of the first (unless you intend to do incremental everywhere?).  On average, the codegen portion only covers about 30% of compile time, so it would be repeating the first 70% for every crate.</p>",
        "id": 166492691,
        "sender_full_name": "Eric Huss",
        "timestamp": 1558733844
    },
    {
        "content": "<p>The thread doesn't say whether <code>-Zalways-encode-mir</code> suffices or not. Running the compiler twice should still enable a speedup if there's unused parallelism (and it is much easier to implement/experiment with in Bazel this way), but it's not strictly better like the alternative. </p>\n<p>Has changing compilation unit to module instead of crate come up in this vein? In the abstract (ie. in ignorance) it seems like a similar way to make more pipelining possible.</p>",
        "id": 166499037,
        "sender_full_name": "Marco",
        "timestamp": 1558741033
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> why does rustc have no pipelining opportunities? librustc takes so long that I was hoping to get some speedup there.</p>",
        "id": 166595406,
        "sender_full_name": "njn",
        "timestamp": 1558910120
    },
    {
        "content": "<p>The time is spent in LLVM, which is parallel already. Might help with incremental when few LLVM modules change</p>",
        "id": 166599658,
        "sender_full_name": "Zoxc",
        "timestamp": 1558917722
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> rustc is entirely dylibs right now, which means that there are no pipelining opportunities</p>",
        "id": 166711001,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1559046687
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/pull/59800\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/pull/59800\">https://github.com/rust-lang/rust/pull/59800</a> is the solution for that</p>",
        "id": 166711024,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1559046707
    },
    {
        "content": "<p>I see, thanks for the info</p>",
        "id": 166766597,
        "sender_full_name": "njn",
        "timestamp": 1559083186
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116015\">@Alex Crichton</span> (asking this again:) Has changing compilation unit to module instead of crate come up in this vein? In the abstract (ie. in ignorance) it seems like a similar way to make more pipelining possible.</p>",
        "id": 166932309,
        "sender_full_name": "Marco",
        "timestamp": 1559239989
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"222771\">@Marco</span> oh oops sorry must have missed this earlier! Currently we haven't considered that, mostly because it doesn't really fit cleanly into the compilation model today and would be a pretty significant undertaking</p>",
        "id": 166945589,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1559249941
    },
    {
        "content": "<p>the current implementation of pipelining was actually very low effort for what is hoped to be quite a high reward (relative)</p>",
        "id": 166945606,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1559249954
    },
    {
        "content": "<p>but you're right in that it's by no means the end-all-be-all of pipelining compilations, and there's a lot of theoretical possibilities for how we can improve things even more</p>",
        "id": 166945632,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1559249981
    },
    {
        "content": "<p>it's mostly just a question of balancing that with the amoutn of effort needed to implement</p>",
        "id": 166945647,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1559249996
    },
    {
        "content": "<p>Got it. Is there anywhere that has listed out some of those possibilities?</p>",
        "id": 166950533,
        "sender_full_name": "Marco",
        "timestamp": 1559254407
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"222771\">@Marco</span> not currently, but we should probably make one!</p>",
        "id": 166999373,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1559311340
    }
]