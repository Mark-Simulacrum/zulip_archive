[
    {
        "content": "<p>I wrote up a doc to drive our discussion today of benchmark categorization: <a href=\"https://hackmd.io/@rust-compiler-team/rw-v-synth-dawn-of-benchmarks\">https://hackmd.io/@rust-compiler-team/rw-v-synth-dawn-of-benchmarks</a></p>",
        "id": 265294450,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639747306
    },
    {
        "content": "<p>(We will be meeting in about 92 minutes or so)</p>",
        "id": 265295139,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639747692
    },
    {
        "content": "<p><span class=\"user-group-mention\" data-user-group-id=\"897\">@T-compiler/meeting</span> ^</p>",
        "id": 265295150,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639747700
    },
    {
        "content": "<p>hi <span class=\"user-group-mention\" data-user-group-id=\"897\">@T-compiler/meeting</span> , lets get started; add a <span aria-label=\"wave\" class=\"emoji emoji-1f44b\" role=\"img\" title=\"wave\">:wave:</span> to show you’re here</p>",
        "id": 265306681,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753309
    },
    {
        "content": "<p>and I want to ping <span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> and <span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> as well</p>",
        "id": 265306697,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753316
    },
    {
        "content": "<p><span aria-label=\"wave\" class=\"emoji emoji-1f44b\" role=\"img\" title=\"wave\">:wave:</span> I'm around</p>",
        "id": 265306781,
        "sender_full_name": "simulacrum",
        "timestamp": 1639753330
    },
    {
        "content": "<p>awesome</p>",
        "id": 265306795,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753337
    },
    {
        "content": "<p>I figured we could start by taking maybe 10 or 15 minutes to read over the <a href=\"https://hackmd.io/@rust-compiler-team/rw-v-synth-dawn-of-benchmarks\">doc</a> I posted above, and also everyone should look at <a href=\"https://perf.rust-lang.org\">https://perf.rust-lang.org</a> itself, just to establish context.</p>",
        "id": 265306896,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753403
    },
    {
        "content": "<p>Lets see, what’s the right pattern for the emojis</p>",
        "id": 265306941,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753431
    },
    {
        "content": "<p>Click the <span aria-label=\"book\" class=\"emoji emoji-1f4d6\" role=\"img\" title=\"book\">:book:</span> when you start reading the <a href=\"https://hackmd.io/@rust-compiler-team/rw-v-synth-dawn-of-benchmarks\">doc</a> (and leave it clicked). Click the <span aria-label=\"checkered flag\" class=\"emoji emoji-1f3c1\" role=\"img\" title=\"checkered flag\">:checkered_flag:</span> when you are done reading.</p>",
        "id": 265307020,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753454
    },
    {
        "content": "<p>if you have questions about the text itself, you can use hackmd’s commenting system to highlight those parts. I’ll add a section at the very bottom, after the appendices, for people to post questions/topics they want to explicitly discuss here</p>",
        "id": 265307251,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753560
    },
    {
        "content": "<p>Hi folks!</p>",
        "id": 265307471,
        "sender_full_name": "rylev",
        "timestamp": 1639753650
    },
    {
        "content": "<p>Okay, the <a href=\"https://hackmd.io/kOkd1YSNRWesWYogxL4pGw?both#Meeting-QuestionsTopics\">place</a> to add explicit discussion points has now been added to the document. (Though, ha ha, the link doesn’t quite work as I expect from the non-two-column view…)</p>",
        "id": 265307577,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753687
    },
    {
        "content": "<p>Okay it looks like everyone has clicked both the <span aria-label=\"book\" class=\"emoji emoji-1f4d6\" role=\"img\" title=\"book\">:book:</span> and the <span aria-label=\"checkered flag\" class=\"emoji emoji-1f3c1\" role=\"img\" title=\"checkered flag\">:checkered_flag:</span></p>",
        "id": 265307938,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753850
    },
    {
        "content": "<p>I see zero comments added via the hackmd interface</p>",
        "id": 265307973,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753873
    },
    {
        "content": "<p>and the only meeting  question/topic was the one I added at the bottom.</p>",
        "id": 265308019,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753893
    },
    {
        "content": "<p>I think an important question is less about how we categorize benchmarks, but how do we use the categorization to decide if a PR is acceptable perf-wise</p>",
        "id": 265308033,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639753898
    },
    {
        "content": "<p>okay lets add that to the bottoom</p>",
        "id": 265308056,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753909
    },
    {
        "content": "<p>(on mobile, it's a bit hard to switch back and forth to the doc)</p>",
        "id": 265308182,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639753946
    },
    {
        "content": "<p>no problem</p>",
        "id": 265308211,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753952
    },
    {
        "content": "<p>A little off-topic - I think it would be good to be able to display changes in disk usage / artifact sizes, in the same way that we display changes in compilation time</p>",
        "id": 265308251,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639753961
    },
    {
        "content": "<p>E.g. a listing of percentage changes</p>",
        "id": 265308265,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639753969
    },
    {
        "content": "<p>lets maybe take a few minutes now: people, just feel free to say whatever you want about this topic here</p>",
        "id": 265308284,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753981
    },
    {
        "content": "<p>Right now, the only way to see artifact size changes is to open each individual benchmark page</p>",
        "id": 265308299,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639753986
    },
    {
        "content": "<p>and we’ll collect it all into topics after you’re done purging</p>",
        "id": 265308321,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639753996
    },
    {
        "content": "<p>and there can often be trade-offs between compilation speed and disk usage</p>",
        "id": 265308339,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639754003
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"125294\">Aaron Hill</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265308299\">said</a>:</p>\n<blockquote>\n<p>Right now, the only way to see artifact size changes is to open each individual benchmark page</p>\n</blockquote>\n<p>This is a well known limitation and has simply not been implemented. We'd love to implement it but just haven't had the time yet</p>",
        "id": 265308370,
        "sender_full_name": "rylev",
        "timestamp": 1639754017
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265308033\">said</a>:</p>\n<blockquote>\n<p>I think an important question is less about how we categorize benchmarks, but how do we use the categorization to decide if a PR is acceptable perf-wise</p>\n</blockquote>\n<p>We in general leave it up to not-very-scientific heuristics on what is acceptable to merge or not. We'll likely want to decide what is acceptable for \"important\" benchmarks first and then decide how other categories differ.</p>",
        "id": 265308527,
        "sender_full_name": "rylev",
        "timestamp": 1639754093
    },
    {
        "content": "<p>Right now we use statistics extensively to hide what we consider to be irrelevant (right now only those that are within noise, I think?) changes in performance. I think it is important that we are careful to not hide what we consider synthetic benchmarks the same way.</p>",
        "id": 265308567,
        "sender_full_name": "nagisa",
        "timestamp": 1639754117
    },
    {
        "content": "<p>On a similar topic: I think our current dashboard both shows too much (as in, too many benchmarks, or at least too many toy ones that are given equal prominence to real-world ones), but it also shows too little (having to switch between different metrics, rather than being able to see multiple metrics at once.</p>",
        "id": 265308568,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754118
    },
    {
        "content": "<p>anyone else have any big picture discussion points to raise before we dive into them individually?</p>",
        "id": 265308813,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754202
    },
    {
        "content": "<p>We also need to take profile (e.g., release, debug, doc, etc.) and scenario (i.e., incremental comp with different changes and cache states) into account</p>",
        "id": 265308816,
        "sender_full_name": "rylev",
        "timestamp": 1639754202
    },
    {
        "content": "<p>Okay. So let me see</p>",
        "id": 265309132,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754283
    },
    {
        "content": "<p>The topics I see here: I think both <span class=\"user-mention\" data-user-id=\"232957\">@Jack Huey</span> and <span class=\"user-mention\" data-user-id=\"123586\">@nagisa</span> ’s points can be combined into one: Assuming <em>some</em> (simple) classification, maybe a binary one, how would we <strong>use</strong> it to improve things?</p>",
        "id": 265309275,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754361
    },
    {
        "content": "<p>does that sound like it captures both of your intents, <span class=\"user-mention\" data-user-id=\"123586\">@nagisa</span> and <span class=\"user-mention\" data-user-id=\"232957\">@Jack Huey</span> ?</p>",
        "id": 265309309,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754379
    },
    {
        "content": "<p>and <span class=\"user-mention\" data-user-id=\"125294\">@Aaron Hill</span> ’s point I think can be combined with the one I added: what metrics should we gather and make prominent, by default, in our dashboard.</p>",
        "id": 265309437,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754426
    },
    {
        "content": "<p>Seems close enough. I was kind of worried more about accidentally making things worse in certain scenarios, even if they get better in others.</p>",
        "id": 265309440,
        "sender_full_name": "nagisa",
        "timestamp": 1639754428
    },
    {
        "content": "<p>Another thought: Thinking about the nofib section of the doc, I wonder if it's worth have a more extensive \"real-world\" collection of benchmarks that runs e.g. nightly but takes longer</p>",
        "id": 265309449,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639754433
    },
    {
        "content": "<p>That’s an interesting idea</p>",
        "id": 265309487,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754448
    },
    {
        "content": "<p>Being able to add some benchmarks around projection caching would be great</p>",
        "id": 265309509,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639754459
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265309449\">said</a>:</p>\n<blockquote>\n<p>Another thought: Thinking about the nofib section of the doc, I wonder if it's worth have a more extensive \"real-world\" collection of benchmarks that runs e.g. nightly but takes longer</p>\n</blockquote>\n<p>crater but for compiler performance</p>",
        "id": 265309546,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639754478
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265309449\">said</a>:</p>\n<blockquote>\n<p>Another thought: Thinking about the nofib section of the doc, I wonder if it's worth have a more extensive \"real-world\" collection of benchmarks that runs e.g. nightly but takes longer</p>\n</blockquote>\n<p>I had a similar idea from the opposite end of the spectrum: Make the toy benchmarks not part of the default flow, but rather, a tool that developers would use when trying to dissect the root cause of some regression that was witnesssed.</p>",
        "id": 265309660,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754530
    },
    {
        "content": "<p>so, okay, it sounds like <span class=\"user-mention\" data-user-id=\"232957\">@Jack Huey</span> ’s thought might be worthy of a separate topic</p>",
        "id": 265309748,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754573
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265309660\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265309449\">said</a>:</p>\n<blockquote>\n<p>Another thought: Thinking about the nofib section of the doc, I wonder if it's worth have a more extensive \"real-world\" collection of benchmarks that runs e.g. nightly but takes longer</p>\n</blockquote>\n<p>I had a similar idea from the opposite end of the spectrum: Make the toy benchmarks not part of the default flow, but rather, a tool that developers would use when trying to dissect the root cause of some regression that was witnesssed.</p>\n</blockquote>\n<p>What about regressions that only show up on \"toy\" benchmarks?</p>",
        "id": 265309789,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639754594
    },
    {
        "content": "<p>For example, I think some large CTFE regressions were only noticed on the 'stress' benchmarks</p>",
        "id": 265309839,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639754617
    },
    {
        "content": "<p>Would it make sense to have benchmarks run by default run only the subset that seems relevant to changed directories and stuff? Much like you wouldn't e.g. assign somebody from T-libs to review a LLVM change or vice versa.</p>",
        "id": 265309868,
        "sender_full_name": "nagisa",
        "timestamp": 1639754629
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"125294\">@Aaron Hill</span>  So, that may get at the heart of my concern</p>",
        "id": 265309875,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754635
    },
    {
        "content": "<p>That would give us an opportunity to also benchmark not just the compiler but also libstd and stuff more readily</p>",
        "id": 265309956,
        "sender_full_name": "nagisa",
        "timestamp": 1639754657
    },
    {
        "content": "<p>My concern would be that I often use perf runs when I don't know what the impact of a change is</p>",
        "id": 265310006,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639754678
    },
    {
        "content": "<p>and even when I think I do, there may be suprises</p>",
        "id": 265310019,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639754686
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"123586\">nagisa</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265309868\">said</a>:</p>\n<blockquote>\n<p>Would it make sense to have benchmarks run by default run only the subset that seems relevant to changed directories and stuff? Much like you wouldn't e.g. assign somebody from T-libs to review a LLVM change or vice versa.</p>\n</blockquote>\n<p>I’m not sure how easy it is to predict this ahead of time</p>",
        "id": 265310027,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754691
    },
    {
        "content": "<p>stdlib performance will probably also have an impact on compiler performance.</p>",
        "id": 265310057,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639754705
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"125294\">Aaron Hill</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265309789\">said</a>:</p>\n<blockquote>\n<p>What about regressions that only show up on \"toy\" benchmarks?</p>\n</blockquote>\n<p>My thinking here is that we, at the very least, need to distinguish between “<em>important</em> toy benchmarks” and non-important ones. And then, non-important toy benchmarks should have a much higher threshold for when we care about a regression.</p>",
        "id": 265310210,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754771
    },
    {
        "content": "<p>Right now, we rely on a lot of human intuition and group discussion to figure this out</p>",
        "id": 265310261,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754800
    },
    {
        "content": "<p>(one might also reasonably argue that we shouldn’t be spending benchmarking cycles on non-important toys)</p>",
        "id": 265310337,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754839
    },
    {
        "content": "<p>I'm not sure I fully understand what \"important\" benchmarks are. If they are unimportant, why do we have them?</p>",
        "id": 265310348,
        "sender_full_name": "rylev",
        "timestamp": 1639754844
    },
    {
        "content": "<p>Right. Do you think we ever go and remove benchmarks that we’ve decided do not reflect any stakeholder nor the community at large?</p>",
        "id": 265310389,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754869
    },
    {
        "content": "<p>nor the community at large?</p>",
        "id": 265310409,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754875
    },
    {
        "content": "<p>I don't think we've removed any benchmark in the last year at least</p>",
        "id": 265310473,
        "sender_full_name": "rylev",
        "timestamp": 1639754891
    },
    {
        "content": "<p>(nor has there been a discussion about this as far as I recall)</p>",
        "id": 265310499,
        "sender_full_name": "rylev",
        "timestamp": 1639754910
    },
    {
        "content": "<p>Yes. I suspect we tend to keep all of them. And its not like thats totally unjustified.</p>",
        "id": 265310505,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754915
    },
    {
        "content": "<p>but it does complicate analysis</p>",
        "id": 265310532,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639754927
    },
    {
        "content": "<p>I'm not sure we have enough coverage to consider removing more than one or two benchmarks.</p>",
        "id": 265310621,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639754943
    },
    {
        "content": "<p>It seems like the importance of a benchmark is more like a multiplier on how much we care about the changes in it's performance: important benchmarks could be a 1x multiplier and non-important benchmarks a 0.5x multiplier. </p>\n<p>If there's a large regression in a non-important benchmark then that's the circumstance in which we care about it, and the \"value\" of the regression will still be significant, the magnitude of the regression will make up for the multiplier. Similarly, if there's a smaller regression in a important benchmark (serde) or something, then the multiplier means that this has a larger \"value\" than the equivalent smaller regression in a non-important benchmark.</p>",
        "id": 265310660,
        "sender_full_name": "davidtwco",
        "timestamp": 1639754948
    },
    {
        "content": "<p>If we did have a more extensive set of benchmarks, we could learn what micro benches are most predictive of real benches</p>",
        "id": 265310780,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639754968
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265310780\">said</a>:</p>\n<blockquote>\n<p>If we did have a more extensive set of benchmarks, we could learn what micro benches are most predictive of real benches</p>\n</blockquote>\n<p>Right, and this is where less-frequent runs could be important. I.e., we could be a lot more liberal about what benchmarks we include if the runs were only once-per-night.</p>",
        "id": 265310942,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755014
    },
    {
        "content": "<p>(and we would continue to have an important subset of those benchmarks run against every commit)</p>",
        "id": 265310997,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755038
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116107\">davidtwco</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265310660\">said</a>:</p>\n<blockquote>\n<p>It seems like the importance of a benchmark is more like a multiplier on how much we care about the changes in it's performance: important benchmarks could be a 1x multiplier and non-important benchmarks a 0.5x multiplier. </p>\n</blockquote>\n<p>I agree with this (I might quibble with the specific numbers, but that’s minor). But I continue to think that the presentation should change as well</p>",
        "id": 265311162,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755113
    },
    {
        "content": "<p>Yeah, the numbers are just for example's sake.</p>",
        "id": 265311181,
        "sender_full_name": "davidtwco",
        "timestamp": 1639755122
    },
    {
        "content": "<p>oh jeez I just realized that I don’t think <span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> was pinged on this meeting, even though they are a clear stakeholder here.</p>",
        "id": 265311229,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755134
    },
    {
        "content": "<p>We probably also would want a way to opt in to running all benchmarks for example when doing <code>@rust-timer queue</code>.</p>",
        "id": 265311239,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639755141
    },
    {
        "content": "<p>I am not too worried about more independent benchmarks; we can scale horizontally to more machines if necessary, it's \"just\" some relatively low amount of work. I think our impediment to more benchmarks has primarily been data-overload</p>",
        "id": 265311243,
        "sender_full_name": "simulacrum",
        "timestamp": 1639755144
    },
    {
        "content": "<p>It's 2am for him, I think he's probably not going to make this one <span aria-label=\"sleeping\" class=\"emoji emoji-1f634\" role=\"img\" title=\"sleeping\">:sleeping:</span></p>",
        "id": 265311285,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639755169
    },
    {
        "content": "<p>oh, right</p>",
        "id": 265311298,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755174
    },
    {
        "content": "<p>(but yeah, we should definitely try to get feedback from him as well before making any substantial changes)</p>",
        "id": 265311360,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639755209
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265311243\">said</a>:</p>\n<blockquote>\n<p>I am not too worried about more independent benchmarks; we can scale horizontally to more machines if necessary, it's \"just\" some relatively low amount of work</p>\n</blockquote>\n<p>Hmm, are we not concerned about variance between machines making the data harder to interpret?</p>",
        "id": 265311376,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755221
    },
    {
        "content": "<p>I think we would pin benchmarks to machines</p>",
        "id": 265311400,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639755234
    },
    {
        "content": "<p>for some reason I thought that was a concern, but perhaps its outdated</p>",
        "id": 265311404,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755236
    },
    {
        "content": "<p>right, yeah, pinning would be the solution tot hat.</p>",
        "id": 265311423,
        "sender_full_name": "simulacrum",
        "timestamp": 1639755242
    },
    {
        "content": "<p>okay</p>",
        "id": 265311451,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755245
    },
    {
        "content": "<p>and at that point, one would just have to take (even more) care when comparing different benchmarks.</p>",
        "id": 265311486,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755268
    },
    {
        "content": "<p>(and ideally being on near-equivalent hardware, too, though that may be a harder thing to do.)</p>",
        "id": 265311513,
        "sender_full_name": "simulacrum",
        "timestamp": 1639755290
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"125294\">@Aaron Hill</span> , I think you and I would agree that we could get a lot of value out of revising the perf site to be able to display more one metric at once, in some manner. Does that sound right?</p>",
        "id": 265311690,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755377
    },
    {
        "content": "<p>Question: do we have info from crater on how long it takes to build all the crates? <span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span></p>",
        "id": 265311706,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639755388
    },
    {
        "content": "<p>It might be worth considering the \"kind\" of regression any given benchmark is written to try detect. Some benchmarks are just for general performance regression (we don't want serde to get slower in general, it isn't exercising any part of the compiler in particular), whereas others are more canary-in-the-coalmine type benchmarks to catch accidental non-linear regressions in specific circumstances/parts of the compiler. Perhaps this is just another way of framing \"importance\" or splitting that category up.</p>",
        "id": 265311712,
        "sender_full_name": "davidtwco",
        "timestamp": 1639755394
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"125294\">@Aaron Hill</span> would you be willing to spend some time working with me to try to design+prototype alternative?</p>",
        "id": 265311760,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755418
    },
    {
        "content": "<p>(anyone else is welcome to chime in there too. I just want to identify stakeholders who have both interest and time/resources)</p>",
        "id": 265311798,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755443
    },
    {
        "content": "<p>Sure! (full disclosure, I have basically no experience with designing/prototyping web pages :))</p>",
        "id": 265311799,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639755443
    },
    {
        "content": "<p>Yeah, me either. My main experience here is with messing with gnuplot or latex to get graphs to align in a printed page</p>",
        "id": 265311862,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755481
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265311706\">said</a>:</p>\n<blockquote>\n<p>Question: do we have info from crater on how long it takes to build all the crates? <span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span></p>\n</blockquote>\n<p>Not readily available, but in theory could be extracted I think. Crater data is likely to be incredibly noisy though: the machines are seriously over-subscribed (e.g., 2-3x, I suspect, more threads than cores).</p>",
        "id": 265311955,
        "sender_full_name": "simulacrum",
        "timestamp": 1639755516
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"125294\">Aaron Hill</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265311799\">said</a>:</p>\n<blockquote>\n<p>Sure! (full disclosure, I have basically no experience with designing/prototyping web pages :))</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> and I have talked about this before. I welcome and applaud anyone else to take a dive into this. I've been really busy lately, but assuming that gets better, I'd be happy to help brainstorm ideas</p>",
        "id": 265312033,
        "sender_full_name": "rylev",
        "timestamp": 1639755568
    },
    {
        "content": "<p>I more am just curious, because in theory we could selection a \"representative\" subset of crates of various build times for a \"real-world\" suite</p>",
        "id": 265312090,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639755596
    },
    {
        "content": "<p>Yeah, I'm in a similar place in general in being open to discussion.</p>",
        "id": 265312101,
        "sender_full_name": "simulacrum",
        "timestamp": 1639755599
    },
    {
        "content": "<p>Okay</p>",
        "id": 265312271,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755639
    },
    {
        "content": "<p>one really important take away I’m currently getting: Some people have definitely expressed concern with risk of hiding or obscuring currently visible benchmark data</p>",
        "id": 265312322,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755674
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"315412\">@jsha</span> is very busy but has a lot of experience designing UIs, you might be able to convince him to help</p>",
        "id": 265312351,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1639755691
    },
    {
        "content": "<p>i.e. more than one person I think has relayed risk with just saying “toy benchmark =&gt; hide it from view and don’t include it in summary\"</p>",
        "id": 265312365,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755700
    },
    {
        "content": "<p>I think that if a benchmark is running during the main perf suite, there's a reason it's included and we shouldn't hide it</p>",
        "id": 265312547,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639755730
    },
    {
        "content": "<p>I’m not so convinced that’s universally true (i.e. I think some benchmarks might represent baggage that could be discarded)</p>",
        "id": 265312705,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755759
    },
    {
        "content": "<p>But I’m also willing to say there’s no reason to outright hide anything</p>",
        "id": 265312845,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755783
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> that's not consistent with the current site though, it hides a <em>ton</em> of benchmarks that aren't significant</p>",
        "id": 265313058,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1639755833
    },
    {
        "content": "<p>Maybe your position is more nuanced and you mean nothing should be hidden <em>unconditionally</em>?</p>",
        "id": 265313185,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1639755851
    },
    {
        "content": "<p>yes that’s what I meant</p>",
        "id": 265313209,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755857
    },
    {
        "content": "<p>I think the main problem is that the page requires a lot of background context to interpret meaningfully. Some users are trying to work on perf improvements and have that context (or are trying to acquire it) and other people just see that the perf bot left a comment on their PR, click through and have no idea what they're looking at. </p>\n<p>Part of designing a better UI is likely going to have to take into account the UX of who is visiting the page and why.</p>",
        "id": 265313269,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639755868
    },
    {
        "content": "<p>Well, thinking about <span class=\"user-mention\" data-user-id=\"116107\">@davidtwco</span>'s comment on \"muliplier\", I think we could conceivably rank benchmark data not by the percentage of the benchmark itself, but by the expected percentage of change in \"real-world\" scenarios</p>",
        "id": 265313328,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639755880
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"232957\">@Jack Huey</span> I think that basically aligns with what <span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> had proposed, though <span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> ’s proposal was a simple two-level scheme</p>",
        "id": 265313520,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639755929
    },
    {
        "content": "<p>while it sounds like what <span class=\"user-mention\" data-user-id=\"116107\">@davidtwco</span> ’s multiplier and <span class=\"user-mention\" data-user-id=\"232957\">@Jack Huey</span> ’s \"expected percentage of change in \"real-world” scenarios” means would be a more dynamic selection of how to prioritize the output</p>",
        "id": 265313690,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756006
    },
    {
        "content": "<p>i.e. sorting by impact</p>",
        "id": 265313717,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756014
    },
    {
        "content": "<p>its a tough call</p>",
        "id": 265313727,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756021
    },
    {
        "content": "<p>e.g. I’m a little hesitant to sort the benchmark graphs themselves by impact</p>",
        "id": 265313754,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756037
    },
    {
        "content": "<p>because there is value in having consistency</p>",
        "id": 265313767,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756046
    },
    {
        "content": "<p>when you are trying to compare today’s data against last weeks</p>",
        "id": 265313781,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756053
    },
    {
        "content": "<p>fwiw, it's not clear to me whether prioritizing the output/impact is <em>quite</em> moving us toward better interface: I'm not sure that sorting is going to help the average user. My (perhaps wrong) assumption is that most people really don't know what to do with the 10-20 numbers, roughly, they may get from a perf run.</p>\n<p>Sorting those numbers or showing <em>even more</em> metadata may not help</p>",
        "id": 265313811,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756074
    },
    {
        "content": "<p>It's also true to think that not all real crates will be affected by different changes equally (it sounds obvious, but it's subtle)</p>",
        "id": 265313822,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639756081
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265313690\">said</a>:</p>\n<blockquote>\n<p>while it sounds like what <span class=\"user-mention silent\" data-user-id=\"116107\">davidtwco</span> ’s multiplier and <span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> ’s \"expected percentage of change in \"real-world” scenarios” means would be a more dynamic selection of how to prioritize the output</p>\n</blockquote>\n<p>It could also just be another column that acts as a numeric indicator of \"how much you should care about this\", making that context that some have explicit.</p>",
        "id": 265313900,
        "sender_full_name": "davidtwco",
        "timestamp": 1639756104
    },
    {
        "content": "<p>If there's a PR that makes 10% of crates 5x faster, but the remaning 10% slower, is that acceptable?</p>",
        "id": 265313938,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639756127
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265313811\">said</a>:</p>\n<blockquote>\n<p>fwiw, it's not clear to me whether prioritizing the output/impact is <em>quite</em> moving us toward better interface: I'm not sure that sorting is going to help the average user. My (perhaps wrong) assumption is that most people really don't know what to do with the 10-20 numbers, roughly, they may get from a perf run.</p>\n<p>Sorting those numbers or showing <em>even more</em> metadata may not help</p>\n</blockquote>\n<p>That is true too, and may speak in favor of <span class=\"user-mention\" data-user-id=\"120989\">@nnethercote</span> ’s proposal, in that if you just put the most important benchmarks in a section at the top, with a dedicated summary graph for <em>just</em> those benchmarks ...</p>",
        "id": 265313961,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756141
    },
    {
        "content": "<p>(and that change could be explained by a single micro-benchmark)</p>",
        "id": 265313977,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639756152
    },
    {
        "content": "<p>then that may be easier for the average person to interpret</p>",
        "id": 265313984,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756155
    },
    {
        "content": "<p>i.e. they won’t know whether “externs regressed by 0.8%” is important</p>",
        "id": 265314027,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756177
    },
    {
        "content": "<p>but seeing “diesel regressed by 0.8%” is hopefully more meaningful?</p>",
        "id": 265314058,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756195
    },
    {
        "content": "<p>maybe? but it's still unclear to me -- if the expected action to such a regression is \"go run cachegrind and improve performance on your PR\", then it's not really different whether it's externs or diesel regressing</p>",
        "id": 265314220,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756238
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265313938\">said</a>:</p>\n<blockquote>\n<p>If there's a PR that makes 10% of crates 5x faster, but the remaning 10% slower, is that acceptable?</p>\n</blockquote>\n<p>Did you mean to say 90% for one of those two percentages?</p>",
        "id": 265314222,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756238
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265314220\">said</a>:</p>\n<blockquote>\n<p>maybe? but it's still unclear to me -- if the expected action to such a regression is \"go run cachegrind and improve performance on your PR\", then it's not really different whether it's externs or diesel regressing</p>\n</blockquote>\n<p>I guess I was more thinking along the lines of “will my PR be rejected based on performance concerns\"</p>",
        "id": 265314475,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756280
    },
    {
        "content": "<p>I.e. I don’t want people who only see a 0.8% regression on externs to spend any of their time thinking about performance investigation.</p>",
        "id": 265314541,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756303
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265314541\">said</a>:</p>\n<blockquote>\n<p>I.e. I don’t want people who only see a 0.8% regression on externs to spend any of their time thinking about performance investigation.</p>\n</blockquote>\n<p>this seems interesting to me</p>",
        "id": 265314673,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756335
    },
    {
        "content": "<p>and that is where the tiered scheme matters to me</p>",
        "id": 265314688,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756339
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265314673\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265314541\">said</a>:</p>\n<blockquote>\n<p>I.e. I don’t want people who only see a 0.8% regression on externs to spend any of their time thinking about performance investigation.</p>\n</blockquote>\n<p>this seems interesting to me</p>\n</blockquote>\n<p>oh, you think someone <em>should</em> react to that, and do more performance investigation?</p>",
        "id": 265314730,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756354
    },
    {
        "content": "<p>we can have higher thresholds for showing that data / separate toggle/section</p>",
        "id": 265314737,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756357
    },
    {
        "content": "<p>but I <em>would</em> expect someone to justify the 0.8% statistically significant regression</p>",
        "id": 265314793,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756373
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265314222\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265313938\">said</a>:</p>\n<blockquote>\n<p>If there's a PR that makes 10% of crates 5x faster, but the remaning 10% slower, is that acceptable?</p>\n</blockquote>\n<p>Did you mean to say 90% for one of those two percentages?</p>\n</blockquote>\n<p>no, but I changed the wording a bit to be more clear</p>",
        "id": 265314821,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639756384
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265314793\">said</a>:</p>\n<blockquote>\n<p>but I <em>would</em> expect someone to justify the 0.8% statistically significant regression</p>\n</blockquote>\n<p>Not me, not on that benchmark.</p>",
        "id": 265314841,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756393
    },
    {
        "content": "<p>Not if that was the <em>only</em> significant regression</p>",
        "id": 265314873,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756410
    },
    {
        "content": "<p>this is partly why I tried to tease apart the various properties in the doc.</p>",
        "id": 265314894,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756424
    },
    {
        "content": "<p>I guess what I would say: I expect <em>some</em> justification. It may be that for externs I'd expect 15 minutes of investigation, whereas diesel might suggest more time, though.</p>",
        "id": 265314989,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756450
    },
    {
        "content": "<p>No first time contributor is going to come up with any useful data in 15 minutes</p>",
        "id": 265315043,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1639756473
    },
    {
        "content": "<p>yeah I’m worried it ends up having an indavertant gate-keeping effect</p>",
        "id": 265315085,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756488
    },
    {
        "content": "<p>And even experienced contributors will probably take a while unless it's very obvious</p>",
        "id": 265315091,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1639756490
    },
    {
        "content": "<p>I'm not saying it's necessarily on the author</p>",
        "id": 265315096,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756492
    },
    {
        "content": "<p>I think this is a core \"indicator\" for perf: At what point does someone need to justify or fix a perf \"regression\"</p>",
        "id": 265315148,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639756511
    },
    {
        "content": "<blockquote>\n<p>I'm not saying it's necessarily on the author</p>\n</blockquote>\n<p>We should make that a <em>lot</em> more clear in the comment the bot leaves then</p>",
        "id": 265315163,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1639756517
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265315096\">said</a>:</p>\n<blockquote>\n<p>I'm not saying it's necessarily on the author</p>\n</blockquote>\n<p>Then whom? do you think its a good use of the weekly performance triage time?</p>",
        "id": 265315205,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756532
    },
    {
        "content": "<p>I've seen at least one person spending hours to investigate a regression</p>",
        "id": 265315213,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1639756535
    },
    {
        "content": "<p>reviewer, for example</p>",
        "id": 265315227,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756538
    },
    {
        "content": "<p>I've been the reviewer on several PRs that have been held up for long times (sometimes months) over a perf regression of 2-3%</p>",
        "id": 265315254,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639756548
    },
    {
        "content": "<p>ultimately it's the reviewers call whether they deem more investigation is necessary before they r+</p>",
        "id": 265315334,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756562
    },
    {
        "content": "<p>Mark does bring up a good point that we have different audiences - author, reviewer, perf team, compiler team, etc?</p>",
        "id": 265315363,
        "sender_full_name": "rylev",
        "timestamp": 1639756563
    },
    {
        "content": "<p>in general, I think if we don't care about triaging small regressions on these benchmarks, we will tend to regress (over time) in a not-small way on them. </p>\n<p>it may be that the triage should be on perf team, compiler team, etc. -- not individual PR authors/reviewers -- but we also lack significant capacity on that side of things.</p>",
        "id": 265315686,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756625
    },
    {
        "content": "<p>that may mean we just need to be \"ok\" with a slower compiler and that's fine, but I <em>do</em> think there's a tradeoff here.</p>",
        "id": 265315797,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756645
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265313984\">said</a>:</p>\n<blockquote>\n<p>then that may be easier for the average person to interpret</p>\n</blockquote>\n<p>I have a bit of background doing UX work and typically the first thing I would start with is figuring out who your users (or your target users) are. Ideally this would actually involve talking to people who use perf.rlo now (which are some of us here). As pure conjecture, I would hypothesis we have two main groups of users: users who are trying to analyze compiler performance for a given change, probably working to improve it and casual users who got a notification their PR regressed performance somehow. (perf triage team may be a distinct 3rd group)</p>\n<p>Once you know who your users are, you can begin to understand some facets of those users. For example, users in the first category likely want to see everything (or, at least, everything meaningful) and want to easily slice and analyze the data different ways. Users in the second category probably do not want to see a lot of data. A simplified categorization of the impact of their change on \"real world\" vs \"artificial\" code might be enough. However, they will probably want some kind of \"call to action\" as to what to do next. </p>\n<p>Once we understand the behaviors of the users, then we can design a UI that is appropriate for each kind of user and a way to serve the correct UI to the proper set of users. </p>\n<p>This is a lot of work but I have seen some really amazing results with this kind of methodology.</p>",
        "id": 265315908,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639756667
    },
    {
        "content": "<p>So, we are almost out of time</p>",
        "id": 265316307,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756734
    },
    {
        "content": "<p>first off: this has been a great discussion. Lots of food for thought here</p>",
        "id": 265316412,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756751
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265315334\">said</a>:</p>\n<blockquote>\n<p>ultimately it's the reviewers call whether they deem more investigation is necessary before they r+</p>\n</blockquote>\n<p>Ok, but this is usually not how it happens in practice. What usually happens is the author sees the regression, says \"do I need to investigate this before landing\", the reviewer says \"sure if you have time\" expecting it to take maybe an hour, and then the author spends ages on it. If the author gives up <em>and never reports back</em> then we don't even have a way to say \"this is an unreasonable amount of time\" - that happened to me on <a href=\"https://github.com/rust-lang/rust/issues/90323#issuecomment-962142791\">https://github.com/rust-lang/rust/issues/90323#issuecomment-962142791</a>, the author spent something like <em>3 weeks</em> on the problem before I checked in on them and saw they had the wrong approach</p>",
        "id": 265316513,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1639756768
    },
    {
        "content": "<p>I think there’s more conversation/analyses that are remaining for us before we can make major (or even minor?) changes to the <em>default</em> UI for perf.rlo (or to the data we gather, the PR feedback from the bot, etc)</p>",
        "id": 265316722,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756803
    },
    {
        "content": "<p>but the most immediate call to action would be to add some more abilities to slice-and-dice the data here</p>",
        "id": 265317016,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756848
    },
    {
        "content": "<p>at least, I agree with <span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span> that I think that’s the thing that we, the compiler team, want the most right now</p>",
        "id": 265317165,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756877
    },
    {
        "content": "<p>Regarding the nightly \"real-world\" extensive perf testing, what are people's thoughts? It seems like something orthogonal that can happen?</p>",
        "id": 265317231,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639756890
    },
    {
        "content": "<p>(we should also be thinking about the broader set of contributors, and how we can have perf.rlo better serve them too. But that’s longer term, I think…)</p>",
        "id": 265317289,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756901
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"232957\">@Jack Huey</span> that’s a great point. these <em>are</em> potentially orthogonal items</p>",
        "id": 265317389,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756922
    },
    {
        "content": "<p>I don't dispute that our methods for investigating regressions are major impediments, and we don't have great docs around them. I think doing more \"user interviews\" to improve that side of things, and work towards audiences Wesley is talking about, makes sense to me. <em>in general</em> perf triage can be an enormous time sink, since it's fairly unbounded. how to prevent people thinking they <em>must</em> find an answer when we leave comments and working on improving tooling to help them get quick answers both seem like great things to work on to me.</p>",
        "id": 265317456,
        "sender_full_name": "simulacrum",
        "timestamp": 1639756930
    },
    {
        "content": "<p>its possible there may be potential synergy, in terms of both systems being able to share the same underlying infrastructure, maybe.</p>",
        "id": 265317523,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756946
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265317231\">said</a>:</p>\n<blockquote>\n<p>Regarding the nightly \"real-world\" extensive perf testing, what are people's thoughts? It seems like something orthogonal that can happen?</p>\n</blockquote>\n<p>(although I'd like to push on it if there's interesting, I don't think I could do this alone, for lack of time and \"infra\")</p>",
        "id": 265317530,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639756947
    },
    {
        "content": "<p>Is there anyone else here who thinks they might have time/resources to work with <span class=\"user-mention\" data-user-id=\"232957\">@Jack Huey</span> on the nightly-extensive-perf thing?</p>",
        "id": 265317769,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639756993
    },
    {
        "content": "<p>(as I said above, I personally think I’ll be focusing on trying to work with <span class=\"user-mention\" data-user-id=\"125294\">@Aaron Hill</span> on adding ways to show multiple metrics at once)</p>",
        "id": 265317957,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639757026
    },
    {
        "content": "<p>(partly, it is somewhat clear to me that we <strong>could</strong> expect first impressions in 15 minutes, I believe: it generally takes around that long for me to make a call as to whether a regression is likely to be readily fixable, and while I am likely on the upper-end of the spectrum of knowledge, I'm fairly confident in saying we could lower that knowledge for basic cases into code with extensive work.)</p>",
        "id": 265317997,
        "sender_full_name": "simulacrum",
        "timestamp": 1639757034
    },
    {
        "content": "<p>I don't think we should add more metrics (particularly differently-timed ones like nightly data) until we have better ways of dealing with existing ones, personally</p>",
        "id": 265318213,
        "sender_full_name": "simulacrum",
        "timestamp": 1639757073
    },
    {
        "content": "<p>(I'm not sure how much value we'd get either.)</p>",
        "id": 265318426,
        "sender_full_name": "simulacrum",
        "timestamp": 1639757133
    },
    {
        "content": "<p>I know we're over time already, but quick point: There's a shiny future where we can \"automatically\" analyze e.g. cachegrind data to figure out whether a regression is e.g. just inlining changes.</p>",
        "id": 265318437,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639757135
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265317231\">said</a>:</p>\n<blockquote>\n<p>Regarding the nightly \"real-world\" extensive perf testing, what are people's thoughts? It seems like something orthogonal that can happen?</p>\n</blockquote>\n<p>I think this is important. A significant number of the compiler regressions we've had over the last year or two are perf ones and crater basically isn't able to catch them. At the same time, I don't think I have bandwidth to work on this right now <span aria-label=\"frown\" class=\"emoji emoji-1f641\" role=\"img\" title=\"frown\">:frown:</span></p>",
        "id": 265318506,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639757155
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"232957\">@Jack Huey</span> maybe we back-burner this idea until after I have a chance to make progress with <span class=\"user-mention\" data-user-id=\"125294\">@Aaron Hill</span></p>",
        "id": 265318656,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639757186
    },
    {
        "content": "<p>?</p>",
        "id": 265318661,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639757187
    },
    {
        "content": "<p>i.e. I’d be happy to help with it, but I don’t want to be doing both things at once (in terms of time budget etc)</p>",
        "id": 265318679,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639757196
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265318426\">said</a>:</p>\n<blockquote>\n<p>(I'm not sure how much value we'd get either.)</p>\n</blockquote>\n<p>Well, if it's extensive <em>enough</em>, then regressions like the projection caching issues would pop out</p>",
        "id": 265318740,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639757222
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265318656\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> maybe we back-burner this idea until after I have a chance to make progress with <span class=\"user-mention silent\" data-user-id=\"125294\">Aaron Hill</span></p>\n</blockquote>\n<p>That's fine</p>",
        "id": 265318794,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639757252
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"125250\">Wesley Wiser</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265318506\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265317231\">said</a>:</p>\n<blockquote>\n<p>Regarding the nightly \"real-world\" extensive perf testing, what are people's thoughts? It seems like something orthogonal that can happen?</p>\n</blockquote>\n<p>I think this is important. A significant number of the compiler regressions we've had over the last year or two are perf ones and crater basically isn't able to catch them. At the same time, I don't think I have bandwidth to work on this right now <span aria-label=\"frown\" class=\"emoji emoji-1f641\" role=\"img\" title=\"frown\">:frown:</span></p>\n</blockquote>\n<p>When you say crater hasn't been able to catch them...have we looked? Does that just pop out as a build failure?</p>",
        "id": 265318986,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639757331
    },
    {
        "content": "<p>I haven't personally looked into it but as <span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> said, I suspect the variance between crater runs is high enough due to things like over subscription and leaky performance isolation from other guests on the same node that most of the perf regressions would have been lost in the noise.</p>",
        "id": 265319296,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639757455
    },
    {
        "content": "<p>crater timeouts are too common, in general, for us to get much value I suspect -- maybe some kind of more intelligent retry scheme would help there, but there's not been a lot of time available for that kind of work.</p>",
        "id": 265319304,
        "sender_full_name": "simulacrum",
        "timestamp": 1639757459
    },
    {
        "content": "<p>I think for something like compiler performance, we want to target a more narrow slice of the ecosystem than \"every Rust repo we could find\". Like, perhaps just the top 500/1000 crates on <a href=\"http://crates.io\">crates.io</a> or something. Or those crates plus a set of crates which are consumers of those popular crates.</p>",
        "id": 265319711,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1639757536
    },
    {
        "content": "<p>Anyway, I don’t want to take more of anyone’s time. Thanks again for everyone in <span class=\"user-group-mention\" data-user-group-id=\"897\">@T-compiler/meeting</span> for attending! Great meeting y’all.</p>",
        "id": 265319757,
        "sender_full_name": "pnkfelix",
        "timestamp": 1639757552
    },
    {
        "content": "<p>This discussion was super helpful!</p>",
        "id": 265320457,
        "sender_full_name": "Jack Huey",
        "timestamp": 1639757711
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232957\">Jack Huey</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265317231\">said</a>:</p>\n<blockquote>\n<p>Regarding the nightly \"real-world\" extensive perf testing, what are people's thoughts? It seems like something orthogonal that can happen?</p>\n</blockquote>\n<p>I think it would be useful to do a more controlled crater run to compare current to older rustc versions and obtain a set of crates that have regressed in performance that aren't reflected by any of the existing benchmarks. Maybe no such examples exist because users report serious regressions, but who knows.</p>",
        "id": 265340350,
        "sender_full_name": "The 8472",
        "timestamp": 1639766102
    },
    {
        "content": "<p>/me was scrolling through the thread... most of that has already been discussed.</p>",
        "id": 265340715,
        "sender_full_name": "The 8472",
        "timestamp": 1639766281
    },
    {
        "content": "<p>I'm awake now and reading through this.</p>",
        "id": 265360509,
        "sender_full_name": "nnethercote",
        "timestamp": 1639775228
    },
    {
        "content": "<p>My motivation is that there are typically many results on a page and I am mentally filtering them into primary/secondary, so it would be nice if I didn't have to do that, and if other people with less knowledge of the benchmark suite could do likewise.</p>",
        "id": 265360654,
        "sender_full_name": "nnethercote",
        "timestamp": 1639775289
    },
    {
        "content": "<p>This is relevant not only for regressions. E.g. I improved match-stress-enum recently, and saw opportunity to improve it more, then I remembered that it's an artificial benchmark and not worth that much effort.</p>",
        "id": 265360721,
        "sender_full_name": "nnethercote",
        "timestamp": 1639775335
    },
    {
        "content": "<p>In the past all the results for each benchmark were aggregated into a single line (in the text output) showing min/max/avg changes, and then you could click to expand and see all of them. At some point that was removed and now there is a lot more entries. I preferred the older style, TBH</p>",
        "id": 265360883,
        "sender_full_name": "nnethercote",
        "timestamp": 1639775407
    },
    {
        "content": "<p>E.g. compare the screenshot in <a href=\"https://blog.mozilla.org/nnethercote/2019/07/25/the-rust-compiler-is-still-getting-faster/\">https://blog.mozilla.org/nnethercote/2019/07/25/the-rust-compiler-is-still-getting-faster/</a> (moderately large) against the one in <a href=\"https://nnethercote.github.io/2021/11/12/the-rust-compiler-has-gotten-faster-again.html\">https://nnethercote.github.io/2021/11/12/the-rust-compiler-has-gotten-faster-again.html</a> (enormous)</p>",
        "id": 265361107,
        "sender_full_name": "nnethercote",
        "timestamp": 1639775535
    },
    {
        "content": "<p>On the latter one, I have to go down 27 entries to get to an entry that I consider important (serde check)</p>",
        "id": 265361386,
        "sender_full_name": "nnethercote",
        "timestamp": 1639775679
    },
    {
        "content": "<p>So the use case of interest is explaining results to an interested non-expert. And the main problems are (a) OMG so many results, and (b) many of them fall into the \"you can kind of ignore that one, it's not so important\" category</p>",
        "id": 265361647,
        "sender_full_name": "nnethercote",
        "timestamp": 1639775800
    },
    {
        "content": "<p>I consider helloworld check relevant because it gives a latency baseline for editors.</p>",
        "id": 265361650,
        "sender_full_name": "The 8472",
        "timestamp": 1639775802
    },
    {
        "content": "<p>As for some of the earlier points: I don't think a bigger suite is important. I think it's important to have a suite that can be run locally in a reasonable amount of time. I do that <em>all the time</em> and it's important. If you have to wait for a CI perf run every time to get feedback that slows things down massively</p>",
        "id": 265361836,
        "sender_full_name": "nnethercote",
        "timestamp": 1639775896
    },
    {
        "content": "<p>I really like the significance filtering, BTW. It's a good way to reduce the number of results to consider.</p>",
        "id": 265361948,
        "sender_full_name": "nnethercote",
        "timestamp": 1639775959
    },
    {
        "content": "<p>Anyway, to summarize and repeat myself, I think the guiding light here should be  \"explaining results to an interested non-expert\". That covers authors of PRs that unintentionally regressed, and also readers of blog posts explaining how compiler performance has changed over the past 6 months, for example.</p>",
        "id": 265362213,
        "sender_full_name": "nnethercote",
        "timestamp": 1639776083
    },
    {
        "content": "<p>If nothing else changed, going back to the \"one line per benchmark\" would please me.</p>",
        "id": 265362242,
        "sender_full_name": "nnethercote",
        "timestamp": 1639776106
    },
    {
        "content": "<p>One final thing on the primary/secondary split: <a href=\"https://perf.rust-lang.org/dashboard.html\">https://perf.rust-lang.org/dashboard.html</a> already does this. It measures a subset of the benchmarks, I think it's just a handful of the biggest stable ones. But that's not labelled in any way</p>",
        "id": 265362404,
        "sender_full_name": "nnethercote",
        "timestamp": 1639776199
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"330154\">The 8472</span> <a href=\"#narrow/stream/238009-t-compiler.2Fmeetings/topic/.5Bsteering.20meeting.5D.202021-12-17/near/265361650\">said</a>:</p>\n<blockquote>\n<p>I consider helloworld check relevant because it gives a latency baseline for editors.</p>\n</blockquote>\n<p>Also we are compiling dozens/hundreds/thousands of helloworlds when running rustc's test suite or doc-testing any crate.</p>",
        "id": 265383748,
        "sender_full_name": "Vadim Petrochenkov",
        "timestamp": 1639791134
    }
]