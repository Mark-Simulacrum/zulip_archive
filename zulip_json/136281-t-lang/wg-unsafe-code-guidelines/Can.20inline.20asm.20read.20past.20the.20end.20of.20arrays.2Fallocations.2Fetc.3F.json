[
    {
        "content": "<p>There are a lot of times when implementing certain string algorithms where <em>cautiously</em>¹ reading past the end of an array/allocated piece of memory is beneficial.</p>\n<p>One example is something like <code>strlen</code> — a pure-rust implementation currently must read every byte individually, which is terrible for performance compared to what performant implementations do. Pretty obviously pure Rust code isn't allowed to (unless I've drastically misunderstood things).</p>\n<p>I've been tentatively hoping (and more-or-less assuming) that inline asm would allow this in some capacity²</p>\n<p>The current text for the new inline asm says:</p>\n<blockquote>\n<p>The set of memory locations that assembly code is allowed the read and write are the same as those allowed for an FFI function. ... Refer to the unsafe code guidelines for the exact rules.</p>\n</blockquote>\n<p>It's not exactly clear (to me) which specific bit of the guidelines is being referenced, or if it's more of a \"the UCG is expected to hammer this out eventually\" situation. (Sorry if I'm just blind and missed it, though).</p>\n<p>Concretely, it's hard to imagine this kind of thing <em>not</em> working (in the case that the code is careful to avoid the cases where this would trap, etc).</p>\n<p>Especially as you move the slider towards \"more work done in inline asm\"... Additionally, FFI functions are allowed to pass these pointers to libc, which will absolutely do this... (But maybe they can only do it because they're libc, exercising a similar privilege that Rust stdlib is allowed)....</p>\n<p>Anyway, I felt like it warranted some discussion about whether or not it's strictly allowed, since this is very desirable for certain algorithms.</p>\n<p>¹ Yes, yes, cautiously enough.</p>\n<p>² Even that capacity is \"you have to do most of the work inside the inline asm block\" it would be acceptable... Ideally this wouldn't be the only option, though...</p>",
        "id": 215530220,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604451416
    },
    {
        "content": "<p>(CC <span class=\"user-mention\" data-user-id=\"143274\">@Amanieu</span>, in case you're interested in this discussion / have an opinion here)</p>",
        "id": 215530643,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604451771
    },
    {
        "content": "<p>I've been arguing to have volatile loads do this (allow \"invalid\" reads) so you wouldn't even need inline asm.</p>",
        "id": 215531098,
        "sender_full_name": "Amanieu",
        "timestamp": 1604452230
    },
    {
        "content": "<p>In my view a volatile load/store is the same as an inline asm block with a load/store instruction.</p>",
        "id": 215531114,
        "sender_full_name": "Amanieu",
        "timestamp": 1604452258
    },
    {
        "content": "<p>I'd certainly be happy with that</p>",
        "id": 215531118,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604452261
    },
    {
        "content": "<p>I assume that would produce llvm-style <code>undef</code> though, and e.g. if i read a <code>usize</code> where only one byte is past the end of the allocation, the whole <code>usize</code> is <code>undef</code> (according at least to LLVM)</p>",
        "id": 215531151,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604452312
    },
    {
        "content": "<p>You can't const-propagate through volatile operations, so <code>undef</code> is never returned.</p>",
        "id": 215531211,
        "sender_full_name": "Amanieu",
        "timestamp": 1604452355
    },
    {
        "content": "<p>ah, okay, interesting.</p>",
        "id": 215531218,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604452369
    },
    {
        "content": "<p>Fundamentally, algorithms which read out of bounds rely on the fact that memory protection only works on a page granularity. So if you don't cross page boundaries then memory accesses won't fault. They still return garbage data though.</p>",
        "id": 215531254,
        "sender_full_name": "Amanieu",
        "timestamp": 1604452429
    },
    {
        "content": "<p>yeah that's what i meant by \"cautiously\" above.</p>",
        "id": 215531302,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604452446
    },
    {
        "content": "<p>(garbage only for the bytes that are OOB)</p>",
        "id": 215531305,
        "sender_full_name": "Amanieu",
        "timestamp": 1604452447
    },
    {
        "content": "<p>Though keep in mind that <span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> and I have disagreements on the semantics of volatile.</p>",
        "id": 215531335,
        "sender_full_name": "Amanieu",
        "timestamp": 1604452491
    },
    {
        "content": "<p>it goes against <a href=\"https://www.ralfj.de/blog/2019/07/14/uninit.html\">https://www.ralfj.de/blog/2019/07/14/uninit.html</a> IIUC, although perhaps that could be adjusted for volatile if that's really not an issue.</p>",
        "id": 215531390,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604452549
    },
    {
        "content": "<p>My expection is that all volatile operations are considered I/O - the fact that they access memory is almost incidental. I don't think it goes against the post you linked</p>",
        "id": 215531554,
        "sender_full_name": "Diggsey",
        "timestamp": 1604452703
    },
    {
        "content": "<p>in the same way that you can never get <code>undef</code> when reading from a file, it doesn't make sense to get undef from a volatile load</p>",
        "id": 215531717,
        "sender_full_name": "Diggsey",
        "timestamp": 1604452917
    },
    {
        "content": "<p>if it's actually MMIO its more complex i think since you can still have data races on MMIO (cross process, although obviously same-process too) which is still UB when that happens i think</p>",
        "id": 215532411,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604453641
    },
    {
        "content": "<p>hmmmm, or maybe that doesnt matter</p>",
        "id": 215532462,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604453675
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"143274\">Amanieu</span> <a href=\"#narrow/stream/136281-t-lang.2Fwg-unsafe-code-guidelines/topic/Can.20inline.20asm.20read.20past.20the.20end.20of.20arrays.2Fallocations.2Fetc.3F/near/215531114\">said</a>:</p>\n<blockquote>\n<p>In my view a volatile load/store is the same as an inline asm block with a load/store instruction.</p>\n</blockquote>\n<p>I think that is not compatible with LVLM's model of volatile and your model of asm. You said in the past it should be legal to replace one asm block by another (e.g. self-modifying code), i.e., the compiler may not make any assumptions based on the contents of the asm block. OTOH, LLVM will happily reorder volatile accesses wrt other non-volatile accesses, which I don't think your model for asm blocks permits.</p>",
        "id": 215552657,
        "sender_full_name": "RalfJ",
        "timestamp": 1604478772
    },
    {
        "content": "<p>related issue for OOB loads: <a href=\"https://github.com/rust-lang/unsafe-code-guidelines/issues/2\">https://github.com/rust-lang/unsafe-code-guidelines/issues/2</a></p>",
        "id": 215552680,
        "sender_full_name": "RalfJ",
        "timestamp": 1604478788
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"209168\">Thom Chiovoloni</span> <a href=\"#narrow/stream/136281-t-lang.2Fwg-unsafe-code-guidelines/topic/Can.20inline.20asm.20read.20past.20the.20end.20of.20arrays.2Fallocations.2Fetc.3F/near/215531390\">said</a>:</p>\n<blockquote>\n<p>it goes against <a href=\"https://www.ralfj.de/blog/2019/07/14/uninit.html\">https://www.ralfj.de/blog/2019/07/14/uninit.html</a> IIUC, although perhaps that could be adjusted for volatile if that's really not an issue.</p>\n</blockquote>\n<p>as in, letting volatile loads perform an implicit <code>freeze</code>? that could be done, but LLVM seems unwilling to do so: <a href=\"https://bugs.llvm.org/show_bug.cgi?id=42435\">https://bugs.llvm.org/show_bug.cgi?id=42435</a></p>",
        "id": 215552912,
        "sender_full_name": "RalfJ",
        "timestamp": 1604478943
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"143274\">Amanieu</span> <a href=\"#narrow/stream/136281-t-lang.2Fwg-unsafe-code-guidelines/topic/Can.20inline.20asm.20read.20past.20the.20end.20of.20arrays.2Fallocations.2Fetc.3F/near/215531211\">said</a>:</p>\n<blockquote>\n<p>You can't const-propagate through volatile operations, so <code>undef</code> is never returned.</p>\n</blockquote>\n<p>that's the same LLVM bug -- it seems reasonable to assume that <code>undef</code> is never returned, and I'd be onboard with guaranteeing that for Rust (if the lang team <a href=\"https://bugs.llvm.org/show_bug.cgi?id=42435\">is happy to ignore all the concerns this raises</a>), but we can only do that if our backends also guarantee the same (or if we are okay adjusting codegen to add explicit <code>freeze</code> after each volatile load, I guess)</p>",
        "id": 215553172,
        "sender_full_name": "RalfJ",
        "timestamp": 1604479104
    },
    {
        "content": "<p>Probably normal past-the-end reads could be <code>undef</code>, but the <code>undef</code> part can be bypassed by reading as a newtype over <code>[u8; N]</code>, maybe with <code>repr(align(N))</code>.  (I have no idea about the correctness question of whether the whole read is UB, though.)</p>",
        "id": 215632455,
        "sender_full_name": "scottmcm",
        "timestamp": 1604519425
    },
    {
        "content": "<blockquote>\n<p>Probably normal past-the-end reads could be undef</p>\n</blockquote>\n<p>only if you convince LLVM to treat it that way^^</p>",
        "id": 215952280,
        "sender_full_name": "RalfJ",
        "timestamp": 1604751101
    },
    {
        "content": "<p>AFAIK currently LLVM will do things like \"oh you did an 8-byte read, this means the pointer cannot possibly alias that 4-byte object here\"</p>",
        "id": 215952331,
        "sender_full_name": "RalfJ",
        "timestamp": 1604751136
    },
    {
        "content": "<blockquote>\n<p>as in, letting volatile loads perform an implicit freeze? that could be done, but LLVM seems unwilling to do so: <a href=\"https://bugs.llvm.org/show_bug.cgi?id=42435\">https://bugs.llvm.org/show_bug.cgi?id=42435</a></p>\n</blockquote>\n<p>So, I guess the precise comment is: <a href=\"https://bugs.llvm.org/show_bug.cgi?id=42435#c9\">https://bugs.llvm.org/show_bug.cgi?id=42435#c9</a>, which says:</p>\n<blockquote>\n<p>In terms of adding semantics to volatile, I think that's not a good approach. There's way too much history of under-defined rules and weird semantics associated with volatile operations. And I don't want to unintentionally constrain better solutions for handling \"undefined\" memory like padding, given we're still not completely sure what they look like.</p>\n</blockquote>\n<p>Honestly that's hard to argue with. Volatile has a ton of baggage from C. If it was the way to perform frozen reads, then, among other things, the compiler would be required to follow the other rules of volatile — mostly that every load and store that appears both happens and is in program order. That's actually pretty undesirable, honestly.</p>\n<p>If there were a way to do frozen reads¹, depending on the kind of read I would <strong>want</strong> the compiler to do many² of the same optimizations that it would do on, say, <code>ptr::read</code>, assuming that it would be a valid transformation. That is, stuff like remove it if the result is unused, hoist it out of a loop, merge it with other loads, move the loaded value to a register (e.g. for small structs with padding bytes), remove it using store-load forwarding...</p>\n<p>With volatile (let alone inline asm) I'd get none of those. This is still better than nothing, but I think the LLVM commentor is right to want to know what a better solution here looks like.</p>\n<p>¹ probably with some reasonable limitations that are UB if violated (This being unsafe is fine, I think). For example:</p>\n<ul>\n<li>for OOB reads they can't read past the end of a page line³...</li>\n<li>for uninit reads they're still UB if the page has been freed or has never been written to (or whatever is needed to precisely make the issues with pages from stuff like MADV_FREE irrelevant, besides actually writing to the memory which would really not be good IMO).</li>\n</ul>\n<p>² Presumably with a frozen read, some optimizations aren't valid, or are only conditionally valid, etc. Even of the ones I mention there I bet not all of those can work.</p>\n<p>³ Honestly I wouldn't be surprised if, say, OOB reads is the odd one out and is too problematic to allow this way...  the more I think the dodgier it gets... I'd hope inline asm still works for them, though.</p>",
        "id": 215961689,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604765556
    },
    {
        "content": "<p>With <code>freeze(read(x))</code>, I think uninit memory is already handled properly -- this does represent the fact that doing multiple reads can return different results even if nobody does a write in the mean time</p>",
        "id": 215961842,
        "sender_full_name": "RalfJ",
        "timestamp": 1604765798
    },
    {
        "content": "<p>this will indeed block some optimizations, but way fewer than volatile</p>",
        "id": 215961846,
        "sender_full_name": "RalfJ",
        "timestamp": 1604765819
    },
    {
        "content": "<p>but then we'd need a new surface-level API in Rust that expresses these kinds of reads -- maybe that is a good thing</p>",
        "id": 215961859,
        "sender_full_name": "RalfJ",
        "timestamp": 1604765845
    },
    {
        "content": "<p>my concern with <code>freeze(read(x))</code> is: if the value read has only some bytes uninitialized, i thought the result was fully <code>uninit</code> in llvm's eyes, and <code>freeze</code> wouldn't \"regain\" those bytes of the value which were actually initialized</p>",
        "id": 215961917,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604765930
    },
    {
        "content": "<p>(that LLVM bug was written before <code>freeze</code> existed... assuming that <code>freeze</code> can be applied to all types that one can no voaltile loads at, then indeed nowadays there is no need any more for volatile loads to make such promises, as the same can be expressed more orthogonally with a separate freeze instruction)</p>",
        "id": 215961922,
        "sender_full_name": "RalfJ",
        "timestamp": 1604765937
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"209168\">Thom Chiovoloni</span> <a href=\"#narrow/stream/136281-t-lang.2Fwg-unsafe-code-guidelines/topic/Can.20inline.20asm.20read.20past.20the.20end.20of.20arrays.2Fallocations.2Fetc.3F/near/215961917\">said</a>:</p>\n<blockquote>\n<p>my concern with <code>freeze(read(x))</code> is: if the value read has only some bytes uninitialized, i thought the result was fully <code>uninit</code> in llvm's eyes, and <code>freeze</code> wouldn't \"regain\" those bytes of the value which were actually initialized</p>\n</blockquote>\n<p>that is a problem no matter which approach we use -- <code>iX</code> values in LLVM currently are all-or-nothing in terms of initialization</p>",
        "id": 215961927,
        "sender_full_name": "RalfJ",
        "timestamp": 1604765965
    },
    {
        "content": "<p>one can use <code>[i8 x N]</code> or whatever the syntax is to obtain per-byte initialization</p>",
        "id": 215961985,
        "sender_full_name": "RalfJ",
        "timestamp": 1604766008
    },
    {
        "content": "<p>a benefit of it not being a new read api is that <code>freeze(read_atomic(x, ord))</code>, <code>freeze(read_unaligned(x))</code>, <code>freeze(read_volatile(x))</code> all work.</p>",
        "id": 215961989,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604766016
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> can [i8 x N] be used to do, say, an atomic load?</p>",
        "id": 215962059,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604766131
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"209168\">Thom Chiovoloni</span> <a href=\"#narrow/stream/136281-t-lang.2Fwg-unsafe-code-guidelines/topic/Can.20inline.20asm.20read.20past.20the.20end.20of.20arrays.2Fallocations.2Fetc.3F/near/215962059\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"120791\">RalfJ</span> can [i8 x N] be used to do, say, an atomic load?</p>\n</blockquote>\n<p>no idea</p>",
        "id": 215962064,
        "sender_full_name": "RalfJ",
        "timestamp": 1604766167
    },
    {
        "content": "<p>i mean, even if it can rust doesn't expose anyway of using anything besides u64 to load a, well, u64 atomically</p>",
        "id": 215962081,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604766229
    },
    {
        "content": "<p>maybe that's just a libcore design problem though.</p>",
        "id": 215962224,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1604766398
    }
]