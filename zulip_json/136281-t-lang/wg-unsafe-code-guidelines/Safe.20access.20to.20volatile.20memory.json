[
    {
        "content": "<p>Hi everyone... long time no see... I've been writing some unsafe code, which I thiiiiink is safe, but as always who knows?</p>",
        "id": 174588949,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567205724
    },
    {
        "content": "<p>It's providing safe access to shared memory, which means that there could be attacker processes writing into the memory at arbitrary times, so taking <code>&amp;T</code> access is definitely UB for some <code>T</code>s (e,g. <code>&amp;usize</code> into shared memory is right out).</p>",
        "id": 174589028,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567205834
    },
    {
        "content": "<p>I put all the unsafe code in one file, which will hopefully make it easier to nerd-snipe. <a href=\"https://github.com/asajeffrey/shared-data/blob/master/src/unsafe_code.rs\" target=\"_blank\" title=\"https://github.com/asajeffrey/shared-data/blob/master/src/unsafe_code.rs\">https://github.com/asajeffrey/shared-data/blob/master/src/unsafe_code.rs</a></p>",
        "id": 174589038,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567205871
    },
    {
        "content": "<p>One issue I'm aware of is that if the attacker can write <code>undef</code> values into shared memory, then we have UB, if <code>read_volatile</code> can read back the <code>undef</code>.</p>",
        "id": 174589125,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567205985
    },
    {
        "content": "<p>But I think <code>read_volatile</code> can only produce defined values, not <code>undef</code>.</p>",
        "id": 174589470,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567206428
    },
    {
        "content": "<p>this seems related to the question of whether int's can have undefined bits</p>",
        "id": 174592106,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1567209890
    },
    {
        "content": "<p><code>int</code> semantics is relevant here. If only some of the bits are undefined, it's clearly a bit-string or a packed <code>struct</code> and not an <code>int</code>. However if all of the bits are undefined it could be anything of that size.</p>",
        "id": 174593590,
        "sender_full_name": "Tom Phinney",
        "timestamp": 1567212116
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"128323\">@Alan Jeffrey</span> ah, you're opening that can of worms again ;)</p>",
        "id": 174605744,
        "sender_full_name": "RalfJ",
        "timestamp": 1567234936
    },
    {
        "content": "<p>there's a loooong discussion on the subject at <a href=\"https://github.com/rust-lang/unsafe-code-guidelines/issues/152\" target=\"_blank\" title=\"https://github.com/rust-lang/unsafe-code-guidelines/issues/152\">https://github.com/rust-lang/unsafe-code-guidelines/issues/152</a></p>",
        "id": 174605747,
        "sender_full_name": "RalfJ",
        "timestamp": 1567234969
    },
    {
        "content": "<blockquote>\n<p>But I think <code>read_volatile</code> can only produce defined values, not <code>undef</code>.</p>\n</blockquote>\n<p>in particular, for this part, I would like to agree -- but LLVM devs seem to think this is not worth stating, and I am reluctant to guarantee it for Rust if LLVM won't guarnatee it to us. See <a href=\"https://github.com/rust-lang/unsafe-code-guidelines/issues/152\" target=\"_blank\" title=\"https://github.com/rust-lang/unsafe-code-guidelines/issues/152\">https://github.com/rust-lang/unsafe-code-guidelines/issues/152</a>.</p>",
        "id": 174605758,
        "sender_full_name": "RalfJ",
        "timestamp": 1567235032
    },
    {
        "content": "<p>If LLVM had a <code>freeze</code> instruction we could just make the <code>read_volatile</code> intrinsic emit a load followed by a freeze and wouldnt not have to rely on LLVM specifying their stuff better... but alas, we've been waiting for <code>freeze</code> since forever</p>",
        "id": 174605809,
        "sender_full_name": "RalfJ",
        "timestamp": 1567235073
    },
    {
        "content": "<p>oh and then of course there are all the concerns about having anything freeze-like in Rust, those would also have to be resolved before we can guarantee this. see e.g. <a href=\"https://github.com/rust-lang/rfcs/pull/837\" target=\"_blank\" title=\"https://github.com/rust-lang/rfcs/pull/837\">https://github.com/rust-lang/rfcs/pull/837</a></p>",
        "id": 174605864,
        "sender_full_name": "RalfJ",
        "timestamp": 1567235166
    },
    {
        "content": "<p>also I think treating <code>AtomicBool</code> as share-safe is plain wrong. After all, if this is shared with untrusted code, that code might write things to memory that are not a valid <code>bool</code></p>",
        "id": 174605994,
        "sender_full_name": "RalfJ",
        "timestamp": 1567235296
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> you're not kidding, that is a long read!</p>",
        "id": 174618273,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567258680
    },
    {
        "content": "<p>In this case, there's two ways to deal with <code>undef</code>, either stop the Rust code from reading it, or make the attacker model one which can't write it.</p>",
        "id": 174618329,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567258756
    },
    {
        "content": "<p>It sounds like restricting the attacker model so they can't write <code>undef</code> is the easiest one to go for?</p>",
        "id": 174618380,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567258848
    },
    {
        "content": "<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"120791\">RalfJ</span> you're not kidding, that is a long read!</p>\n</blockquote>\n<p>yeah... we should get better at having summaries of such discussions. not enough time for all the things that need doing :/</p>",
        "id": 174618396,
        "sender_full_name": "RalfJ",
        "timestamp": 1567258916
    },
    {
        "content": "<blockquote>\n<p>It sounds like restricting the attacker model so they can't write <code>undef</code> is the easiest one to go for?</p>\n</blockquote>\n<p>restricting the attacker is always the easier option, isn't it? ;)</p>",
        "id": 174618439,
        "sender_full_name": "RalfJ",
        "timestamp": 1567258934
    },
    {
        "content": "<blockquote>\n<blockquote>\n<p>But I think <code>read_volatile</code> can only produce defined values, not <code>undef</code>.</p>\n</blockquote>\n<p>in particular, for this part, I would like to agree -- but LLVM devs seem to think this is not worth stating, and I am reluctant to guarantee it for Rust if LLVM won't guarnatee it to us. See <a href=\"https://github.com/rust-lang/unsafe-code-guidelines/issues/152\" target=\"_blank\" title=\"https://github.com/rust-lang/unsafe-code-guidelines/issues/152\">https://github.com/rust-lang/unsafe-code-guidelines/issues/152</a>.</p>\n</blockquote>\n<p>sorry, that link should have been <a href=\"https://bugs.llvm.org/show_bug.cgi?id=42435\" target=\"_blank\" title=\"https://bugs.llvm.org/show_bug.cgi?id=42435\">https://bugs.llvm.org/show_bug.cgi?id=42435</a></p>",
        "id": 174618450,
        "sender_full_name": "RalfJ",
        "timestamp": 1567258967
    },
    {
        "content": "<p>Also, on <code>AtomicBool</code>, I had a look at the source, and it does sanitize reads to make sure they're bools (e.g. read x:u8 and return x!=0) but it may be that future versions of <code>AtomicBool</code> will change that?</p>",
        "id": 174618467,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567259041
    },
    {
        "content": "<p>Goes and reads llvm docs... AFAICT if the attacker is any llvm program, then the attacker just spawns two threads, does concurrent writes into shared memory, and boom the shared memory is filled with undef. I think I'm going to ignore that :)</p>",
        "id": 174618642,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567259343
    },
    {
        "content": "<p>Interesting that llvm is waker than all the hardware models, which (afaik) don't have <code>undef</code>.</p>",
        "id": 174618651,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567259401
    },
    {
        "content": "<p>*weaker</p>",
        "id": 174619068,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567260080
    },
    {
        "content": "<blockquote>\n<p>Also, on <code>AtomicBool</code>, I had a look at the source, and it does sanitize reads to make sure they're bools (e.g. read x:u8 and return x!=0) but it may be that future versions of <code>AtomicBool</code> will change that?</p>\n</blockquote>\n<p>If it's not in the docs it is certainly not guaranteed</p>",
        "id": 174619346,
        "sender_full_name": "RalfJ",
        "timestamp": 1567260512
    },
    {
        "content": "<p>also, even if we allow <code>undef</code> in ints, that sanitization would be UB if the value read is <code>undef</code></p>",
        "id": 174619348,
        "sender_full_name": "RalfJ",
        "timestamp": 1567260527
    },
    {
        "content": "<blockquote>\n<p>Interesting that llvm is waker than all the hardware models, which (afaik) don't have <code>undef</code>.</p>\n</blockquote>\n<p>true. but having such a form of \"delayed UB value\" is basically necessary in an optimizing IR. <a href=\"https://www.cs.utah.edu/~regehr/papers/undef-pldi17.pdf\" target=\"_blank\" title=\"https://www.cs.utah.edu/~regehr/papers/undef-pldi17.pdf\">This paper</a> has a fairly good explanation.</p>",
        "id": 174619413,
        "sender_full_name": "RalfJ",
        "timestamp": 1567260608
    },
    {
        "content": "<p>note: my <code>undef</code> above is Miri's <code>undef</code>; LLVM calls that <code>poison</code>.<br>\nLLVM's <code>undef</code> is a mess that not even LLVM optmizations use correctly, so I am going to pretend that the proposal in said paper is implemented, so LLVM no longer has <code>undef</code>, it just has <code>poison</code>.</p>",
        "id": 174619430,
        "sender_full_name": "RalfJ",
        "timestamp": 1567260666
    },
    {
        "content": "<p>Yes, for regular reads and writes, I can see why you need something like <code>undef</code>, but it's not obvious that's true for atomics.</p>",
        "id": 174619571,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567260905
    },
    {
        "content": "<p>BTW, does anyone know if I'm reinventing the wheel here? Is there a crate that already does safe access to shared memory?</p>",
        "id": 174619704,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1567261171
    },
    {
        "content": "<blockquote>\n<p>Yes, for regular reads and writes, I can see why you need something like <code>undef</code>, but it's not obvious that's true for atomics.</p>\n</blockquote>\n<p>atomics can be replaced by non-atomic accesses if the compiler can determine that they are only used in a single thread</p>",
        "id": 175747985,
        "sender_full_name": "RalfJ",
        "timestamp": 1568552521
    },
    {
        "content": "<p>so, we have to be able to deal with them yieldung <code>undef</code>/<code>poison</code> as well</p>",
        "id": 175748028,
        "sender_full_name": "RalfJ",
        "timestamp": 1568552530
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"128323\">@Alan Jeffrey</span> ^</p>",
        "id": 175748032,
        "sender_full_name": "RalfJ",
        "timestamp": 1568552533
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> the compiler is allowed to replace atomics by non-atomics?????? Argh, is there a way to stop that?</p>",
        "id": 175751556,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1568557915
    },
    {
        "content": "<p>In particular, for shared memory, unless the compiler is related to keep track of which memory may be shared with other processes?</p>",
        "id": 175751666,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1568558050
    },
    {
        "content": "<p>(I'm guessing that is under-spec'd, since that seems to be the state of most relaxed models wrt shared memory.)</p>",
        "id": 175751683,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1568558109
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> By what means can the compiler know that?</p>",
        "id": 175783320,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1568610121
    },
    {
        "content": "<p>One possibilities is if the compiler can find out where the object being accesses atomically was allocated and determine that its address does not escape to any place where it might become available to other threads. (Or for a simpler version,, the address just isn't captured at all, period.)</p>",
        "id": 175790683,
        "sender_full_name": "Hanna Kruppe",
        "timestamp": 1568620398
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"124289\">@rkruppe</span> <span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> do we have a minimal example that shows this at work ?</p>",
        "id": 175792000,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568621588
    },
    {
        "content": "<p>IIRC LLVM does not actually do that currently, it's just allowed under as-if rule</p>",
        "id": 175792250,
        "sender_full_name": "Hanna Kruppe",
        "timestamp": 1568621806
    },
    {
        "content": "<p>LLVM does completely eliminate atomic loads in some cases</p>",
        "id": 175792535,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568622112
    },
    {
        "content": "<p>But even for something as simple as: <a href=\"https://play.rust-lang.org/?version=nightly&amp;mode=release&amp;edition=2018&amp;gist=0f1e25e488323bd2d9be740397ce1095\" target=\"_blank\" title=\"https://play.rust-lang.org/?version=nightly&amp;mode=release&amp;edition=2018&amp;gist=0f1e25e488323bd2d9be740397ce1095\">https://play.rust-lang.org/?version=nightly&amp;mode=release&amp;edition=2018&amp;gist=0f1e25e488323bd2d9be740397ce1095</a> no interesting optimizations happen (e.g. I'd expect that to be optimized to just <code>exit(43)</code>.</p>",
        "id": 175793243,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568622802
    },
    {
        "content": "<blockquote>\n<p>(I'm guessing that is under-spec'd, since that seems to be the state of most relaxed models wrt shared memory.)</p>\n</blockquote>\n<p>well, it's not a specs role to say \"the compiler may do this or that transformation\". the spec describes possible program behaviors, and then the compiler can do whatever it wants as long as it doesnt make the program do anything the spec doesn't let it do.</p>",
        "id": 175798104,
        "sender_full_name": "RalfJ",
        "timestamp": 1568627489
    },
    {
        "content": "<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"120791\">RalfJ</span> By what means can the compiler know that?</p>\n</blockquote>\n<p>trivial case:</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"k\">fn</span> <span class=\"nf\">foo</span><span class=\"p\">()</span><span class=\"w\"> </span>-&gt; <span class=\"kt\">u32</span> <span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">  </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">AtomicU32</span>::<span class=\"n\">new</span><span class=\"p\">(</span><span class=\"mi\">14</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">  </span><span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Ordering</span>::<span class=\"n\">SeqCst</span><span class=\"p\">)</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</pre></div>\n\n\n<p>this can be optimized to just return 14. the way that happens is that the atomic-load can first become non-atomic (a non-trivial transformation since the compiler has to prove that this does not remove synchronizes-with edges!), and then it's plain load forwarding.</p>",
        "id": 175798189,
        "sender_full_name": "RalfJ",
        "timestamp": 1568627582
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> more generally speaking, if <code>foo</code> did a hole bunch of stuff but the compiler could tell that all accesses to <code>x</code> come from the same thread, it can just make them all non-atomic.</p>",
        "id": 175798210,
        "sender_full_name": "RalfJ",
        "timestamp": 1568627611
    },
    {
        "content": "<blockquote>\n<p>trivial case:</p>\n</blockquote>\n<p>I saw that case, but the moment I add a write, all optimizatons are disabled :/</p>",
        "id": 175798254,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568627641
    },
    {
        "content": "<blockquote>\n<p>(I'm guessing that is under-spec'd, since that seems to be the state of most relaxed models wrt shared memory.)</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"128323\">@Alan Jeffrey</span> what <em>is</em> under-spec'd is the \"open\" case -- like, having a part of a program that gets composed with some other unknown code and then saying what the heck is going on. that's a genuinely hard research problem even for languages much simpler than C or Rust.</p>",
        "id": 175798308,
        "sender_full_name": "RalfJ",
        "timestamp": 1568627656
    },
    {
        "content": "<blockquote>\n<blockquote>\n<p>trivial case:</p>\n</blockquote>\n<p>I saw that case, but the moment I add a write, all optimizatons are disabled :/</p>\n</blockquote>\n<p>I am not talking about what LLVM <em>does</em> (I have no idea of the status there), but what it is <em>allowed to do</em></p>",
        "id": 175798324,
        "sender_full_name": "RalfJ",
        "timestamp": 1568627680
    },
    {
        "content": "<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"120791\">RalfJ</span> the compiler is allowed to replace atomics by non-atomics?????? Argh, is there a way to stop that?</p>\n</blockquote>\n<p>de jure? no, if the compiler understands what your code is doing (or thinks it does...) then it can do whatever it wants as long as the code still does the same thing <em>when viewed from the abstract machine</em>. so, you'd need something like volatile to be able to \"plug in\" hardware-level reasoning. LLVM has \"atomic volatile\" operations, so maybe those would help here.<br>\nde facto, if the address of an atomic gets leaked to unknown code that is never optimized together (including via xLTO), the compiler has no way to prove that the atomic is <em>not</em> used from multiple threads, so it cannot remove the optimizations.<br>\nbut really, if you want to reason outside of the abstract machine, volatile should come in. that is <em>exactly</em> what it is made for.</p>",
        "id": 175798450,
        "sender_full_name": "RalfJ",
        "timestamp": 1568627825
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> writes here work too: <a href=\"https://rust.godbolt.org/z/m_iP1f\" target=\"_blank\" title=\"https://rust.godbolt.org/z/m_iP1f\">https://rust.godbolt.org/z/m_iP1f</a></p>\n<blockquote>\n<p>I am not talking about what LLVM does (I have no idea of the status there), but what it is allowed to do</p>\n</blockquote>\n<p>I think we all agree that it is allowed to do these optimizations. My question was if it was specifically already doing them in some cases, since from <span class=\"user-mention\" data-user-id=\"124289\">@rkruppe</span> comment I got the impression that it was not.</p>",
        "id": 175798534,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568627934
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> \\</p>",
        "id": 175823607,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1568648105
    },
    {
        "content": "<blockquote>\n<p>LLVM has \"atomic volatile\" operations,</p>\n</blockquote>",
        "id": 175823621,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1568648110
    },
    {
        "content": "<p>does Rust have a way of using them?</p>",
        "id": 175823660,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1568648132
    },
    {
        "content": "<p>no</p>",
        "id": 175824004,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568648338
    },
    {
        "content": "<p>but we've had proposals before to make <code>read_volatile</code> and <code>write_volatile</code> atomic (with the weakest \"unordered\" ordering)</p>",
        "id": 175830940,
        "sender_full_name": "RalfJ",
        "timestamp": 1568653048
    },
    {
        "content": "<p>the issue here is tearing -- volatile accesses can tear, atomic accesses can not</p>",
        "id": 175830953,
        "sender_full_name": "RalfJ",
        "timestamp": 1568653064
    },
    {
        "content": "<p>the thing is, when you use volatile for MMIO you dont want tearing</p>",
        "id": 175830964,
        "sender_full_name": "RalfJ",
        "timestamp": 1568653077
    },
    {
        "content": "<p>so we need to solve this one way or another either way. just noone is pushing for a solution ATM.</p>",
        "id": 175831017,
        "sender_full_name": "RalfJ",
        "timestamp": 1568653093
    },
    {
        "content": "<p>there was also the idea of providing intrinsics that are guaranteed not to tear</p>",
        "id": 175840170,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568659447
    },
    {
        "content": "<p>since whether volatile load / stores tear in LLVM is... unspecified at best</p>",
        "id": 175840185,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568659460
    },
    {
        "content": "<p>at least, i don't know of any way to query that</p>",
        "id": 175840194,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568659468
    },
    {
        "content": "<p>yes. seems like a natural choice then for the compiler to provide \"unordered\" atomic volatile intrinsics that cannot tear, and do the rest in libstd?</p>",
        "id": 175840293,
        "sender_full_name": "RalfJ",
        "timestamp": 1568659557
    },
    {
        "content": "<p>not sure what the status of \"atomic memcpy/memset\" is, but that could have a volatile variant as well (with tearing specified the same way as it is for the atomic variant)</p>",
        "id": 175840405,
        "sender_full_name": "RalfJ",
        "timestamp": 1568659611
    },
    {
        "content": "<p>there is <a href=\"https://github.com/rust-lang-nursery/compiler-builtins/pull/311\" target=\"_blank\" title=\"https://github.com/rust-lang-nursery/compiler-builtins/pull/311\">https://github.com/rust-lang-nursery/compiler-builtins/pull/311</a></p>",
        "id": 175840547,
        "sender_full_name": "RalfJ",
        "timestamp": 1568659687
    },
    {
        "content": "<p>so the various memcpy/memset variants all explicitly specify their tearing. we could have the same for volatile. the only question then is how to implement the existing interface that works for any type.</p>",
        "id": 175840587,
        "sender_full_name": "RalfJ",
        "timestamp": 1568659729
    },
    {
        "content": "<p>in terms of avoiding API duplication, it might make sense to have <code>Ordering::Volatile</code>, so that all the existing atomic APIs can just also be used here.</p>",
        "id": 175840629,
        "sender_full_name": "RalfJ",
        "timestamp": 1568659759
    },
    {
        "content": "<p>FWIW i wouldn't try to do that</p>",
        "id": 175840956,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568659953
    },
    {
        "content": "<p>At least, until we find an use case for volatile operations with tearing</p>",
        "id": 175840986,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568659979
    },
    {
        "content": "<p>I think it would be sufficient to provide the <code>Ordering::UnorderedVolatile</code> for those atomic operations that cannot tear, and have those be unordered+volatile</p>",
        "id": 175841020,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568659996
    },
    {
        "content": "<p>On a platform where those would tear, they should just fail to compile, just like we do for atomics</p>",
        "id": 175841140,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568660060
    },
    {
        "content": "<p>If someone finds out an use case for tearing, they can always at least emulate it on top of non-tearing accesses</p>",
        "id": 175841197,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568660100
    },
    {
        "content": "<p>And we could add support for that in the future if we wanted</p>",
        "id": 175841219,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568660122
    },
    {
        "content": "<blockquote>\n<p>At least, until we find an use case for volatile operations with tearing</p>\n</blockquote>\n<p>we have to support the current <code>read_volatile</code>/<code>write_volatile</code> methods, which in general require tearing</p>",
        "id": 175841412,
        "sender_full_name": "RalfJ",
        "timestamp": 1568660237
    },
    {
        "content": "<p>so we already do support volatile-with-tearing. when we get atomic-with-tearing, we might as well make it support volatile as well.</p>",
        "id": 175841461,
        "sender_full_name": "RalfJ",
        "timestamp": 1568660272
    },
    {
        "content": "<p>atomic-with-tearing does not make sense to me</p>",
        "id": 175848373,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568665125
    },
    {
        "content": "<p>atomic is the opposite of tearing</p>",
        "id": 175848375,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568665130
    },
    {
        "content": "<p>as in, either an operation is atomic, or it tears</p>",
        "id": 175848384,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568665141
    },
    {
        "content": "<p>but it can't be both at the same time</p>",
        "id": 175848395,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568665150
    },
    {
        "content": "<p>Do we need <code>SeqCstVolatile</code>, <code>AqVolatile</code>, <code>RelVolatile</code> and <code>AqRelVolatile</code> too? We'll need something stronger than <code>UnorderedVolatile</code> if we want happens-before to propogate between processes as well as threads.</p>",
        "id": 175853044,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1568668212
    },
    {
        "content": "<p>Or alternatively, can we make <code>SeqCst</code> use <code>VolatileSeqCst</code> under the hood? (and ditto <code>AqRel</code> and friends)</p>",
        "id": 175853090,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1568668267
    },
    {
        "content": "<blockquote>\n<p>Or alternatively, can we make SeqCst use VolatileSeqCst under the hood? (and ditto AqRel and friends)</p>\n</blockquote>\n<p>I don't think so. This would disable pretty much all optimizations on atomic operations.</p>",
        "id": 175890115,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568713023
    },
    {
        "content": "<blockquote>\n<p>Do we need SeqCstVolatile, AqVolatile, RelVolatile and AqRelVolatile too? We'll need something stronger than UnorderedVolatile if we want happens-before to propogate between processes as well as threads.</p>\n</blockquote>\n<p>Probably.</p>",
        "id": 175890135,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568713042
    },
    {
        "content": "<p>Volatile is orthogonal to the ordering</p>",
        "id": 175890142,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568713053
    },
    {
        "content": "<p>and just means that the load / store has unobservable side-effects and must happen just as the code was written (e.g. two subsequent relaxed loads can be merged, but two subsequent volatile relaxed loads cannot).</p>",
        "id": 175890225,
        "sender_full_name": "gnzlbg",
        "timestamp": 1568713107
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"132920\">@gnzlbg</span> Yeah, I was mainly thinking about <code>SeqCst</code>. I don't think there are many valid optimizations we'd lose, the ones I know of are the ones which need (e.g.) escape analysis to ensure no concurrent access.</p>",
        "id": 175906887,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1568727656
    },
    {
        "content": "<p>The reason for asking is that I'm not sure at this late date that we'll be able to add any more cases to <code>Ordering</code>, since that'd be a breaking change.</p>",
        "id": 175906960,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1568727713
    },
    {
        "content": "<p>Many of the optimizations discussed in <a href=\"https://jfbastien.github.io/no-sane-compiler/#/32\" target=\"_blank\" title=\"https://jfbastien.github.io/no-sane-compiler/#/32\">https://jfbastien.github.io/no-sane-compiler/#/32</a> are valid for SeqCst and don't need escape analysis</p>",
        "id": 175907873,
        "sender_full_name": "Hanna Kruppe",
        "timestamp": 1568728241
    },
    {
        "content": "<blockquote>\n<p>as in, either an operation is atomic, or it tears</p>\n</blockquote>\n<p>well there are atomic menset and atomic memcpy. so... I think you are wrong.</p>",
        "id": 176046721,
        "sender_full_name": "RalfJ",
        "timestamp": 1568840819
    },
    {
        "content": "<blockquote>\n<p>We'll need something stronger than UnorderedVolatile if we want happens-before to propogate between processes as well as threads.</p>\n</blockquote>\n<p>what synchronization do you want to propagate to untrusted code? that's a first.</p>",
        "id": 176046784,
        "sender_full_name": "RalfJ",
        "timestamp": 1568840879
    },
    {
        "content": "<p>not a first :) <a href=\"https://internals.rust-lang.org/t/add-volatile-operations-to-core-x86-64/10480/18?u=comex\" target=\"_blank\" title=\"https://internals.rust-lang.org/t/add-volatile-operations-to-core-x86-64/10480/18?u=comex\">https://internals.rust-lang.org/t/add-volatile-operations-to-core-x86-64/10480/18?u=comex</a></p>",
        "id": 176054530,
        "sender_full_name": "comex",
        "timestamp": 1568848071
    },
    {
        "content": "<p>well see my reply there. I dont see why this would need volatile.</p>",
        "id": 176073729,
        "sender_full_name": "RalfJ",
        "timestamp": 1568876038
    },
    {
        "content": "<p>volatile is for \"I want to reason based on assembly-level memory accesses\"</p>",
        "id": 176073770,
        "sender_full_name": "RalfJ",
        "timestamp": 1568876048
    },
    {
        "content": "<p>Oh, and more fun, AFAICT there's no way to stop another process from truncating your shared memory file at any point, causing a SIGBUS if you access it. <a href=\"https://github.com/asajeffrey/shared-data/issues/7\" target=\"_blank\" title=\"https://github.com/asajeffrey/shared-data/issues/7\">https://github.com/asajeffrey/shared-data/issues/7</a></p>",
        "id": 177165402,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570032778
    },
    {
        "content": "<p>So is there any way to use shared memory safely in Rust? Maaaaybe by putting down a SIGBUS signal handler that tears down the current thread, though I suspect this would mean not even running panic or drop code.\\</p>",
        "id": 177165846,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570033043
    },
    {
        "content": "<p>You are not guaranteed any particular signal, I think.</p>",
        "id": 177171303,
        "sender_full_name": "nagisa",
        "timestamp": 1570036527
    },
    {
        "content": "<p>on linux you want a sealed memfd</p>",
        "id": 177171405,
        "sender_full_name": "nagisa",
        "timestamp": 1570036573
    },
    {
        "content": "<p>specifically you can prevent memfd from being resized by placing a seal on it.</p>",
        "id": 177171487,
        "sender_full_name": "nagisa",
        "timestamp": 1570036642
    },
    {
        "content": "<p>reading manpages, it seems like you should be able to seal regular files too</p>",
        "id": 177171627,
        "sender_full_name": "nagisa",
        "timestamp": 1570036731
    },
    {
        "content": "<p>Ah no, you cannot:</p>\n<blockquote>\n<p>Currently, file seals can be applied only to a file descriptor returned by memfd_create(2) (if the MFD_ALLOW_SEALING was employed). On other filesystems, all fcntl() operations that operate on seals will return EINVAL.</p>\n</blockquote>",
        "id": 177171681,
        "sender_full_name": "nagisa",
        "timestamp": 1570036777
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"123586\">@nagisa</span> Yeah, on Linux it seems like sealed memfd is the way to go; I wonder what the equivalent is on other OSs?</p>",
        "id": 177257096,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570117324
    },
    {
        "content": "<p>Not sure, I haven’t looked in what options there are elsewhere; I bet on Windows you can just do an exclusive lock for that kind of operation on a filesystem file as well.</p>",
        "id": 177257418,
        "sender_full_name": "nagisa",
        "timestamp": 1570117571
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"123586\">@nagisa</span> yes, though it depends on the permission system as to whether you can hand out write permission but not truncate permission.</p>",
        "id": 177257684,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570117744
    },
    {
        "content": "<p>On BSDs you might be able to set a append-only flag: <a href=\"https://man.openbsd.org/FreeBSD-11.1/chflags.2\" target=\"_blank\" title=\"https://man.openbsd.org/FreeBSD-11.1/chflags.2\">https://man.openbsd.org/FreeBSD-11.1/chflags.2</a>, though not sure if that allows mutating the already written data.</p>",
        "id": 177257886,
        "sender_full_name": "nagisa",
        "timestamp": 1570117905
    },
    {
        "content": "<p>Yes, but that makes the current contents immutable :(</p>",
        "id": 177258044,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570118015
    },
    {
        "content": "<p>But coming back to the original question: I would indeed not consider it to be a part of attacker model. What hebaviour you get here seems dependent on the kernel that the code is running on too</p>",
        "id": 177258120,
        "sender_full_name": "nagisa",
        "timestamp": 1570118054
    },
    {
        "content": "<p>i.e. linux may guarantee a sigbus, but  I can imagine a bad kernel just returning garbage <span aria-label=\"slight smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"slight smile\">:slight_smile:</span></p>",
        "id": 177258159,
        "sender_full_name": "nagisa",
        "timestamp": 1570118088
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"123586\">@nagisa</span> yeah, we're into <span aria-label=\"shrug\" class=\"emoji emoji-1f937\" role=\"img\" title=\"shrug\">:shrug:</span> territory here I suspect, about when you're allowed to mark access to shared memory safe.</p>",
        "id": 177258184,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570118119
    },
    {
        "content": "<p>So it would be out-of-scope for unsafe-code-guidelines past what is already specified (e.g. reading unallocated memory is UB)</p>",
        "id": 177258199,
        "sender_full_name": "nagisa",
        "timestamp": 1570118129
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"123586\">@nagisa</span> nondeterministic values are fine, as long as they're not poison, or another form of UB.</p>",
        "id": 177258231,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570118161
    },
    {
        "content": "<p>Well this is a volatile read, so you won’t get poison, but you could read out 42 as a bool <span aria-label=\"slight smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"slight smile\">:slight_smile:</span></p>",
        "id": 177258367,
        "sender_full_name": "nagisa",
        "timestamp": 1570118237
    },
    {
        "content": "<p>Which I guess is the same concern you need to handle when dealing with any shared memory anyway</p>",
        "id": 177258394,
        "sender_full_name": "nagisa",
        "timestamp": 1570118262
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"123586\">@nagisa</span> 42 is fine, there's a lot of jumping through hoops to make sure that any time you construct a &amp;T into shared memory that it's OK, e.g. <code>&amp;AtomicUSize</code> but not <code>&amp;bool</code>.</p>",
        "id": 177258558,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570118363
    },
    {
        "content": "<p>the thing I don't know how to handle safely is the shared memory being truncated <span aria-label=\"frown\" class=\"emoji emoji-1f641\" role=\"img\" title=\"frown\">:frown:</span></p>",
        "id": 177258686,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570118414
    },
    {
        "content": "<p><a href=\"https://github.com/asajeffrey/shared-data\" target=\"_blank\" title=\"https://github.com/asajeffrey/shared-data\">https://github.com/asajeffrey/shared-data</a></p>",
        "id": 177258746,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570118453
    },
    {
        "content": "<p>In general I don't think you <em>can</em> make that avoid a sigbus or equivalent, not on all systems. Certainly not on general POSIX. And even on Linux you can't count on having memfd.</p>",
        "id": 177314800,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570172034
    },
    {
        "content": "<p>That said, I don't think that's <em>unsafe</em>. It might raise a signal, but it won't generate UB.</p>",
        "id": 177314918,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570172209
    },
    {
        "content": "<p>So I think you should just ignore it. (You can recommend memfd though.)</p>",
        "id": 177314926,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570172234
    },
    {
        "content": "<p>So I think you should just ignore it. (You can recommend memfd though.)</p>",
        "id": 177335414,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570193538
    },
    {
        "content": "<p>Is behaviour of reading mapped pages from previously-mapped area of the file that has been since truncated specified for all posixes?</p>",
        "id": 177337695,
        "sender_full_name": "nagisa",
        "timestamp": 1570195193
    },
    {
        "content": "<p>(as in, behaviour of \"you will get a signal\" and not \"unspecified/undefined\")</p>",
        "id": 177337762,
        "sender_full_name": "nagisa",
        "timestamp": 1570195225
    },
    {
        "content": "<p>I've never heard of one that has undefined behavior.</p>",
        "id": 177338887,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570196026
    },
    {
        "content": "<p>You might check the spec.</p>",
        "id": 177338900,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570196035
    },
    {
        "content": "<p>The Single Unix Specification v2 specifies that behavior.</p>",
        "id": 177339475,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570196372
    },
    {
        "content": "<p>I think you can safely rely on it.</p>",
        "id": 177339485,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570196382
    },
    {
        "content": "<p>Ah, hang on...</p>",
        "id": 177339663,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570196521
    },
    {
        "content": "<p>So, it specifies that references past the end will generate SIGBUS.</p>",
        "id": 177339758,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570196585
    },
    {
        "content": "<p>But:</p>",
        "id": 177339767,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570196591
    },
    {
        "content": "<blockquote>\n<p>If the size of the mapped file changes after the call to mmap() as a result of some other operation on the mapped file, the effect of references to portions of the mapped region that correspond to added or removed portions of the file is unspecified.</p>\n</blockquote>",
        "id": 177339782,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1570196611
    },
    {
        "content": "<blockquote>\n<p>Well this is a volatile read, so you won’t get poison, but you could read out 42 as a bool :)</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"123586\">@nagisa</span>  we hope you don't but LLVM doesn't want to guarantee that :(</p>",
        "id": 177712966,
        "sender_full_name": "RalfJ",
        "timestamp": 1570627524
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"128323\">@Alan Jeffrey</span>  regarding SIGBUS, one tricky bit here is that the compiler might have license to insert spurious accesses (e.g. if you had a reference into that memory) so you might get SIGBUS even if your code didn't access that memory</p>",
        "id": 177713060,
        "sender_full_name": "RalfJ",
        "timestamp": 1570627593
    },
    {
        "content": "<p>Indeed. I don't think <code>Atomic*</code> makes any guarantees that it won't introduce extra reads, just that the accesses in your program will be preserved (up to the appropriate <code>Ordering</code>).</p>",
        "id": 177722398,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570633438
    },
    {
        "content": "<p>no it doesnt even guarantee that anything is preserved</p>",
        "id": 177724003,
        "sender_full_name": "RalfJ",
        "timestamp": 1570634303
    },
    {
        "content": "<p>it makes no guarantees in terms of the final assembly memory pattern, only <code>volatile</code> does</p>",
        "id": 177724032,
        "sender_full_name": "RalfJ",
        "timestamp": 1570634320
    },
    {
        "content": "<p>\"atomic\" just guarantees something about the behavior observable <em>inside the Rust program</em></p>",
        "id": 177724047,
        "sender_full_name": "RalfJ",
        "timestamp": 1570634332
    },
    {
        "content": "<p>and with a whole-program analysis, that might well mean optimizing away memory accesses entirely, demoting atomic to non-atomic, or whatever</p>",
        "id": 177724110,
        "sender_full_name": "RalfJ",
        "timestamp": 1570634375
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> I was brushing a lot of stuff under the carpet with \"up to the appropriate <code>Ordering</code>\" :)</p>",
        "id": 177726602,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570635638
    },
    {
        "content": "<p>and yes, it's only giving guarantees to other threads, not other processes.</p>",
        "id": 177726664,
        "sender_full_name": "Alan Jeffrey",
        "timestamp": 1570635677
    },
    {
        "content": "<p>“to other threads of execution”</p>",
        "id": 177780947,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570685907
    },
    {
        "content": "<p>Something like:</p>\n<p>‘’’rust<br>\nfn main() {<br>\n   extern “C” {<br>\n       static PTR: *mut u8;<br>\n   }<br>\n   let x = atomic_load(PTR, Relaxed);<br>\n   let y = atomic_load(PTR, Relaxed);<br>\n   assert_eq!(x, y); // Might fail <br>\n}<br>\n‘’’</p>",
        "id": 177781091,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570686167
    },
    {
        "content": "<p>can fail if there is a second thread of execution mutating the memory behind PTR</p>",
        "id": 177781104,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570686200
    },
    {
        "content": "<p>If we want to be able to support multiple processes Rust cannot assume that won’t happen</p>",
        "id": 177781160,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570686265
    },
    {
        "content": "<p>If Rust can assume that won’t happen, volatile won’t help you here anyways</p>",
        "id": 177781259,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570686393
    },
    {
        "content": "<blockquote>\n<p>can fail if there is a second thread of execution mutating the memory behind PTR</p>\n</blockquote>\n<p>there isnt, though, as we can see the entire program</p>",
        "id": 177788876,
        "sender_full_name": "RalfJ",
        "timestamp": 1570695712
    },
    {
        "content": "<blockquote>\n<p>If Rust can assume that won’t happen, volatile won’t help you here anyways</p>\n</blockquote>\n<p>yes volatile will help precise because it applies assembly-level semantics instead of Rust-level semantics</p>",
        "id": 177788926,
        "sender_full_name": "RalfJ",
        "timestamp": 1570695735
    },
    {
        "content": "<blockquote>\n<p>there isnt, though, as we can see the entire program</p>\n</blockquote>\n<p>If two processes are working on inter-process shared memory, Rust doesn't see all the changes to that memory</p>",
        "id": 177827360,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570724850
    },
    {
        "content": "<p>In the program I have above, <code>PTR</code> address is opaque to Rust, and it could point to inter-process shared memory, so other processes might be modifying it concurrently</p>",
        "id": 177827493,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570724915
    },
    {
        "content": "<p>It should be enough for all processes to use atomics to synchronize reads and write without having to reach for volatile</p>",
        "id": 177827651,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570725024
    },
    {
        "content": "<p>And using volatile would actually prevent optimizations that are sound for this type of program, so it isn't a good solution either.</p>",
        "id": 177827736,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570725087
    },
    {
        "content": "<p>So IMO the problematic issue here is the incorrect assumption that, if the Rust compiler does not see a program creating a second thread of execution, such second thread of execution does not exist</p>",
        "id": 177827831,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570725132
    },
    {
        "content": "<p>This assumption would also break signal handling, e.g., if only the threads of execution that are created by the program exist, then only they can raise signals, so if none of them do, then no signals can be raised and we could optimize appropriately.</p>",
        "id": 177828222,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570725407
    },
    {
        "content": "<blockquote>\n<blockquote>\n<p>there isnt, though, as we can see the entire program</p>\n</blockquote>\n<p>If two processes are working on inter-process shared memory, Rust doesn't see all the changes to that memory</p>\n</blockquote>\n<p>there's no code to set up shared memory so I think it is fair game for rustc to assume there's no such thing</p>",
        "id": 177839958,
        "sender_full_name": "RalfJ",
        "timestamp": 1570732500
    },
    {
        "content": "<p>if you'd call unknown code <em>somewhere</em> -- like, make a single syscall -- the game would be very different</p>",
        "id": 177839996,
        "sender_full_name": "RalfJ",
        "timestamp": 1570732517
    },
    {
        "content": "<blockquote>\n<p>This assumption would also break signal handling, e.g., if only the threads of execution that are created by the program exist, then only they can raise signals, so if none of them do, then no signals can be raised and we could optimize appropriately.</p>\n</blockquote>\n<p>yeah signals suck. I am not aware of any formal model of them.^^</p>",
        "id": 177840062,
        "sender_full_name": "RalfJ",
        "timestamp": 1570732558
    },
    {
        "content": "<p>but to e clear, the only memory that might be shared here is <code>PTR</code>, right? <code>x</code> and <code>y</code> are definitely private and we can do whatever we want</p>",
        "id": 177840169,
        "sender_full_name": "RalfJ",
        "timestamp": 1570732611
    },
    {
        "content": "<p>so we could imagine having a clause that says that the compiler always needs to assume that there <em>might</em> be other threads in the Abstract Machine, even if it can see <code>main</code>. like, the initial state could consist of more than just running <code>main</code>. That seems reaosnable, but I am not  sure if LLVM implements this correctly.</p>",
        "id": 177840361,
        "sender_full_name": "RalfJ",
        "timestamp": 1570732716
    },
    {
        "content": "<p>PTR can have been placed by the linker. And it can have been placed at an MMIO address for example. However, since they are atomic loads and not volatile loads, my understanding is that LLVM is free to make x and y both share the value of a single load.</p>",
        "id": 177858795,
        "sender_full_name": "Lokathor",
        "timestamp": 1570745679
    },
    {
        "content": "<blockquote>\n<p>there's no code to set up shared memory so I think it is fair game for rustc to assume there's no such thing</p>\n</blockquote>\n<p>I don't think that's required ? The OS kernel can set that for you and, e.g., it can share part of the kernel address space with that of a process when that process is launched and put an address to it in a static somewhere (e.g. in the auxiliary vectors of an ELF binary, etc.).</p>\n<blockquote>\n<p>but to e clear, the only memory that might be shared here is PTR, right? x and y are definitely private and we can do whatever we want</p>\n</blockquote>\n<p>Yes, I think that's a reasonable restriction. In practice, this isn't necessarily the case, e.g., a debugger or a profiler might want to inspect what a running thread is doing behind its back, including inspecting its stack, or maybe even modify it, but I think its reasonable for Rust to assume this never happens. If that happens, we provide no guarantees.</p>\n<blockquote>\n<p>so we could imagine having a clause that says that the compiler always needs to assume that there might be other threads in the Abstract Machine, even if it can see main. like, the initial state could consist of more than just running main. That seems reaosnable, but I am not sure if LLVM implements this correctly.</p>\n</blockquote>\n<p>Yes, such a clause is what I had in mind.</p>",
        "id": 177883570,
        "sender_full_name": "gnzlbg",
        "timestamp": 1570778018
    },
    {
        "content": "<blockquote>\n<blockquote>\n<p>One big concern here is reading undef, but even if we added a dedicated intrinsic to Rust, we still couldn't prevent that -- LLVM quite simply does not provide the required primitives to avoid undef here.</p>\n</blockquote>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> LLVM provides volatile inline assembly blocks</p>",
        "id": 178089448,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571047334
    },
    {
        "content": "<p>LLVM probably doesn't guarantee that you can use them to turn <code>undef</code> into \"not <code>undef</code>\", but... it wouldn't not work either</p>",
        "id": 178089465,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571047364
    },
    {
        "content": "<p>That particular implementation strategy would just have \"stronger\" semantics than what we guarantee for those intrinsics</p>",
        "id": 178089504,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571047428
    },
    {
        "content": "<p>(e.g. atomic loads can be re-ordered around volatile loads, but if we were to use such an approach, they probably wouldn't be)</p>",
        "id": 178089570,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571047465
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"132920\">@gnzlbg</span> You have to admit though, that it would be a shame to resolve the issue in an architecture-specific way (and therefore once per architecture), when all it would take to resolve the problem cleanly and portably is for LLVM to close what is admittedly a spec hole that makes \"their\" volatile less well-defined than the underlying hardware loads and stores are.</p>",
        "id": 178100494,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571057954
    },
    {
        "content": "<p>The purpose of undefined behavior is to enable optimizations of higher-level languages. The purpose of volatile is to disable optimizations and defer to hardware semantics. Therefore, the two are fundamentally at odds, and volatile should only have the minimal possible amount of UB to enable optimization of surrounding non-volatile code.</p>",
        "id": 178100592,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571058008
    },
    {
        "content": "<p>Besides, if \"LLVM doesn't guarantee it but it wouldn't not work either\" is considered enough, using today's <code>volatile</code> is arguably enough for many purposes ;)</p>",
        "id": 178100799,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571058191
    },
    {
        "content": "<blockquote>\n<p>when all it would take to resolve the problem cleanly and portably is for LLVM to close what is admittedly a spec hole </p>\n</blockquote>\n<p>That looks like quite a bit of work to me.</p>",
        "id": 178108173,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571063553
    },
    {
        "content": "<p>There is an LLVM bug open, and nobody appears to think that such hole is worth fixing, so if somebody wanted to fix it they would probably need to achieve consensus on what the fix is, implement it, and land it.</p>",
        "id": 178108230,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571063591
    },
    {
        "content": "<p>That is indeed quite a bit of work. However, the same amount of work would be needed to guarantee that inline assembly loads are not allowed to return <code>undef</code>.</p>",
        "id": 178108739,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571063988
    },
    {
        "content": "<p>My point was that we only have to guarantee that at the Rust language level</p>",
        "id": 178108887,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064094
    },
    {
        "content": "<p>Therefore, in my view...</p>\n<p>- If we want to fix that hole, we need LLVM's help, one way or another.<br>\n- If we are satisfied with a \"should work, but isn't guaranteed to\" statu quo, volatile is as good as inline assembly.</p>",
        "id": 178108906,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064111
    },
    {
        "content": "<p>I'm of the opinion that we should provide whatever guarantees we want for Rust</p>",
        "id": 178108978,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064148
    },
    {
        "content": "<p>If the inline assembly gets into LLVM's hands (and I think it does eventually), then we need LLVM's help too.</p>",
        "id": 178108988,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064158
    },
    {
        "content": "<p>On paper, sure. In practice, I'm fine with opening an issue about that, and punting how to fix it when the first user provides an example of a miscompilation</p>",
        "id": 178109038,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064211
    },
    {
        "content": "<blockquote>\n<p>I'm of the opinion that we should provide whatever guarantees we want for Rust</p>\n</blockquote>\n<p>I would agree with you in an ideal world. But as long as <code>rustc</code> uses LLVM as a backend, we are not living in such an ideal world.</p>",
        "id": 178109042,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064215
    },
    {
        "content": "<p>We don't have to use inline assembly, we could use <code>global_asm!</code></p>",
        "id": 178109056,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064227
    },
    {
        "content": "<p>or the linker to call some assembly blob, or ..</p>",
        "id": 178109064,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064235
    },
    {
        "content": "<p>And writing hardware-specific backends is an awful lot of work.</p>",
        "id": 178109066,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064236
    },
    {
        "content": "<p>If we put the intrinsics in <code>core::arch</code>, we can implement them as users need them</p>",
        "id": 178109138,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064270
    },
    {
        "content": "<p>AFAIK, non-inline assembly means one CALL per volatile load or store, which seems too much.</p>",
        "id": 178109154,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064294
    },
    {
        "content": "<p>I don't think its worth it to invest too much time to support, e.g., <code>s390x</code>, when no user might ever need that there</p>",
        "id": 178109176,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064308
    },
    {
        "content": "<blockquote>\n<p>AFAIK, non-inline assembly means one CALL per volatile load or store, which seems too much.</p>\n</blockquote>\n<p>Such is life if inline assembly turns out to not be enough.</p>",
        "id": 178109200,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064337
    },
    {
        "content": "<p>The point is, we can enforce Rust semantics with a hammer if needed be. Right now, we don't have to because inline assembly would work, and maybe even <code>read_volatile</code></p>",
        "id": 178109263,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064364
    },
    {
        "content": "<p>Sure, but just supporting \"mainstream\" platforms already means supporting x86, x86_64, infinitely many dialects of ARM and POWER, RISC-V, WASM...</p>",
        "id": 178109282,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064378
    },
    {
        "content": "<p>We have <code>asm</code> tests in rustc that check the assembly that some code generates, we can add them for these intrinsics</p>",
        "id": 178109285,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064381
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"211871\">@Hadrien Grasland</span> if you want to invest time in implementing the codegen for all those platforms, or to fix LLVM, go for it</p>",
        "id": 178109319,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064417
    },
    {
        "content": "<p>For me, x86_64 is enough</p>",
        "id": 178109321,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064421
    },
    {
        "content": "<p>And if some day all platforms support these, we can even implement generic ones on top</p>",
        "id": 178109335,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064433
    },
    {
        "content": "<p>But I think we are in violent agreement here regarding what concretely needs to be done.</p>",
        "id": 178109341,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064438
    },
    {
        "content": "<p>Yes, I think we are kind of also in agreement of what the ideal implementation would be</p>",
        "id": 178109361,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064458
    },
    {
        "content": "<p>(LLVM fixing their bug)</p>",
        "id": 178109366,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064463
    },
    {
        "content": "<p>But that's not going to happen on its own, and I don't think waiting for that fix should block adding a way to solve these problems in Rust</p>",
        "id": 178109419,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064488
    },
    {
        "content": "<ol>\n<li>Specify the semantics that we want at the Rust level.</li>\n<li>Implement it via LLVM's atomic volatile.</li>\n<li>Add checks that the generated ASM is correct on various archs.</li>\n<li>Postpone any other work, including arch-specific volatile loads and stores, until we actually have a problem that this solves.</li>\n</ol>",
        "id": 178109456,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064535
    },
    {
        "content": "<p>One example is SIMD, x86_64 is good enough for 99% of our current users, some ARM SIMD intrinsics are ok for the rest, MIPS intrinsics for ~1 user, and PPC intrinsics for ~1 user</p>",
        "id": 178109482,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064553
    },
    {
        "content": "<p>The users that cared about MIPS, ARM, and PPC, invested their time into implementing them</p>",
        "id": 178109493,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064570
    },
    {
        "content": "<p>That's short-sighted. Many supercomputing centers use POWER CPUs..</p>",
        "id": 178109511,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064591
    },
    {
        "content": "<p>And ARM SVE is also getting a lot of attention from that community.</p>",
        "id": 178109581,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064617
    },
    {
        "content": "<p>So? If they want to use them, they should implement them, or pay someone that does</p>",
        "id": 178109610,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064633
    },
    {
        "content": "<p>That's how open source works</p>",
        "id": 178109619,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064640
    },
    {
        "content": "<p>We are not making supporting those platforms impossible</p>",
        "id": 178109627,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064650
    },
    {
        "content": "<p>We are providing a trivial way to supporting those platforms</p>",
        "id": 178109632,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064657
    },
    {
        "content": "<p>For anybody interested</p>",
        "id": 178109642,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064662
    },
    {
        "content": "<p>For this particular case, implementing two intrinsics for any platform using inline assembly should be trivial</p>",
        "id": 178109666,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064680
    },
    {
        "content": "<p>My point is, you're introducing extra rustc porting work for every new platform just to solve a problem which we're not sure we actually have.</p>",
        "id": 178109727,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064720
    },
    {
        "content": "<p>And it's not even guaranteed to resolve said problem in any meaningful way.</p>",
        "id": 178109779,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064735
    },
    {
        "content": "<p>Not really</p>",
        "id": 178109813,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064768
    },
    {
        "content": "<p>Adding a new target to rustc doesn't require adding those intrinsics</p>",
        "id": 178109841,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064794
    },
    {
        "content": "<p>E.g. ARM does not support x86 SIMD intrinsics</p>",
        "id": 178109848,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064802
    },
    {
        "content": "<p>Sure. But there's nothing x86-specific about volatile loads and stores.</p>",
        "id": 178109858,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064821
    },
    {
        "content": "<p>So it doesn't belong in core::arch imo.</p>",
        "id": 178109866,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064831
    },
    {
        "content": "<p>No, but since LLVM doesn't guarantee any useful semantics, we can't provide them in a portable way either that's guaranteed to work</p>",
        "id": 178109934,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064860
    },
    {
        "content": "<p>So either we don't provide them, or provide intrinsics that are not guaranteed to work, or provide target-dependent intrinsics that are guaranteed to work</p>",
        "id": 178109949,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064885
    },
    {
        "content": "<p>Right now we provide intrinsics that are not guaranteed to work, and people want more guarantees</p>",
        "id": 178109968,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571064900
    },
    {
        "content": "<p>We can also provide arch-agnostic intrinsics that are guaranteed to work at the spec level, implement them the LLVM way, and wait for bug reports proving that it doesn't work before going for anything more complicated.</p>",
        "id": 178110080,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064970
    },
    {
        "content": "<p>I mean, we're already relying on a lot of undocumented LLVM semantics.</p>",
        "id": 178110093,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571064982
    },
    {
        "content": "<p>Sure.</p>",
        "id": 178110129,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065029
    },
    {
        "content": "<p>But then we are back to \"volatile are hardware semantics\", \"native types\", and all other target-dependent stuff</p>",
        "id": 178110164,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065063
    },
    {
        "content": "<p>We'd need to document the guarantees in a target-dependent way</p>",
        "id": 178110253,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065122
    },
    {
        "content": "<p>Sure. Volatile is an arch-agnostic interface with arch-specific semantics.</p>",
        "id": 178110258,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065125
    },
    {
        "content": "<p>We don't need to document the semantics ourselves, they are in each architecture's respective documentation.</p>",
        "id": 178110277,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065143
    },
    {
        "content": "<p>We need to guarantee that loads / stores won't tear</p>",
        "id": 178110301,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065171
    },
    {
        "content": "<p>All we need to do is to defeat the stupid spec hole that allows a wide volatile access to be split into multiple narrow ones.</p>",
        "id": 178110303,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065175
    },
    {
        "content": "<p>And that can be done just by switching to atomic volatile.</p>",
        "id": 178110312,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065186
    },
    {
        "content": "<p>no, that's not enough</p>",
        "id": 178110321,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065195
    },
    {
        "content": "<p>Why?</p>",
        "id": 178110327,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065200
    },
    {
        "content": "<p>if you then try to use an <code>atomic volatile</code> operation for a platform that doesn't have an appropriate instruction, you'll get at best an LLVM assertion</p>",
        "id": 178110406,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065225
    },
    {
        "content": "<p>more likely a \"instruction fails to select\"-type LLVM error, or a segfault]</p>",
        "id": 178110417,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065240
    },
    {
        "content": "<p>I think I have a way to resolve that.</p>",
        "id": 178110444,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065253
    },
    {
        "content": "<p>at worst, LLVM will just ignore the atomic and emit multiple instructions that tear</p>",
        "id": 178110460,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065266
    },
    {
        "content": "<p>so we need to encode in the front-end all that information for each target</p>",
        "id": 178110481,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065278
    },
    {
        "content": "<p>It's not even a very original one, someone already suggested it before. But it needs to be bikeshedded to death by the RFC process ;)</p>",
        "id": 178110482,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065278
    },
    {
        "content": "<p>The way I propose to resolve this is to add <code>AtomicXyz::load_volatile(self_: *const Self, o: Ordering)</code> and <code>AtomicXyz::store_volatile(self_: *const Self, o: Ordering)</code>.</p>",
        "id": 178110587,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065325
    },
    {
        "content": "<p>If I can call such an intrinsic for a 64-bit load for some target, but that fails to compile for another target, then that intrinsic isn't \"portable\" either</p>",
        "id": 178110612,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065333
    },
    {
        "content": "<p>How is that any different from adding <code>core::arch</code> intrinsics ?</p>",
        "id": 178110647,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065352
    },
    {
        "content": "<p>its just different \"in syntax\", but the atomic types are not available on all targets</p>",
        "id": 178110660,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065364
    },
    {
        "content": "<p>each target exposes different subsets of the atomic types, or even none</p>",
        "id": 178110687,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065375
    },
    {
        "content": "<p>This is being fixed.</p>",
        "id": 178110723,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065389
    },
    {
        "content": "<p>How?</p>",
        "id": 178110738,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065397
    },
    {
        "content": "<p>They are implemented in libcore, so they can't be emulated</p>",
        "id": 178110758,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065412
    },
    {
        "content": "<p>Do they then fail at run-time ?</p>",
        "id": 178110764,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065418
    },
    {
        "content": "<p>By adding support for platforms which have only atomic loads and stores.</p>",
        "id": 178110769,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065422
    },
    {
        "content": "<p>?</p>",
        "id": 178110784,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065437
    },
    {
        "content": "<p>(Which are not actually atomic, but anyhow)</p>",
        "id": 178110830,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065445
    },
    {
        "content": "<p>Where is that being done ?</p>",
        "id": 178110849,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065462
    },
    {
        "content": "<p>Adding atomic types for platforms for which their semantics aren't atomic sounds like a bad idea</p>",
        "id": 178110896,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065493
    },
    {
        "content": "<p>Not even C++ does that</p>",
        "id": 178110901,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065496
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/pull/65214\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/pull/65214\">https://github.com/rust-lang/rust/pull/65214</a></p>",
        "id": 178110904,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065497
    },
    {
        "content": "<p>That's not what I was referring to</p>",
        "id": 178110930,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065513
    },
    {
        "content": "<p>That's just conditionally making different atomic operations available depending on the target</p>",
        "id": 178110949,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065527
    },
    {
        "content": "<p>some targets have 0 operations available</p>",
        "id": 178110958,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065533
    },
    {
        "content": "<p>Sorry, that was wrong.</p>",
        "id": 178110967,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065542
    },
    {
        "content": "<p>Code that uses <code>Atomic_</code> isn't portable</p>",
        "id": 178110969,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065545
    },
    {
        "content": "<blockquote>\n<p>some targets have 0 operations available</p>\n</blockquote>\n<p>Sure about that?</p>",
        "id": 178110984,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065553
    },
    {
        "content": "<p>For any example you give me, I can give you a <code>--target</code> that will fail to compile</p>",
        "id": 178110992,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065560
    },
    {
        "content": "<blockquote>\n<p>Sure about that?</p>\n</blockquote>\n<p>Yes, for example, nvptx64-nvidia-cuda</p>",
        "id": 178111058,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065581
    },
    {
        "content": "<p>Not even relaxed loads and stores of <code>AtomicUsize</code>?</p>",
        "id": 178111079,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065598
    },
    {
        "content": "<p>That's plain broken, as CUDA kernels can actually have atomic operations inside...</p>",
        "id": 178111099,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065618
    },
    {
        "content": "<p>Yes, but not with C11 semantics</p>",
        "id": 178111112,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065627
    },
    {
        "content": "<p>they have different atomic operations, with different semantics, that do not match the orderings that we have</p>",
        "id": 178111125,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065642
    },
    {
        "content": "<p><em>sigh</em> one more argument for adding unordered, I guess.</p>",
        "id": 178111142,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065654
    },
    {
        "content": "<p>There are proposals for trying to bring C11 atomics to nvidia GPUs though</p>",
        "id": 178111147,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065657
    },
    {
        "content": "<p>Anyhow.</p>",
        "id": 178111150,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065658
    },
    {
        "content": "<p>You started this discussion by saying you wanted a solution for mainstream platforms, not obscure niche.</p>",
        "id": 178111238,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065690
    },
    {
        "content": "<p>Now we're definitely getting into obscure niche territory.</p>",
        "id": 178111251,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065702
    },
    {
        "content": "<p>Does i386 have atomics ?</p>",
        "id": 178111267,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065717
    },
    {
        "content": "<p>Yes</p>",
        "id": 178111274,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065723
    },
    {
        "content": "<p>we also have at least one i386 target</p>",
        "id": 178111277,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065727
    },
    {
        "content": "<p>Otherwise you couldn't implement a mutex on it.</p>",
        "id": 178111281,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065730
    },
    {
        "content": "<p>IMO, getting volatile to work as intended on every platform that has <code>Relaxed</code> atomic loads and stores would already be a reasonable achievement.</p>",
        "id": 178111467,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065840
    },
    {
        "content": "<p>And I could live with having arch-specific abstractions <em>for the other platforms</em> until either 1/they become C11-compliant or 2/we add weaker atomic orderings that match their semantics.</p>",
        "id": 178111548,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571065908
    },
    {
        "content": "<blockquote>\n<p>IMO, getting volatile to work as intended on every platform that has Relaxed atomic loads and stores would already be a reasonable achievement.</p>\n</blockquote>\n<p>This should be doable for those platforms</p>",
        "id": 178111607,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571065933
    },
    {
        "content": "<p>I think this would match the spirit of what you're trying to do with x86-specific volatile operations, but in a way that can reach basically all mainstream platforms targeted by Rust.</p>",
        "id": 178111766,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066048
    },
    {
        "content": "<p>Without custom assembly, CALL overhead on each volatile operation, etc.</p>",
        "id": 178111782,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066068
    },
    {
        "content": "<p>oh, check this out: <a href=\"https://www.youtube.com/watch?v=VogqOscJYvk\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=VogqOscJYvk\">https://www.youtube.com/watch?v=VogqOscJYvk</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"VogqOscJYvk\" href=\"https://www.youtube.com/watch?v=VogqOscJYvk\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=VogqOscJYvk\"><img src=\"https://i.ytimg.com/vi/VogqOscJYvk/default.jpg\"></a></div>",
        "id": 178111792,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571066072
    },
    {
        "content": "<p>I haven't watched it, but maybe they manage to make C++ atomics work with cuda already?</p>",
        "id": 178111806,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571066092
    },
    {
        "content": "<p>IIRC, the problem is that GPUs are not cache-coherent.</p>",
        "id": 178111993,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066237
    },
    {
        "content": "<p>So they miss one guarantee from C11, which is that all threads must observe stores to a given memory location as occuring in the same order.</p>",
        "id": 178112014,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066260
    },
    {
        "content": "<p>Unless fences are used to flush them.</p>",
        "id": 178112088,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066287
    },
    {
        "content": "<p>They could probably be modeled by LLVM's unordered atomics, but well... I think we've established that someone must do a PhD on them before they're accepted into the Rust memory model.</p>",
        "id": 178112155,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066340
    },
    {
        "content": "<p>yes,  just like POWER, GPUs share store buffers between threads</p>",
        "id": 178112190,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571066368
    },
    {
        "content": "<p>or at least nvidia ones do</p>",
        "id": 178112200,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571066377
    },
    {
        "content": "<p>so multiple stores can be viewed by other threads in the wrong orders</p>",
        "id": 178112284,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571066408
    },
    {
        "content": "<p>They have store coherence on a given Compute Unit, but not across CUs, I believe.</p>",
        "id": 178112285,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066409
    },
    {
        "content": "<p>yes</p>",
        "id": 178112292,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571066413
    },
    {
        "content": "<blockquote>\n<p>I think we've established that someone must do a PhD on them before they're accepted into the Rust memory model.</p>\n</blockquote>\n<p>More like 5 PhDs should build up an operational semantics for Rust using atomics, and then another 5 PhDs should extend that with further useful orderings</p>",
        "id": 178112381,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571066493
    },
    {
        "content": "<p>That was <span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> plan all along</p>",
        "id": 178112415,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571066524
    },
    {
        "content": "<p>So barring that, NVidia providing a way to implement C11 Relaxed on top of nvptx, even if with suboptimal efficiency, sounds like a more reasonable short-term plan.</p>",
        "id": 178112482,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066553
    },
    {
        "content": "<p>Possibly combined with NVPTX-specific unordered atomic intrinsics with a big warning attached to them.</p>",
        "id": 178112676,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066683
    },
    {
        "content": "<p>Maybe they did already, that cppcon video is  a couple of days old</p>",
        "id": 178112683,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571066694
    },
    {
        "content": "<p>\"We have no formal Rust semantics for these and they will eat your laundry if you try to use them to synchronize Rust memory operations. See the CUDA Programming Guide to learn more about their hardware-level semantics.\"</p>",
        "id": 178112711,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066722
    },
    {
        "content": "<p>But thanks, that's actually useful material for one question which I'm currently asking myself as part of my nefarious plan for soft-deprecating non-atomic volatile.</p>",
        "id": 178113088,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571066996
    },
    {
        "content": "<p>Namely, do we actually a platform where we can have non-atomic volatile, but not Relaxed atomic volatile.</p>",
        "id": 178113178,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571067040
    },
    {
        "content": "<p>Now I have one example.</p>",
        "id": 178113181,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571067044
    },
    {
        "content": "<p>Posted at <a href=\"https://github.com/rust-lang/unsafe-code-guidelines/issues/152\" target=\"_blank\" title=\"https://github.com/rust-lang/unsafe-code-guidelines/issues/152\">https://github.com/rust-lang/unsafe-code-guidelines/issues/152</a></p>",
        "id": 178113547,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571067318
    },
    {
        "content": "<p>So, I have given the NVidia presentation a quick skim.</p>",
        "id": 178114026,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571067669
    },
    {
        "content": "<p>Essentially, they propose to have C11-like atomic semantics, but on a restricted device scope (e.g. thread block).</p>",
        "id": 178114051,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571067707
    },
    {
        "content": "<p>So atomics don't have C11-like semantics within a warp ?</p>",
        "id": 178114292,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571067881
    },
    {
        "content": "<p>But they do across warps?</p>",
        "id": 178114307,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571067892
    },
    {
        "content": "<p>No, I think it's more like, you have C11 semantics \"up to\" a certain scope.</p>",
        "id": 178114334,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571067914
    },
    {
        "content": "<p>That could be only inside a warp (architecturally free), or within warps + thread blocks (what CUDA has today if I understand correctly).</p>",
        "id": 178114365,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571067954
    },
    {
        "content": "<p>But again, I only gave this a quick skim, and that's definitely not enough for something as subtle as a memory model.</p>",
        "id": 178114431,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571067982
    },
    {
        "content": "<p>Also, best slide ever. <a href=\"/user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png\" target=\"_blank\" title=\"SAT.png\">SAT.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png\" target=\"_blank\" title=\"SAT.png\"><img src=\"/user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png\"></a></div>",
        "id": 178114602,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571068110
    },
    {
        "content": "<p>Apparently, they even have a way to do device-wide or cross-device atomics if you're interested in some expensive fences.</p>",
        "id": 178115003,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571068423
    },
    {
        "content": "<p>But well, I'm speculating quite a bit here, and this presentation is more about <em>how</em> they got C++-style atomics than <em>what</em> the finally shipped product actually does.</p>",
        "id": 178115365,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571068683
    },
    {
        "content": "<p>I tried to search for documentation a bit on the web, but it doesn't seem published yet. Even the CUDA programming guide is not up to date yet, it's not marked as implemented in the famous <a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cpp11-language-features\" target=\"_blank\" title=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cpp11-language-features\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cpp11-language-features</a> table, nor does this document contain any occurence of \"std::atomic\" or \"&lt;atomic&gt;\"</p>",
        "id": 178115628,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571068881
    },
    {
        "content": "<p>So I suspect that this hasn't shipped in an actual CUDA toolkit release yet.</p>",
        "id": 178116614,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571069643
    },
    {
        "content": "<p>And it will be hard to make any progress on nvptx until it does.</p>",
        "id": 178116839,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571069793
    },
    {
        "content": "<p>So, to summarize...</p>\n<p>For the common case of a platform that has at least <code>Relaxed</code> loads and stores that directly map into native loads and stores, I think we agree that <code>AtomicXyz::load_volatile/store_volatile</code> is better than current <code>ptr::read_volatile/write_volatile</code> in every way and should basically replace it:</p>\n<ul>\n<li>It almost perfectly matches hardware-defined data race semantics (and is guaranteed to as long as it is the only kind of memory access in use on that memory location).</li>\n<li>It does not build if hardware loads and stores of the requested size do not exist (and AFAIK the error is nicely reported at the Rust layer, not at the LLVM layer, which is the main argument I have for putting this in the <code>AtomicXyz</code> API instead of having some sort of <code>ptr::read_atomic_volatile</code>)</li>\n</ul>\n<p>I <em>personally think</em> that <code>atomic volatile</code> would be good enough for \"defer to hardware semantics\" use cases in order to alleviate the need for platform-specific volatile load and store intrinsics in <code>core::arch</code> for those platforms. I'll probably keep thinking that unless someone can come up with a concrete case of LLVM compiling  atomic volatile loads or stores into anything other than a hardware load and stores in an important use case like interaction with untrusted code via shared memory.</p>\n<p>So far, even if LLVM don't want to guarantee that it won't happen, it has never happened, and it is unclear under which circumstances that could possibly happen (as it seems that UB-induced miscompilation could only be triggered by knowledge which LLVM doesn't have, such as \"this memory location is never initialized\" or \"some foreign code will do a non-atomic store there\"). Therefore, I think the \"wait for a bug report\" strategy is fine. If and when that happens we can think about going for more complex solutions to this problem, such as custom assembly.</p>\n<p>The intrinsics route can still be useful today, in the case of platforms like nvptx which...</p>\n<ol>\n<li>Cannot have Relaxed atomic volatile operations, or which implement them in a needlessly inefficient way.</li>\n<li>Benefit from having sane volatile ops (in the nvptx case, that can be used when calling NVidia's synchronization intrinsics).</li>\n</ol>\n<p>Sounds like a plan?</p>",
        "id": 178118386,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571071079
    },
    {
        "content": "<p>Sounds good to me</p>",
        "id": 178118688,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571071292
    },
    {
        "content": "<p>Regarding the API I think you raised an issue about <code>Self</code> pointer types, aren't those supported on nightly behind <code>abstract_self_types</code> ? If so, then the standard library can just use them.</p>",
        "id": 178118779,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571071356
    },
    {
        "content": "<p>But I don't know how that feature works for raw pointers exactly</p>",
        "id": 178118803,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571071380
    },
    {
        "content": "<p>Can clients of the standard library using stable do so as well?</p>",
        "id": 178118812,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571071385
    },
    {
        "content": "<p>In any case, if Self pointer types are supported, I would expect all the basic cases to work, and brokenness to lie in fat pointer corner cases like trait objects or slices ;)</p>",
        "id": 178118978,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571071499
    },
    {
        "content": "<p>Okay, I tested it.</p>",
        "id": 178119650,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571072103
    },
    {
        "content": "<p>It's clunkier than references because there isn't e.g. an automatic *mut -&gt; *const conversion, but <a href=\"https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=48ded877d702297a1b54bb1442cb2010\" target=\"_blank\" title=\"https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=48ded877d702297a1b54bb1442cb2010\">it actually works</a>.</p>",
        "id": 178119679,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571072129
    },
    {
        "content": "<p>People may be anxious about de-facto stabilizing this feature by making it part of a public std API though.</p>",
        "id": 178119907,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571072326
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"211871\">@Hadrien Grasland</span> clients of the standard library can use standard library APIs that use this feature on stable</p>",
        "id": 178122397,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571074559
    },
    {
        "content": "<p>they cannot use the feature themselves on stable Rust to write their own APIs</p>",
        "id": 178122444,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571074575
    },
    {
        "content": "<p>This much I know. What I'm afraid of is that someone could say that it is not acceptable to stabilize a public API that uses <code>#![feature(arbitrary_self_types)]</code>, because then it is not possible to remove or significantly change the <code>arbitrary_self_types</code> feature without breaking the public API of std, which amounts to making a compatibility-binding promise that at least the relevant subset of <code>arbitrary_self_types</code> will be eventually stabilized.</p>",
        "id": 178125336,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571076903
    },
    {
        "content": "<p>For example, at the time where the array traits implementation was switched to const generics, <span class=\"user-mention\" data-user-id=\"126931\">@centril</span> made sure that this implementation was fully hidden from the public API for this reason. In that case, I don't think they feared that const generics would be eventually removed, but understand that they were more anxious about the possibility of const generics semantics changing in a fundamental way that would alter the semantics of the public std API throughout the feature development process.</p>",
        "id": 178125728,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571077177
    },
    {
        "content": "<p>As far as I know, the \"pointer receiver\" subset of <code>arbitrary_self_types</code> is somewhat contentious at this point in time, and it's not 100% clear whether we want to eventually stabilize it or not. Therefore, there might be similar reservations about the prospect of exposing it in a public API.</p>",
        "id": 178125912,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571077304
    },
    {
        "content": "<blockquote>\n<p>LLVM provides volatile inline assembly blocks</p>\n</blockquote>\n<p>I have no idea what that means</p>",
        "id": 178136761,
        "sender_full_name": "RalfJ",
        "timestamp": 1571085412
    },
    {
        "content": "<blockquote>\n<p>On paper, sure. In practice, I'm fine with opening an issue about that, and punting how to fix it when the first user provides an example of a miscompilation</p>\n</blockquote>\n<p>I find it highly unlikely that an affected user will be able to identify that they are affected, let alone provide a clear miscompilation examples. So this strategy IMO equals \"we ignore the problem and put our head into the sand\".<br>\nWhich, admittedly, seems like a fair approach with LLVM :P</p>",
        "id": 178136824,
        "sender_full_name": "RalfJ",
        "timestamp": 1571085469
    },
    {
        "content": "<blockquote>\n<blockquote>\n<p>I think we've established that someone must do a PhD on them before they're accepted into the Rust memory model.</p>\n</blockquote>\n<p>More like 5 PhDs should build up an operational semantics for Rust using atomics, and then another 5 PhDs should extend that with further useful orderings</p>\n</blockquote>\n<p>I don't think the scope is quite that large^^ but yes I personally think that to accept more things into our concurrency model I'd like to see at least 2 papers studying the same model and proving some stuff about it.<br>\nthat said, I do realize that those are high standards and could totally understand if the lang team would set the bar lower.</p>",
        "id": 178137053,
        "sender_full_name": "RalfJ",
        "timestamp": 1571085628
    },
    {
        "content": "<p>(also I didn't read all of your conversation, I figured the gist of it ends up on GH. too much text here.^^)</p>",
        "id": 178137119,
        "sender_full_name": "RalfJ",
        "timestamp": 1571085683
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> In GCC, marking an inline assembly block volatile means that the compiler can neither move it around in code nor remove it.</p>",
        "id": 178137154,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571085718
    },
    {
        "content": "<p>LLVM probably understands it similarly.</p>",
        "id": 178137218,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571085736
    },
    {
        "content": "<p>And yes, I did add the TL;DR to my github post.</p>",
        "id": 178137307,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571085801
    },
    {
        "content": "<p>Basically, we ended up agreeing that atomic volatile is fine for most archs, but there are actual archs like nvptx where regular loads and stores are not Relaxed and we may want to keep non-atomic volatile for the sake of those.</p>",
        "id": 178137416,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571085860
    },
    {
        "content": "<p>Also, here's a nice NVidia slide for you : <a href=\"user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png\" target=\"_blank\" title=\"user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png\">https://rust-lang.zulipchat.com/user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png</a></p>\n<div class=\"message_inline_image\"><a href=\"user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png\" target=\"_blank\" title=\"https://rust-lang.zulipchat.com/user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png\"><img src=\"user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png\"></a></div>",
        "id": 178138012,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571086278
    },
    {
        "content": "<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"120791\">RalfJ</span> In GCC, marking an inline assembly block volatile means that the compiler can neither move it around in code nor remove it.</p>\n</blockquote>\n<p>wait so normally the compiler can remove it...?!?</p>",
        "id": 178138473,
        "sender_full_name": "RalfJ",
        "timestamp": 1571086612
    },
    {
        "content": "<p>and how is \"you cant move it around\" different from \"it clobbers all the state of everything\"?</p>",
        "id": 178138508,
        "sender_full_name": "RalfJ",
        "timestamp": 1571086634
    },
    {
        "content": "<p>IIRC, compiler can indeed drop an asm block if it can prove that e.g. the clobbers are not observable.</p>",
        "id": 178138544,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571086668
    },
    {
        "content": "<p>Yeah, optimizers are a pain at times.</p>",
        "id": 178138620,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571086683
    },
    {
        "content": "<blockquote>\n<p>Also, here's a nice NVidia slide for you : <a href=\"user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png\" target=\"_blank\" title=\"user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png\">https://rust-lang.zulipchat.com/user_uploads/4715/yoBU9sbUqpr5SJ7IPpkeiJn-/SAT.png</a></p>\n</blockquote>\n<p>yeah seen that :D<br>\nnot big into SAT solvers / model checkers myself. they work great for whole-program stuff but I'm more interested in compositional methods. but that's just saying my research focus is different, those are still great tools!</p>",
        "id": 178138621,
        "sender_full_name": "RalfJ",
        "timestamp": 1571086684
    },
    {
        "content": "<blockquote>\n<p>IIRC, compiler can indeed drop an asm block if it can prove that e.g. the clobbers are not observable.</p>\n</blockquote>\n<p>lol</p>",
        "id": 178138649,
        "sender_full_name": "RalfJ",
        "timestamp": 1571086704
    },
    {
        "content": "<blockquote>\n<p>that said, I do realize that those are high standards and could totally understand if the lang team would set the bar lower.</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"120791\">@RalfJ</span> If you think that's where the bar should be set then I will set the bar there <span aria-label=\"slight smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"slight smile\">:slight_smile:</span></p>",
        "id": 178169350,
        "sender_full_name": "centril",
        "timestamp": 1571126244
    },
    {
        "content": "<blockquote>\n<p>I have no idea what that means</p>\n</blockquote>\n<p>It means that no operations are re-ordered around the inline assembly block IIUC, and that the block is also assumed to have side-effects</p>",
        "id": 178175429,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571131911
    },
    {
        "content": "<blockquote>\n<p>wait so normally the compiler can remove it...?!?</p>\n</blockquote>\n<p>That's how I understand it. Rust makes all inline assembly blocks \"volatile\", but LLVM doesn't require that</p>",
        "id": 178175499,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571131976
    },
    {
        "content": "<p>So if your inline assembly block would be \"<code>const</code>\" (like a <code>const fn</code> in the Rust sense), then LLVM could execute it once, cache the result, and replace other executions with the result</p>",
        "id": 178175543,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571132016
    },
    {
        "content": "<p>or if you write a assembly block that has no inputs, no outputs, and no effects, LLVM could just remove it</p>",
        "id": 178175654,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571132105
    },
    {
        "content": "<p>\"volatile inline assembly\" is a GCC-ism and a misleading name IMO. LLVM IR calls the flag <code>sideeffect</code>, a much more indicative name: the asm is doing <em>something</em> besides what's visible from the output specifiers, and of course correct optimizations must not duplicate or drop that side effect in any execution or reorder it with respect to other side effects. But unlike the <code>volatile</code> term it can't be interpreted as meaning \"don't duplicate this snippet of assembly in the <code>--emit asm</code> output\" (which is something some people want when e.g. inserting labels into the assembly for instrumentation or post-processing).</p>",
        "id": 178177186,
        "sender_full_name": "Hanna Kruppe",
        "timestamp": 1571133575
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 178212210,
        "sender_full_name": "Lokathor",
        "timestamp": 1571158322
    },
    {
        "content": "<p>So my question is, and I think I've asked this before and gotten a vague answer: if a platform has no atomic instructions, how do atomic types work? Like old ARM has no atomics, can the compiler just fake it and use normal load/store as relaxed or something like that?</p>",
        "id": 178212442,
        "sender_full_name": "Lokathor",
        "timestamp": 1571158498
    },
    {
        "content": "<p>This PR by <span class=\"user-mention\" data-user-id=\"143274\">@Amanieu</span> suggests that it can work indeed: <a href=\"https://github.com/rust-lang/rust/pull/65214\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/pull/65214\">https://github.com/rust-lang/rust/pull/65214</a></p>",
        "id": 178215868,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571160929
    },
    {
        "content": "<p>Conceptually speaking, there is no reason why it shouldn't, since <code>Relaxed atomic load/store</code> basically translates into \"global cache coherence on regular loads and stores\" in hardware, and most CPUs actually guarantee this.</p>",
        "id": 178215984,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571161000
    },
    {
        "content": "<p>(though as <span class=\"user-mention\" data-user-id=\"132920\">@gnzlbg</span> found out, GPUs don't)</p>",
        "id": 178215999,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571161013
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"224471\">@Lokathor</span> The <code>Atomic*</code> types are still available but they only provide <code>load</code> and <code>store</code>, not any of the other atomic operations.</p>",
        "id": 178216000,
        "sender_full_name": "Amanieu",
        "timestamp": 1571161016
    },
    {
        "content": "<p>oh well that's fine then</p>",
        "id": 178216195,
        "sender_full_name": "Lokathor",
        "timestamp": 1571161140
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"143274\">@Amanieu</span> Out of curiosity, do we ever allow ourselves to not support all atomic orderings ?</p>",
        "id": 178216252,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571161182
    },
    {
        "content": "<p>That could be convenient if we ever allowed ourselves to port to platforms without global cache coherence.</p>",
        "id": 178216340,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571161214
    },
    {
        "content": "<p>No, all orderings must be supported.</p>",
        "id": 178216355,
        "sender_full_name": "Amanieu",
        "timestamp": 1571161228
    },
    {
        "content": "<p>OK, then if I understand the <code>nvptx</code> weirdness correctly, we'll need to do something similar to what NVidia did in C++ and provide hardware-specific \"scoped atomics\" which only synchronize at a certain scale.</p>",
        "id": 178216454,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571161298
    },
    {
        "content": "<p>If we ever want atomics on nvptx, that is.</p>",
        "id": 178216469,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571161309
    },
    {
        "content": "<p>In practice this means that the architecture must support a simple memory barrier instruction (SeqCst fence) or be so simple (no OoO, caches, etc) that only a compiler fence is needed.</p>",
        "id": 178216472,
        "sender_full_name": "Amanieu",
        "timestamp": 1571161313
    },
    {
        "content": "<p>I'll need to look into nvptx, but yea, this sounds like the kind of thing that you would put into std::arch</p>",
        "id": 178216545,
        "sender_full_name": "Amanieu",
        "timestamp": 1571161344
    },
    {
        "content": "<p>To save you some search trouble, it seems that the relevant NVidia atomic header is not yet part of the published version of the CUDA toolkit, and only exists as fragments of slides in a C++ conference for now.</p>",
        "id": 178216609,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571161399
    },
    {
        "content": "<p>I'm eagerly waiting for the matching release of NVidia tooling and docs to finally get a proper understanding of what GPU memory models actually look like. Seems they are quite a bit weirder than CPU memory models.</p>",
        "id": 178216666,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571161442
    },
    {
        "content": "<p>I think it's simply a matter of scoping. Normal atomics work across the entire address space, while nvptx has faster atomics that only work within the current thread group or something similar.</p>",
        "id": 178216812,
        "sender_full_name": "Amanieu",
        "timestamp": 1571161526
    },
    {
        "content": "<p>This is what I understood, but I would like to cross-check my understanding.</p>",
        "id": 178216894,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571161572
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"143274\">@Amanieu</span> ARM (at least the early versions) would fall under the \"so simple\" category</p>",
        "id": 178216912,
        "sender_full_name": "Lokathor",
        "timestamp": 1571161590
    },
    {
        "content": "<p>Yes, anything pre-ARMv6 is basically single-core</p>",
        "id": 178216933,
        "sender_full_name": "Amanieu",
        "timestamp": 1571161612
    },
    {
        "content": "<p>and even then, I think ARMv5/ARMv4 have memory barrier instructions, through CP15</p>",
        "id": 178217017,
        "sender_full_name": "Amanieu",
        "timestamp": 1571161662
    },
    {
        "content": "<p>I don't recall seeing a memory barrier in ARMv4T, but i could have missed something</p>",
        "id": 178217126,
        "sender_full_name": "Lokathor",
        "timestamp": 1571161722
    },
    {
        "content": "<blockquote>\n<p>Amanieu 7:45 PM   <br>\nI think it's simply a matter of scoping. Normal atomics work across the entire address space, while nvptx has faster atomics that only work within the current thread group or something similar.</p>\n</blockquote>\n<p>Well, this is a good way to see it I guess.</p>",
        "id": 178219501,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571163310
    },
    {
        "content": "<p>The main issue is that normal atomics on such a target are so slow that nobody would use them.</p>",
        "id": 178219542,
        "sender_full_name": "gnzlbg",
        "timestamp": 1571163335
    },
    {
        "content": "<p>I think this schematic from AMD's GCN presentations best sums up how fsck'd up GPU cache hierarchies are <a href=\"https://mynameismjp.files.wordpress.com/2018/01/amd_caches.png\" target=\"_blank\" title=\"https://mynameismjp.files.wordpress.com/2018/01/amd_caches.png\">https://mynameismjp.files.wordpress.com/2018/01/amd_caches.png</a> .</p>\n<div class=\"message_inline_image\"><a href=\"https://mynameismjp.files.wordpress.com/2018/01/amd_caches.png\" target=\"_blank\" title=\"https://mynameismjp.files.wordpress.com/2018/01/amd_caches.png\"><img src=\"https://mynameismjp.files.wordpress.com/2018/01/amd_caches.png\"></a></div>",
        "id": 178222093,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571164980
    },
    {
        "content": "<p>(schematic also gives an overly simple view as texture L1 cache is also split per compute unit)</p>",
        "id": 178222151,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571165027
    },
    {
        "content": "<p>(while other data and icache may be shared between multiple CUs, and of course flushes are manual... yeah, I'm really curious about how the NVidia folks managed to formalize that kind of craziness)</p>",
        "id": 178222284,
        "sender_full_name": "Hadrien Grasland",
        "timestamp": 1571165095
    },
    {
        "content": "<blockquote>\n<p><a href=\"https://github.com/asajeffrey/shared-data\" target=\"_blank\" title=\"https://github.com/asajeffrey/shared-data\">https://github.com/asajeffrey/shared-data</a></p>\n</blockquote>\n<p>oh wow I entirely missed this link, would definitely have linked you to our crates earlier</p>",
        "id": 179998930,
        "sender_full_name": "nagisa",
        "timestamp": 1573009068
    }
]