[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120827\">@Ben Kimock (Saethlin)</span> you have run miri on many crates. Are your tools available somewhere publicly? I want to run it on a few hundred crates and build a histogram of how long it takes to get a broad picture of how much CPU time we'd need to run it on all of <a href=\"http://crates.io\">crates.io</a></p>",
        "id": 276494125,
        "sender_full_name": "oli",
        "timestamp": 1648136463
    },
    {
        "content": "<p>The code that I'm running is right here: <a href=\"https://github.com/saethlin/miri-tools/blob/main/src/main.rs\">https://github.com/saethlin/miri-tools/blob/main/src/main.rs</a></p>\n<p>If you want to run it as-is, you need to <code>docker build . -t miri:latest</code> then <code>cargo r -- --crates=100</code> or such.</p>\n<p>It's rather purpose-built, and I of course cannot endorse the quality of most of it. I started with using <code>rustwide</code> to implement this, but eventually got into a huge argument with it because <code>rustwide</code>'s toolchain management really does not work if you want to use a custom build of Miri.</p>",
        "id": 276495310,
        "sender_full_name": "Ben Kimock (Saethlin)",
        "timestamp": 1648136881
    },
    {
        "content": "<p>I'm 163 crates in (on account of how many times I've screwed up the setup over the past few days)</p>\n<p>Only one crate has OOMed (adler), the next highest is pkg-config at a very neat 4 GB. Perhaps my fears about memory usage weren't backed up by data after all??</p>\n<p>I don't cache <em>anything</em> in the builds.<br>\nNo crate finishes faster than 28 seconds. But 24 crates have hit my 15 minute wall time limit. The average crate uses about 84 seconds of CPU time. I wonder if that statistic will change a lot as I pick up more crates.</p>",
        "id": 276702981,
        "sender_full_name": "Ben Kimock (Saethlin)",
        "timestamp": 1648266064
    },
    {
        "content": "<p>Thank you! That are great numbers</p>",
        "id": 276710359,
        "sender_full_name": "oli",
        "timestamp": 1648277975
    },
    {
        "content": "<p>Rounding up to 2 minutes per crate, that's 30 crates per hour or 720 per day. To get to 72000 (simplified from 80000) we need 100 cores. Of course that means we lose every 7th crate due to the 15 min limit. I guess we can try running with a higher limit on a 96 core machine for a few days. And if that becomes a problem I can probably get something bigger.</p>\n<p>Going with a mostly worst case assumption of 4GB memory usage, we need 400GB of RAM just to be safe when running 96 crates in parallel. Gotta check what I can requisition.</p>\n<p>I'll have a look at your tool. Maybe I could change it into two tools? One generating machine readable data and one generating a webpage from it? That could simplify doing analysis on the results. Would that be something you'd be ok with?</p>\n<p>Similarly I'd like to add a flag for parallelism.</p>",
        "id": 276710897,
        "sender_full_name": "oli",
        "timestamp": 1648278816
    },
    {
        "content": "<p>I just added parallelism</p>",
        "id": 276728127,
        "sender_full_name": "Ben Kimock (Saethlin)",
        "timestamp": 1648304068
    },
    {
        "content": "<p>(crudely)</p>",
        "id": 276728142,
        "sender_full_name": "Ben Kimock (Saethlin)",
        "timestamp": 1648304102
    },
    {
        "content": "<p>More than anything, the tool/codebase needs any kind of thought about how it's laid out. Right now it just spits out stdout+stderr and aggressively re-renders everything so that I can adjust the rendering code and restart it to see the effect. This is obviously not great. I feel like there is some place for a database better way to organize build logs.</p>\n<p>I'm interested in most things.</p>",
        "id": 276728313,
        "sender_full_name": "Ben Kimock (Saethlin)",
        "timestamp": 1648304366
    },
    {
        "content": "<p>You could switch cargo to output json (including colored diagnostics as a field), that could make processing easier. Not sure about storage and databases. I'll think on it</p>",
        "id": 276729345,
        "sender_full_name": "oli",
        "timestamp": 1648305859
    }
]