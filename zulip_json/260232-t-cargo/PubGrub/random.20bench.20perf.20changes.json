[
    {
        "content": "<p>Something that is kind of annoying me is that I'm getting random perf changes between bench runs, without touching the code, other software or hardware change. Here is the latest I got:</p>\n<div class=\"codehilite\"><pre><span></span><code>Benchmarking large_cases/elm_packages_str_SemanticVersion.ron: Warming up for 3.0000 s\nWarning: Unable to complete 100 samples in 20.0s. You may wish to increase target time to 22.8s, or reduce sample count to 80.\nlarge_cases/elm_packages_str_SemanticVersion.ron\n                        time:   [225.16 ms 226.06 ms 226.93 ms]\n                        change: [+3.0095% +4.1701% +5.3402%] (p = 0.00 &lt; 0.05)\n                        Performance has regressed.\nFound 1 outliers among 100 measurements (1.00%)\n  1 (1.00%) low mild\nBenchmarking large_cases/zuse_str_SemanticVersion.ron: Warming up for 3.0000 s\nWarning: Unable to complete 100 samples in 20.0s. You may wish to increase target time to 38.5s, or reduce sample count to 50.\nlarge_cases/zuse_str_SemanticVersion.ron\n                        time:   [354.92 ms 357.46 ms 360.05 ms]\n                        change: [-5.7625% -5.0405% -4.3488%] (p = 0.00 &lt; 0.05)\n                        Performance has improved.\nlarge_cases/large_case_u16_NumberVersion.ron\n                        time:   [9.9669 ms 10.014 ms 10.064 ms]\n                        change: [+4.8475% +5.8246% +6.8089%] (p = 0.00 &lt; 0.05)\n                        Performance has regressed.\nFound 7 outliers among 100 measurements (7.00%)\n  7 (7.00%) high mild\n</code></pre></div>",
        "id": 239958534,
        "sender_full_name": "Matthieu Pizenberg",
        "timestamp": 1621783610
    },
    {
        "content": "<p>And 2 min later, the following run:</p>\n<div class=\"codehilite\"><pre><span></span><code>Benchmarking large_cases/elm_packages_str_SemanticVersion.ron: Warming up for 3.0000 s\nWarning: Unable to complete 100 samples in 20.0s. You may wish to increase target time to 24.1s, or reduce sample count to 80.\nlarge_cases/elm_packages_str_SemanticVersion.ron\n                        time:   [226.96 ms 227.74 ms 228.55 ms]\n                        change: [+0.2257% +0.7434% +1.2806%] (p = 0.01 &lt; 0.05)\n                        Change within noise threshold.\nBenchmarking large_cases/zuse_str_SemanticVersion.ron: Warming up for 3.0000 s\nWarning: Unable to complete 100 samples in 20.0s. You may wish to increase target time to 39.0s, or reduce sample count to 50.\nlarge_cases/zuse_str_SemanticVersion.ron\n                        time:   [380.32 ms 381.46 ms 382.64 ms]\n                        change: [+5.8767% +6.7136% +7.5496%] (p = 0.00 &lt; 0.05)\n                        Performance has regressed.\nFound 5 outliers among 100 measurements (5.00%)\n  5 (5.00%) high mild\nlarge_cases/large_case_u16_NumberVersion.ron\n                        time:   [9.2866 ms 9.2912 ms 9.2961 ms]\n                        change: [-7.6816% -7.2217% -6.7840%] (p = 0.00 &lt; 0.05)\n                        Performance has improved.\nFound 2 outliers among 100 measurements (2.00%)\n  1 (1.00%) high mild\n  1 (1.00%) high severe\n</code></pre></div>",
        "id": 239958683,
        "sender_full_name": "Matthieu Pizenberg",
        "timestamp": 1621783781
    },
    {
        "content": "<p>Do you experience similar bench instability?</p>",
        "id": 239958741,
        "sender_full_name": "Matthieu Pizenberg",
        "timestamp": 1621783810
    },
    {
        "content": "<p>In short yes. Criterion is incredibly sensitive to <em>everything</em>. At some point I was able to count the number of minimized documentation tabs I had open by looking at the perf numbers. That is when I realized that listening to music, looking at the internet, even interacting with an IDE, are all out when running perf tests. And you never know what the OS is doing in the background.  So I try hard to not compare numbers that where collected more the ~5 mins apart. My preferred workflow is with only a terminal open (no other programs even minimized), <code>git checkout base</code> then <code>cargo bench ...</code> then  <code>git checkout pr</code> then <code>cargo bench ...</code> then <code>git checkout base</code> then <code>cargo bench ...</code>. If the improvements from <code>base</code> -&gt; <code>pr</code> are about the same as the degradation from <code>pr</code> -&gt; <code>base</code> then I have some confidence in the test. I don't always live up to that standard. This all got a lot easier when I got a desktop. I do all my development, IDE, documentation, and music on my laptop.  When things are working, and I am ready for a perf test, I use ssh to git pull the two branches and run the benchmarks on my desktop, witch is otherwise completely idle.</p>",
        "id": 239962775,
        "sender_full_name": "Eh2406",
        "timestamp": 1621787982
    },
    {
        "content": "<p>Another option would be to use an AWS instance that's running nothing else.</p>",
        "id": 239968517,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1621793591
    },
    {
        "content": "<p>Easy to spin up.</p>",
        "id": 239968525,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1621793606
    }
]