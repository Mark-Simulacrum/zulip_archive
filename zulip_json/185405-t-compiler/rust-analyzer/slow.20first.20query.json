[
    {
        "content": "<p>I'm not sure if this is expected, but even on relatively small projects (e.g., a single <a href=\"http://lib.rs\" target=\"_blank\" title=\"http://lib.rs\">lib.rs</a> with ~200 lines), when I first open a file in vim, the server pretty much instantaneously prints \"workspace loaded, 12 rust packages\". Then, when I use \"go to definition\" on anything, the server will jump to 100% CPU for maybe 2-3 seconds before responding.</p>\n<p>On larger projects (e.g., rustc) that 2-3 seconds is probably more like 15 seconds, though I haven't measured. Certainly non-instantaneous. This means that my workflow in pretty much any coding session is to open vim, then go to some random line in a file, and \"goto definition\" and wait. After the first time I've done this, future queries are pretty much instantaneous -- but waiting for the first time is annoying.</p>\n<p>I don't know whether I should file a bug. I guess the behavior I would have expected is to provide some way to say \"please eagerly compute things in the background\" -- most of the time, I'll only run a query after dozens of seconds have passed, so I don't need instantaneous feedback as soon as my editor opens... but I would prefer that I don't have to wait once I do decide to run a query.</p>\n<p>One considerations is that in my case, at least, I don't care at all about battery/CPU usage/memory usage pretty much, because I'm running rust analyzer on a remote server (where I also run rustc and cargo), whereas my editor is local (on my laptop). That server has enough CPU cores and so forth that I'm not worried about rust analyzer doing work up front.</p>\n<p>Happy to provide more details -- this is probably long enough already :)</p>",
        "id": 188258217,
        "sender_full_name": "simulacrum",
        "timestamp": 1581723961
    },
    {
        "content": "<p>Yeah, this is a known issue: <a href=\"https://github.com/rust-analyzer/rust-analyzer/issues/1650\" target=\"_blank\" title=\"https://github.com/rust-analyzer/rust-analyzer/issues/1650\">https://github.com/rust-analyzer/rust-analyzer/issues/1650</a></p>",
        "id": 188258478,
        "sender_full_name": "matklad",
        "timestamp": 1581724191
    },
    {
        "content": "<p>The fundamental fix here is to implement on-disk persistence, so that everything can be cached and quickly loaded from disk.</p>\n<p>However, we probably should prioritize a short-term remedy: eagerly \"prime\" freshly opened files, and parallelize the analysis</p>",
        "id": 188258582,
        "sender_full_name": "matklad",
        "timestamp": 1581724262
    },
    {
        "content": "<p>Yes, to be clear, I'd be more than happy with doing lots of (potentially unnecessary) work at startup. Disk space is actually at more of a premium :)</p>",
        "id": 188264878,
        "sender_full_name": "simulacrum",
        "timestamp": 1581732842
    },
    {
        "content": "<p>Thanks for the feedback though, that issue was an interesting read. I'll have to take a look at the Cargo.lock vs toml thing at least.</p>",
        "id": 188267086,
        "sender_full_name": "simulacrum",
        "timestamp": 1581737506
    },
    {
        "content": "<p>Can we notify the user through progress notifications on the first slow query while everything is 'warming up' so they at least know that something is going on?</p>",
        "id": 188289880,
        "sender_full_name": "Jeremy Kolb",
        "timestamp": 1581784690
    },
    {
        "content": "<p>We can, but I do feel that there's some low hanging fruit to make it actually faster that we should pick first. Like, an extremely basic thing to do would be to collect profile traces for this case and see if we do something stupid.</p>",
        "id": 188300595,
        "sender_full_name": "matklad",
        "timestamp": 1581804303
    },
    {
        "content": "<p>Pushed a bench harness for goto-def specifically: <a href=\"https://github.com/rust-analyzer/rust-analyzer/pull/3172\" target=\"_blank\" title=\"https://github.com/rust-analyzer/rust-analyzer/pull/3172\">https://github.com/rust-analyzer/rust-analyzer/pull/3172</a></p>\n<p>Here's a run for one of my projects, haven't investigated this yet</p>\n<p><a href=\"https://gist.github.com/matklad/1812e96746e1d2befa5858da0cc888c4\" target=\"_blank\" title=\"https://gist.github.com/matklad/1812e96746e1d2befa5858da0cc888c4\">https://gist.github.com/matklad/1812e96746e1d2befa5858da0cc888c4</a></p>",
        "id": 188332605,
        "sender_full_name": "matklad",
        "timestamp": 1581874359
    },
    {
        "content": "<p>Looked a little bit and found interesting high-level bit we should optimize. A lot of time is spend collecting the impls. We need to collect all impls eagarly for trait solving, thatâ€™s how it works. However, our code for impls is lazy: we process each imple one by one, and, I think, we also look into impls bodies for that.</p>\n<p>I think we need to change this to store impl headers (trait and type name, but probably not generics and where clauses) inside modules in crate def map, and only compute impl bodies lazily. Cc <span class=\"user-mention\" data-user-id=\"129457\">@Florian Diebold</span> , this looks like a fun find.</p>",
        "id": 188333374,
        "sender_full_name": "matklad",
        "timestamp": 1581875939
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 188368255,
        "sender_full_name": "std::Veetaha",
        "timestamp": 1581936398
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> what exactly are you using with <code>vim</code>? I think there are many implementations of LSP for vim, I wonder if this somehow relates to this as well...</p>",
        "id": 188981046,
        "sender_full_name": "matklad",
        "timestamp": 1582590712
    },
    {
        "content": "<p>ah, coc with vim 8 I thnk</p>",
        "id": 188981098,
        "sender_full_name": "simulacrum",
        "timestamp": 1582590736
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"133169\">@matklad</span> in theory I can try to run some experiments or so if it would be helpful</p>",
        "id": 188981903,
        "sender_full_name": "simulacrum",
        "timestamp": 1582591737
    },
    {
        "content": "<p>likewise I'm running coc with vim 8 too</p>",
        "id": 188986738,
        "sender_full_name": "qmx",
        "timestamp": 1582598164
    },
    {
        "content": "<p>happy to do some debugging</p>",
        "id": 188986739,
        "sender_full_name": "qmx",
        "timestamp": 1582598170
    },
    {
        "content": "<p>Status update here: in the recent update, we forecfully initiate analysis of opened files after workspace is loaded. It's not the super correct logic, but it shoudl help. In particular, one problem is that, if you modifiy a file during initial analysis, it won't be resceduled. </p>\n<p>Low-hanging fruit about lazy impls turned out to be quite a yak shave: <a href=\"https://github.com/rust-analyzer/rust-analyzer/issues/3485\" target=\"_blank\" title=\"https://github.com/rust-analyzer/rust-analyzer/issues/3485\">https://github.com/rust-analyzer/rust-analyzer/issues/3485</a><br>\nI've also did a quick experiement with parallelization, but it was blocked by the same yak shave: <a href=\"https://github.com/rust-analyzer/rust-analyzer/pull/3529\" target=\"_blank\" title=\"https://github.com/rust-analyzer/rust-analyzer/pull/3529\">https://github.com/rust-analyzer/rust-analyzer/pull/3529</a></p>",
        "id": 190200862,
        "sender_full_name": "matklad",
        "timestamp": 1583861295
    }
]