[
    {
        "content": "<p>I am trying to use Currency Visualizer to analyse this bug, It still in progress, but I found out that a lot of times in main-loop thread is wasting on waiting the salsa inner <code>RWLock</code>.</p>",
        "id": 185692950,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579091687
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/cx4RiIgmG5HvIfJpC3kUGfDr/thread.png\" target=\"_blank\" title=\"thread.png\">thread.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/cx4RiIgmG5HvIfJpC3kUGfDr/thread.png\" target=\"_blank\" title=\"thread.png\"><img src=\"/user_uploads/4715/cx4RiIgmG5HvIfJpC3kUGfDr/thread.png\"></a></div>",
        "id": 185693259,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579091995
    },
    {
        "content": "<p>-deleted-</p>",
        "id": 185693514,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579092155
    },
    {
        "content": "<p>The left side of the frame I added is the time the <code>Send&lt;Enter&gt;</code>  request is sent in the test thread.</p>",
        "id": 185693630,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579092257
    },
    {
        "content": "<p>And the actual lock is in <a href=\"https://github.com/salsa-rs/salsa/blob/d1e9471bee517e66b8af2b82e263878d02353547/src/runtime.rs#L313\" target=\"_blank\" title=\"https://github.com/salsa-rs/salsa/blob/d1e9471bee517e66b8af2b82e263878d02353547/src/runtime.rs#L313\">https://github.com/salsa-rs/salsa/blob/d1e9471bee517e66b8af2b82e263878d02353547/src/runtime.rs#L313</a></p>",
        "id": 185693712,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579092358
    },
    {
        "content": "<p>Wild guess: the comment on that function says \"Acquires the <strong>global query write lock</strong> (ensuring that no queries are executing)\". Maybe some readers are starving the main thread which is trying make a change?</p>",
        "id": 185695776,
        "sender_full_name": "Laurențiu",
        "timestamp": 1579093963
    },
    {
        "content": "<p>(I'm not familiar with the code, nor can I see the colors you mention)</p>",
        "id": 185695837,
        "sender_full_name": "Laurențiu",
        "timestamp": 1579094039
    },
    {
        "content": "<p>Yes, and this is what the test is doing..  <a href=\"https://github.com/rust-analyzer/rust-analyzer/blob/4444192b05c107a40a5a05ea3c9091ad8f8cbbcc/crates/ra_lsp_server/tests/heavy_tests/main.rs#L463\" target=\"_blank\" title=\"https://github.com/rust-analyzer/rust-analyzer/blob/4444192b05c107a40a5a05ea3c9091ad8f8cbbcc/crates/ra_lsp_server/tests/heavy_tests/main.rs#L463\">https://github.com/rust-analyzer/rust-analyzer/blob/4444192b05c107a40a5a05ea3c9091ad8f8cbbcc/crates/ra_lsp_server/tests/heavy_tests/main.rs#L463</a></p>",
        "id": 185695881,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579094048
    },
    {
        "content": "<p>Oh, I am color weakness, so I just grap the RGB value and use <a href=\"https://www.color-blindness.com/color-name-hue/\" target=\"_blank\" title=\"https://www.color-blindness.com/color-name-hue/\">https://www.color-blindness.com/color-name-hue/</a> to tell the color</p>",
        "id": 185695913,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579094091
    },
    {
        "content": "<p>:D</p>",
        "id": 185695974,
        "sender_full_name": "Laurențiu",
        "timestamp": 1579094154
    },
    {
        "content": "<p>That's a useful site. I was in a work meeting about 6 months ago and a coworker realized that he was color blind when someone had green and red stoplights in an excel spreadsheet...</p>",
        "id": 185696043,
        "sender_full_name": "Jeremy Kolb",
        "timestamp": 1579094193
    },
    {
        "content": "<p>On the other hand, in that issue matklad mentioned stuff being blocked on clippy, not RA diagnostics. I'm not sure how that works.</p>",
        "id": 185696066,
        "sender_full_name": "Laurențiu",
        "timestamp": 1579094229
    },
    {
        "content": "<p>The scheduler on windows isn't very fair and it could be exposing the issue.</p>",
        "id": 185696168,
        "sender_full_name": "Jeremy Kolb",
        "timestamp": 1579094290
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/ovCBIYYSAeVQ9SoAVnk7ARlv/color_table.png\" target=\"_blank\" title=\"color_table.png\">color_table.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/ovCBIYYSAeVQ9SoAVnk7ARlv/color_table.png\" target=\"_blank\" title=\"color_table.png\"><img src=\"/user_uploads/4715/ovCBIYYSAeVQ9SoAVnk7ARlv/color_table.png\"></a></div>",
        "id": 185696198,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579094312
    },
    {
        "content": "<p>It could also be something different, like slower disk access</p>",
        "id": 185696271,
        "sender_full_name": "Laurențiu",
        "timestamp": 1579094384
    },
    {
        "content": "<p>antivirus...</p>",
        "id": 185696326,
        "sender_full_name": "Jeremy Kolb",
        "timestamp": 1579094408
    },
    {
        "content": "<p>Clippy just make the case worse, OTOH github VM is only have 2 core, so that's why bors always failed <span aria-label=\"smiley\" class=\"emoji emoji-1f603\" role=\"img\" title=\"smiley\">:smiley:</span></p>",
        "id": 185696328,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579094413
    },
    {
        "content": "<p>But why is clippy running during that test? Aren't the files in-memory?</p>",
        "id": 185696479,
        "sender_full_name": "Laurențiu",
        "timestamp": 1579094557
    },
    {
        "content": "<p>And one of the lock is aquired is, when we create a world snapshot (e.g.<code> update_file_notifications_on_threadpool</code>) , a read lock is held inside SalsaDB.</p>",
        "id": 185697081,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579095049
    },
    {
        "content": "<p>I tried the CPU affinity trick on Linux, that request takes ~80 ms on a single core (and 7 ms without setting the affinity)</p>",
        "id": 185698082,
        "sender_full_name": "Laurențiu",
        "timestamp": 1579095854
    },
    {
        "content": "<p>I think I've noticed at leas something suspicious!</p>\n<p>cc <span class=\"user-mention\" data-user-id=\"212936\">@Emil Lauridsen</span> </p>\n<p>In <code>WorldSnapshot</code>, we store an <code>Arc&lt;RwLock&lt;CheckWatcherSharedState&gt;&gt;</code>. We read lock this lock in <code>handle_diagnostics</code>.</p>\n<p>Additionally, we <code>.write</code> this lock from the watcher thread in <code>CheckWatcherState::run</code>. </p>\n<p>I think in general this is less then ideal, b/c diagnostics request can be blocked on another thread. I think it makes sense to architect this in a way which does not block.</p>\n<p>For that, we stop sharing the state between <code>ServerWorld</code> and <code>CheckWatcherState</code>. Instead, the watcher thread sends new diagnostics via a channel, and we accomodate thouse diagnostics intot he server state in the main loop.</p>\n<p>So, instead of </p>\n<div class=\"codehilite\"><pre><span></span><span class=\"k\">struct</span> <span class=\"nc\">Server</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"n\">diagnostics</span>: <span class=\"nc\">Arc</span><span class=\"o\">&lt;</span><span class=\"n\">Mutex</span><span class=\"o\">&lt;</span><span class=\"nb\">Vec</span><span class=\"o\">&lt;</span><span class=\"n\">Diagnostics</span><span class=\"o\">&gt;&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"k\">struct</span> <span class=\"nc\">Watcher</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"n\">diagnostics</span>: <span class=\"nc\">Arc</span><span class=\"o\">&lt;</span><span class=\"n\">Mutex</span><span class=\"o\">&lt;</span><span class=\"nb\">Vec</span><span class=\"o\">&lt;</span><span class=\"n\">Diagnostics</span><span class=\"o\">&gt;&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</pre></div>\n\n\n<p>we'll have something like this:</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"k\">struct</span> <span class=\"nc\">Server</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"c1\">// this bit now *owns* diagnostics</span>\n<span class=\"w\">    </span><span class=\"n\">diagnostisc</span>: <span class=\"nb\">Vec</span><span class=\"o\">&lt;</span><span class=\"n\">Diagnostics</span><span class=\"o\">&gt;</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"k\">struct</span> <span class=\"nc\">Watcher</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"n\">diagnostics_sink</span>: <span class=\"nc\">Sender</span><span class=\"o\">&lt;</span><span class=\"nb\">Vec</span><span class=\"o\">&lt;</span><span class=\"n\">Diagnostics</span><span class=\"o\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</pre></div>\n\n\n<p>I am not sure this is the cuprit of slowness on widnows, but I think we should fix it, because it's very useful when all changes to the server's state can occur only via the main loop.</p>\n<p>Note how VFS is set up in a similar way: instead of modifing some global hash map with files, VFS sends a message to the main looop that hey, I have these new files for you. The main loop than incorporates the changes itself.</p>",
        "id": 185699257,
        "sender_full_name": "matklad",
        "timestamp": 1579096725
    },
    {
        "content": "<p>Note that I think we'll still need some locks here, to share the state between <code>ServerWorld</code> and <code>WorldSnapshot</code>, but we won't actually be changing anyting mid-snapshot</p>",
        "id": 185699318,
        "sender_full_name": "matklad",
        "timestamp": 1579096783
    },
    {
        "content": "<p>That seems possible, and a reasonable re-factoring. I had considered that the locking could be problematic, but haven't actually looked into it yet.  Give me 45 minutes or so and I'll try to have a draft PR up for testing</p>",
        "id": 185700144,
        "sender_full_name": "Emil Lauridsen",
        "timestamp": 1579097273
    },
    {
        "content": "<p>In general, those  <code>RwLocks</code> in <code>ServerWorld</code> are sort-of fake. </p>\n<p>What we <em>really</em> have, conceptually, is a <em>single</em> <code>RwLock</code> across the whole state. </p>\n<p>The only thing that <code>.write</code>s to this lock is the main loop. </p>\n<p>All handlers are run in the background and <code>.read</code> from the lock.</p>\n<p>When a main loop wants to lock the state, but there's some background processing, it cancells background task, which die and release the look for the loop to catch it.</p>\n<p>The reason why this is not literarly strutcutre that way is that the most important lock is actually hidden in salsa, and we can't piggy back on it unfortunately :(</p>",
        "id": 185701062,
        "sender_full_name": "matklad",
        "timestamp": 1579097816
    },
    {
        "content": "<p>PR is up: <a href=\"https://github.com/rust-analyzer/rust-analyzer/pull/2853\" target=\"_blank\" title=\"https://github.com/rust-analyzer/rust-analyzer/pull/2853\">https://github.com/rust-analyzer/rust-analyzer/pull/2853</a></p>",
        "id": 185705174,
        "sender_full_name": "Emil Lauridsen",
        "timestamp": 1579100303
    },
    {
        "content": "<p>Okay, I finally conclude a theory what happens in  <code>diagnostics_dont_block_typing</code> case.</p>\n<p>Some facts:</p>\n<ol>\n<li><code>DidOpenTextDocument</code> notification will be translate to <code>Event::Vfs</code> in <code>main-loop</code>and it will trigger the <code>RWLock</code> write lock in main-loop.</li>\n<li>A write lock will be waited until all other WorldSnapshot cancelled. </li>\n<li>We handle event one by one in main-loop, basically is a read, check, apply-change cycle.</li>\n</ol>\n<p>What <code>diagnostics_dont_block_typing</code> do is, pushing 10 <code>DidOpenTextDocument</code> notification, and then push a <code>OnEnter</code> request and check whether that <code>OnEnter</code> event can handle in a short period. Actually all these events are pushed to main-loop almost instantly. </p>\n<p>However, in the main-loop, (e.g let say we named these events as [1d,2d,3d,4d,5d... , <code>OnEnter</code>]), when we handle <code>2d</code>, because it is a <code>Event::Vfs</code>, it have to wait for <code>1d</code> to cancel. Such that all these events will be handled sequentially.  And depends on how fast the main-loop is processing, and if the <code>1d</code> is already in very deep phase (e.g some parsing phase), it will be quite slow.</p>\n<p>OTOH, if we raise the mainloop priority, the main-loop will be processing very fast such that all these events will be in a very shallow phase, the cancellation will be quite fast and that's why it is fast in that case.</p>\n<p>Do I miss something important I miss to consider ?</p>",
        "id": 185783743,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579147357
    },
    {
        "content": "<p>Hm, I think <code>2</code> is not true. Snapshots don't really hold the read lock all the time, they only acquire readlock for a very brief moment in time to copy data out of the VFS.</p>",
        "id": 185803776,
        "sender_full_name": "matklad",
        "timestamp": 1579172352
    },
    {
        "content": "<p>It might be best to look at logging/profiling output out of the main_loop.</p>\n<div class=\"codehilite\"><pre><span></span>    let _p = profile(&quot;main_loop_inner/loop-turn&quot;);\n    log::info!(&quot;loop turn = {:?}&quot;, event);\n</pre></div>\n\n\n<p>These two lines should show us exactly which event takes long time to process</p>",
        "id": 185804000,
        "sender_full_name": "matklad",
        "timestamp": 1579172538
    },
    {
        "content": "<p>I mean salsa snapshot rwlock</p>",
        "id": 185804310,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579172833
    },
    {
        "content": "<p>This one : <a href=\"https://github.com/salsa-rs/salsa/blob/d1e9471bee517e66b8af2b82e263878d02353547/src/runtime.rs#L35\" target=\"_blank\" title=\"https://github.com/salsa-rs/salsa/blob/d1e9471bee517e66b8af2b82e263878d02353547/src/runtime.rs#L35\">https://github.com/salsa-rs/salsa/blob/d1e9471bee517e66b8af2b82e263878d02353547/src/runtime.rs#L35</a></p>",
        "id": 185804434,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579172969
    },
    {
        "content": "<p>I think this should not be a problem, by itself. </p>\n<ol>\n<li>main loop gets DidOpen from the editor</li>\n<li>main loop write-locks vfs and pushes a new pending change. This should be fast (assuming lock fairness), b/c we never hold vfs lock for long</li>\n<li>main loop calls <code>world_state.process_changes()</code>, which actually intorporates the change</li>\n<li><code>process_changes</code> calls <code>self.analysis_host.apply_change(change)</code>, which advances salsa's revision (thus cancelling background tasks) and waits for the salsa lock</li>\n<li>now, all background tasks should be cancelled as soon as they do a next salsa query. They can spend some CPU time finish parsing or something like that, but it shoulnd take more than a dozen of millis</li>\n</ol>",
        "id": 185804891,
        "sender_full_name": "matklad",
        "timestamp": 1579173351
    },
    {
        "content": "<p>in step 4, it have to wait for the current task to cancel, so that means as in <code>diagnostics_dont_block_typing</code>, all events will be handle sequentially, right ? if these cancelling take 80 mills, it will take 800 mills.</p>",
        "id": 185805318,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579173736
    },
    {
        "content": "<p>Hmm, I see.</p>\n<p>So your hypothesis is that we have 10 <code>DidOpen</code> events, and every one of them takes <em>some</em> time to process, but together it ends up to a substantional delay?</p>",
        "id": 185805498,
        "sender_full_name": "matklad",
        "timestamp": 1579173938
    },
    {
        "content": "<p>yes !</p>",
        "id": 185805570,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579173980
    },
    {
        "content": "<p>That is possible I think, though I'd still expect that we spend &lt;200ms per event</p>",
        "id": 185805580,
        "sender_full_name": "matklad",
        "timestamp": 1579173988
    },
    {
        "content": "<p>We really need to look at the traces and profiling info...</p>",
        "id": 185805630,
        "sender_full_name": "matklad",
        "timestamp": 1579174050
    },
    {
        "content": "<p>But even we spend only 80ms per event, 10 events add up will be 800ms</p>",
        "id": 185805634,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579174063
    },
    {
        "content": "<p>One of the problem here for logging is, IO will disturb how thread behavior. But  I have some event graphs captured by Concurrency Visualizer which show these behavior, I will post it here when Im back home.</p>",
        "id": 185806281,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579174590
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/HA2Irrx4AXEUQku79tikX0ig/prove2.png\" target=\"_blank\" title=\"prove2.png\">prove2.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/HA2Irrx4AXEUQku79tikX0ig/prove2.png\" target=\"_blank\" title=\"prove2.png\"><img src=\"/user_uploads/4715/HA2Irrx4AXEUQku79tikX0ig/prove2.png\"></a></div>",
        "id": 185812123,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579179725
    },
    {
        "content": "<p>I hope it could be proved my hypothesis:)</p>",
        "id": 185812163,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579179782
    },
    {
        "content": "<p>off topic but where do you get concurrency visualizer from?</p>",
        "id": 185815682,
        "sender_full_name": "Jeremy Kolb",
        "timestamp": 1579182475
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"203366\">@Jeremy Kolb</span> <a href=\"https://marketplace.visualstudio.com/items?itemName=Diagnostics.ConcurrencyVisualizer2017\" target=\"_blank\" title=\"https://marketplace.visualstudio.com/items?itemName=Diagnostics.ConcurrencyVisualizer2017\">https://marketplace.visualstudio.com/items?itemName=Diagnostics.ConcurrencyVisualizer2017</a></p>\n<p>Installed as Visual Studio 2017 Extension</p>",
        "id": 185819195,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1579185049
    }
]