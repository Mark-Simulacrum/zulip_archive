[
    {
        "content": "<p>I'm looking into <a href=\"https://github.com/rust-analyzer/rust-analyzer/issues/9358\">https://github.com/rust-analyzer/rust-analyzer/issues/9358</a>, and it looks like rust-analyzer is generally a bit prone to run into stack overflows, which makes we wonder, perhaps we should make use of <code>stacker</code>, like rustc does, to prevent these problems from bringing down the whole server?</p>",
        "id": 243513752,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624367701
    },
    {
        "content": "<p>Specifically, I managed to provoke a stack overflow in the parser, with macro output that is still well below the token limit of 524288 tokens</p>",
        "id": 243513858,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624367749
    },
    {
        "content": "<p>Or maybe I'm overthinking this, and we should instead try to detect exponential macros once the output goes beyond a certain size?</p>",
        "id": 243513990,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624367796
    },
    {
        "content": "<p>In this case we go from <code>input 36859 -&gt; output 73727</code> tokens just before the stack overflow</p>",
        "id": 243514131,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624367853
    },
    {
        "content": "<p>I think we should rather place limits on the depths of things, and err out when we hit them. Stack space is just one resource, it's possible to burn loads of CPU without burning loads of stack.  See, for example, delightful <code>test_macro_expand_will_stop_2</code> test</p>",
        "id": 243514253,
        "sender_full_name": "matklad",
        "timestamp": 1624367894
    },
    {
        "content": "<p>That being said, I think it's OK to bump the actual stack limit to 64 megs or something</p>",
        "id": 243514306,
        "sender_full_name": "matklad",
        "timestamp": 1624367921
    },
    {
        "content": "<p>Ah, good call, I figured we'd have some related test</p>",
        "id": 243514497,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624368006
    },
    {
        "content": "<p>For parser, we already have <code>start_node</code> / <code>finish_node</code>, so we can use those as depth check.</p>",
        "id": 243514528,
        "sender_full_name": "matklad",
        "timestamp": 1624368023
    },
    {
        "content": "<p>The question is, what to do when we hit the bottom? I'd rather not return an <code>Error</code>, we should never fail. I guess, the parser can just give up and put all the rest of the tokens into an error node?</p>",
        "id": 243514642,
        "sender_full_name": "matklad",
        "timestamp": 1624368084
    },
    {
        "content": "<p>Btw, in the <code>test_macro_expand_will_stop_2</code>'s <code>do_resolve</code>, try renaming <code>not_ra_fixture</code> to <code>ra_fixture</code> ^^</p>",
        "id": 243514892,
        "sender_full_name": "matklad",
        "timestamp": 1624368193
    },
    {
        "content": "<p>no thanks :D</p>",
        "id": 243515206,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624368345
    },
    {
        "content": "<p>I already crashed r-a several times while adding a test for the stack overflow</p>",
        "id": 243515272,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624368363
    },
    {
        "content": "<p>Originally, it was names something like <code>code</code>, and I got into the trap of \"fixing\" it a couple of times</p>",
        "id": 243517038,
        "sender_full_name": "matklad",
        "timestamp": 1624369111
    },
    {
        "content": "<p>hmm, this all seems to relate to how we recover from mismatched delimiters</p>",
        "id": 243530394,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624374334
    },
    {
        "content": "<p>the macro only produces a subtree with a depth of &lt;30, but it contains a lot of unmatched <code>(</code> tokens that don't increase the depth, and the parser chokes on that</p>",
        "id": 243530448,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624374361
    },
    {
        "content": "<p>perhaps we should change subtree lowering to always produce subtrees with matched delimiters?</p>",
        "id": 243530540,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624374386
    },
    {
        "content": "<p>cc <a href=\"https://github.com/rust-analyzer/rust-analyzer/pull/4680#issuecomment-641346748\">https://github.com/rust-analyzer/rust-analyzer/pull/4680#issuecomment-641346748</a></p>",
        "id": 243531726,
        "sender_full_name": "matklad",
        "timestamp": 1624374921
    },
    {
        "content": "<p>hard problem, we need to figure out which invariants we want to have here</p>",
        "id": 243531760,
        "sender_full_name": "matklad",
        "timestamp": 1624374942
    },
    {
        "content": "<p>I was thinking to just do it in the <code>syntax_bridge</code> without involving the parser</p>",
        "id": 243533131,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624375406
    },
    {
        "content": "<p>would result in tokens that don't actually exist in the input though</p>",
        "id": 243533150,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1624375416
    }
]