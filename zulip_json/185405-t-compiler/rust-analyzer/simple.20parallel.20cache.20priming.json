[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"211727\">@Jonas Schievink  [he/him]</span> I am looking at the <a href=\"https://github.com/rust-analyzer/rust-analyzer/blob/5c704f11d2ef82d7517680bba6dd0015d750fca2/crates/ide/src/prime_caches.rs#L24-L26\">https://github.com/rust-analyzer/rust-analyzer/blob/5c704f11d2ef82d7517680bba6dd0015d750fca2/crates/ide/src/prime_caches.rs#L24-L26</a> and I am having a crazy idea!</p>",
        "id": 251260693,
        "sender_full_name": "matklad",
        "timestamp": 1630343231
    },
    {
        "content": "<p>Recently, I've been thinking that it's in general bad for code to <em>do</em> work in parallel or concurrently. It's much better if the code just <em>returns</em> the work (packaged up as closures or something), and lets the caller to schedule the work over multiple threads</p>",
        "id": 251260902,
        "sender_full_name": "matklad",
        "timestamp": 1630343303
    },
    {
        "content": "<p>So, what if we split the <code>prime_caches</code> thing into two functions:</p>\n<ul>\n<li>give me a toposort list of crates</li>\n<li>prime caches for crate i</li>\n</ul>",
        "id": 251261026,
        "sender_full_name": "matklad",
        "timestamp": 1630343354
    },
    {
        "content": "<p>Than, in the main loop, we put the toposorted list into <code>Arc&lt;ConcurrentQueue&lt;Crate&gt;&gt;</code>, and spawn 4 tasks onto a theradpool which share the queue and just pull items off it</p>",
        "id": 251261123,
        "sender_full_name": "matklad",
        "timestamp": 1630343400
    },
    {
        "content": "<p>(the actual idea here I guess is \"let's not use rayon, lets use our own thread pool\")</p>",
        "id": 251261284,
        "sender_full_name": "matklad",
        "timestamp": 1630343451
    },
    {
        "content": "<p>Interesting idea</p>",
        "id": 251262121,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630343773
    },
    {
        "content": "<p>Although I'm not sure that FIXME comment about rayon is really accurate, I feel like we should be able to make it work with rayon too</p>",
        "id": 251262230,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630343835
    },
    {
        "content": "<p>An interesting side-effect of such inversion of concurrency API is that the progress becomes a non-issue -- rather than accepting this weird dyn Sync closure, you let the caller to iterate</p>",
        "id": 251263347,
        "sender_full_name": "matklad",
        "timestamp": 1630344294
    },
    {
        "content": "<p>I think we also want a better topological sort, with ready sets.</p>",
        "id": 251289404,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630355042
    },
    {
        "content": "<p>Like in this Python API <a href=\"https://docs.python.org/3/library/graphlib.html\">https://docs.python.org/3/library/graphlib.html</a></p>",
        "id": 251289546,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630355099
    },
    {
        "content": "<p>Otherwise it seems harder to use a thread pool</p>",
        "id": 251289660,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630355159
    },
    {
        "content": "<p>Would it make sense to allow preemption such that you can skip the topo-sorting or allow it to be a heuristic sort? If workers can ask for the result of another item and suspend themselves to an \"in progress but paused\" pool, you may be able to skip sorting entirely, or operate on incomplete topology information.</p>",
        "id": 251326475,
        "sender_full_name": "Zoey",
        "timestamp": 1630380330
    },
    {
        "content": "<p>Toposort is needed for accurate progress reports as priming one crate will automatically at least partially prime all dependencies first as the information is necessary for analysis of the to be primed crate.</p>",
        "id": 251335942,
        "sender_full_name": "bjorn3",
        "timestamp": 1630389520
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"211727\">@Jonas Schievink  [he/him]</span> am I correct in assuming that you'd want to look into this once you have time?</p>",
        "id": 251352329,
        "sender_full_name": "matklad",
        "timestamp": 1630400155
    },
    {
        "content": "<p>Will this still be useful once salsa can do parallelization internally? Then we could make this a query</p>",
        "id": 251352626,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630400327
    },
    {
        "content": "<p>I think it'll still be useful regardless, yes, as we get eagerness and progress report that way. It's also the case that \"salsa will be parallel soon\"  for at least a year now I think? Maybe more -- I think we discussed that on the call with Niko in Ferrous office :)</p>",
        "id": 251353157,
        "sender_full_name": "matklad",
        "timestamp": 1630400627
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"211727\">@Jonas Schievink  [he/him]</span> did you by any chance started hacking here? My hands are itching to try this out</p>",
        "id": 252008052,
        "sender_full_name": "matklad",
        "timestamp": 1630769378
    },
    {
        "content": "<p>not yet, I might be able to make some time next week, but feel free to take this on if you want to!</p>",
        "id": 252008111,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630769409
    },
    {
        "content": "<p>Will try to hack something together than!</p>",
        "id": 252008258,
        "sender_full_name": "matklad",
        "timestamp": 1630769576
    },
    {
        "content": "<p>Preliminary results:</p>\n<ul>\n<li>it works</li>\n<li>it's not embarrassingly parallel</li>\n<li>it is significant</li>\n</ul>\n<p>For nearcore, I was able to cut the time down from 18 seconds to 10 seconds</p>",
        "id": 252011596,
        "sender_full_name": "matklad",
        "timestamp": 1630773128
    },
    {
        "content": "<p>It seems that we actually pick the worst topological order for this though. For <code>a &lt;- b &lt;- c  d &lt;-e &lt;-f</code>we more or less generate <code>abcdef</code>, while we want <code>adbecf</code></p>",
        "id": 252011764,
        "sender_full_name": "matklad",
        "timestamp": 1630773310
    },
    {
        "content": "<p>Ok, so with better toposort it goes from 18 secs to 8 secs, which is a solid 2x improvement!</p>",
        "id": 252012485,
        "sender_full_name": "matklad",
        "timestamp": 1630774162
    },
    {
        "content": "<p>Interestingly, for rust-analyzer the win is not as much pronounced</p>",
        "id": 252012579,
        "sender_full_name": "matklad",
        "timestamp": 1630774243
    },
    {
        "content": "<p>Does it process a and d in parallel?</p>",
        "id": 252014219,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630776155
    },
    {
        "content": "<p>In the second tobosort, yest, in the first, not really</p>",
        "id": 252014724,
        "sender_full_name": "matklad",
        "timestamp": 1630776725
    },
    {
        "content": "<p>Intersting bit of info: <code>db.import_map</code> is ridiculously fast? <code>7.08ms 68.29µs</code> the first number is crate_def_map, the second is <code>import_map</code>, and that's the general pimture for more or less every crate</p>",
        "id": 252015028,
        "sender_full_name": "matklad",
        "timestamp": 1630777086
    },
    {
        "content": "<p>yeah, import map is a fairly simple shim</p>",
        "id": 252015053,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630777134
    },
    {
        "content": "<p>hmm, but it also builds FSTs, I'd expect that to take a bit longer</p>",
        "id": 252015063,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630777149
    },
    {
        "content": "<p>Well, crate def map we've written ourselves, and fst is by <span class=\"user-mention silent\" data-user-id=\"222471\">BurntSushi</span>  :D</p>",
        "id": 252015116,
        "sender_full_name": "matklad",
        "timestamp": 1630777203
    },
    {
        "content": "<p><a href=\"https://github.com/rust-analyzer/rust-analyzer/pull/10149\">https://github.com/rust-analyzer/rust-analyzer/pull/10149</a></p>",
        "id": 252015146,
        "sender_full_name": "matklad",
        "timestamp": 1630777233
    },
    {
        "content": "<p>is that number for a crate_def_map without any macros?</p>",
        "id": 252015152,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630777246
    },
    {
        "content": "<p>no, it's just <a href=\"https://github.com/rust-analyzer/rust-analyzer/pull/10149/files#diff-862bfe71a267bd484b625a7399105f4d89e5357cbf90e83c612e700d85a5297fR31\">https://github.com/rust-analyzer/rust-analyzer/pull/10149/files#diff-862bfe71a267bd484b625a7399105f4d89e5357cbf90e83c612e700d85a5297fR31</a></p>",
        "id": 252015170,
        "sender_full_name": "matklad",
        "timestamp": 1630777282
    },
    {
        "content": "<p>so as this is one of the larger times, I suspect it has some amount of macros</p>",
        "id": 252015180,
        "sender_full_name": "matklad",
        "timestamp": 1630777301
    },
    {
        "content": "<p>I was trying to profile that yesterday, but I feel like I need a PhD to make sense of it with the existing tooling on linux</p>",
        "id": 252015239,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630777344
    },
    {
        "content": "<p>how many parallel jobs are you using for prime caches?</p>",
        "id": 252015352,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630777466
    },
    {
        "content": "<p>Hmm, can we use the tracing infra to do some cool reporting?</p>",
        "id": 252015785,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630777963
    },
    {
        "content": "<p>8</p>",
        "id": 252015984,
        "sender_full_name": "matklad",
        "timestamp": 1630778172
    },
    {
        "content": "<p>lol yes. I'd love PhD in memory profiling for sure</p>",
        "id": 252016013,
        "sender_full_name": "matklad",
        "timestamp": 1630778221
    },
    {
        "content": "<p>What <em>is</em> infuriating is not profiling per se, but the fact that we don't have a baseline. Like, is 2X speedup more or less the best we could have, or is 20X possible, and we are just doing something stupid?</p>",
        "id": 252016051,
        "sender_full_name": "matklad",
        "timestamp": 1630778280
    },
    {
        "content": "<p>I'm pretty sure you could compute that in this case</p>",
        "id": 252016288,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630778519
    },
    {
        "content": "<p>I'm on my phone, but I'm not sure I get that topo sort</p>",
        "id": 252016331,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630778527
    },
    {
        "content": "<p>In the draft PR? </p>\n<p>the idea is to return the full toposort (sort of equivalence class of toposorts) -- rather than returning a single specific order, <code>Vec&lt;Node&gt;</code>, we compute the list of layers, <code>Vec&lt;Vec&lt;Node&gt;&gt;</code>, where all nodes at layer n have  a critical path on lenght n</p>",
        "id": 252016493,
        "sender_full_name": "matklad",
        "timestamp": 1630778762
    },
    {
        "content": "<p>For a -&gt; b, b -&gt; c, a -&gt; d, does c start before d is ready?</p>",
        "id": 252016494,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630778762
    },
    {
        "content": "<p>yaeh</p>",
        "id": 252016539,
        "sender_full_name": "matklad",
        "timestamp": 1630778782
    },
    {
        "content": "<p>Ah, okay</p>",
        "id": 252016544,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630778788
    },
    {
        "content": "<p>both c and d are leaves, so thel will be in the first layer, and will be picked up by the two first threads</p>",
        "id": 252016558,
        "sender_full_name": "matklad",
        "timestamp": 1630778806
    },
    {
        "content": "<p>I meant the other way around, with c and d being the last</p>",
        "id": 252016651,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630778887
    },
    {
        "content": "<p>But yeah, I believe you</p>",
        "id": 252016686,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630778931
    },
    {
        "content": "<p>Yeah, I am not sure the approach is optimal -- it sounds resonable though, and the impl did bring the speed up in comparison to usual toposort</p>",
        "id": 252016724,
        "sender_full_name": "matklad",
        "timestamp": 1630778989
    },
    {
        "content": "<p>ah, you only parallelize within a layer then?</p>",
        "id": 252016822,
        "sender_full_name": "Jonas Schievink  [he/him]",
        "timestamp": 1630779084
    },
    {
        "content": "<p>I was worried about missed parallelism when the graph is not a neat tree with edges only from one level to the next but rather a DAG</p>",
        "id": 252016919,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630779182
    },
    {
        "content": "<p>I mentioned this API <a href=\"https://docs.python.org/3/library/graphlib.html\">https://docs.python.org/3/library/graphlib.html</a> where you don't get a static order, but a set of runnable tasks</p>",
        "id": 252017134,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630779389
    },
    {
        "content": "<p>After each finished one</p>",
        "id": 252017152,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630779421
    },
    {
        "content": "<p>Yeah, that potentially could be smarter... It's a bit of a pain in the back to track progress over several threads</p>",
        "id": 252017614,
        "sender_full_name": "matklad",
        "timestamp": 1630779942
    },
    {
        "content": "<p>WAIT WAT? that graphlib is standard Python? that's such a different design approach than rust's!</p>",
        "id": 252018090,
        "sender_full_name": "matklad",
        "timestamp": 1630780483
    },
    {
        "content": "<p>Well, it's only the topo sort, I think</p>",
        "id": 252018115,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630780519
    },
    {
        "content": "<p>but it's not just toposort, it's the whole paralel scheduler thing! Like, its cool, but weird, but cool!</p>",
        "id": 252018196,
        "sender_full_name": "matklad",
        "timestamp": 1630780620
    },
    {
        "content": "<p>But yeah, it's like they say. It has everything in the standard library, though the APIs are often terrible</p>",
        "id": 252018200,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630780624
    },
    {
        "content": "<p>I sort of thought that all crazy things there are for historical reasons, but TopoSorter is legit a new crazyness</p>",
        "id": 252018225,
        "sender_full_name": "matklad",
        "timestamp": 1630780670
    },
    {
        "content": "<p>Modern Python's wild</p>",
        "id": 252018234,
        "sender_full_name": "matklad",
        "timestamp": 1630780679
    },
    {
        "content": "<p>At least this one seems well-designed. I actually looked for a topo sort implementation like this in the Rust ecosystem and couldn't find any. Not that it's hard to do, but..</p>",
        "id": 252018301,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630780728
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"133169\">matklad</span> <a href=\"#narrow/stream/185405-t-compiler.2Frust-analyzer/topic/simple.20parallel.20cache.20priming/near/252018196\">said</a>:</p>\n<blockquote>\n<p>but it's not just toposort, it's the whole paralel scheduler thing! Like, its cool, but weird, but cool!</p>\n</blockquote>\n<p>It's just a toposort if you implement it the right way. I learned to write it using a list of successors and predecessor count per node, and the scheduling part comes naturally out of that.</p>",
        "id": 252018621,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630781082
    },
    {
        "content": "<p>Is there a short code for algo somewhere? I love learning about the <em>right</em> way to code algo</p>",
        "id": 252018669,
        "sender_full_name": "matklad",
        "timestamp": 1630781146
    },
    {
        "content": "<p>For this one? It's easy, you keep an adjacency list (direct successors, is that what it's called?) and when a task is ready you go through its neighbours and decrease their predecessor count. If it's 0, the neighbour is runnable.</p>",
        "id": 252018955,
        "sender_full_name": "Laurențiu",
        "timestamp": 1630781502
    },
    {
        "content": "<p>I think bevy_ecs has something similar tucked away somewhere.</p>",
        "id": 252023023,
        "sender_full_name": "Daniel Mcnab",
        "timestamp": 1630786061
    },
    {
        "content": "<p>so, i picked this PR up out of boredom &amp; implemented the more ideal topological sort and allowed it to use 32 threads. it's still VERY rough code-wise, just hacked it together, but... it goes from taking ~60 seconds to ~20 (on my rather large work workspace, ~3900 crates). With like 98% of the crates completing in the first 4-8 seconds...</p>",
        "id": 259483666,
        "sender_full_name": "jhgg",
        "timestamp": 1635484387
    },
    {
        "content": "<p>Was it 60s with the draft PR or with <code>master</code>? And what do you mean by the \"more ideal\" sort?</p>",
        "id": 259484627,
        "sender_full_name": "Laurențiu",
        "timestamp": 1635485628
    },
    {
        "content": "<p>No sorry, 60s with master! And the \"more ideal sort\" being the same algorithm that the python lib mentioned earlier uses. </p>\n<p>Essentially I just have a controller thread that receives events from the worker threads, and queues more work as other work finishes. by using 2 crossbeam channels, a work channel and a result channel. So initially it enqueues as much work as it can (crates that have no dependencies). As those complete, the controller thread is notified over the result . When a work item completes, it can unlock more work to be done, and it submits the \"ready\" work to the queue until there is no more work left to do, and then the indexing is complete.</p>\n<p>One optimization I could make is to prioritize work (insert work into the queue first that would unlock the most work if it completes.)</p>",
        "id": 259493777,
        "sender_full_name": "jhgg",
        "timestamp": 1635494715
    },
    {
        "content": "<p>Oh, that sounds great. Would you mind also trying the draft PR? I'm curious how much of a difference it makes.</p>",
        "id": 259494578,
        "sender_full_name": "Laurențiu",
        "timestamp": 1635495314
    },
    {
        "content": "<p>Takes a little longer ~5 secs longer, but since progress reporting doesn't work I have no idea if it's progressing on the initial work faster or slower</p>",
        "id": 259495571,
        "sender_full_name": "jhgg",
        "timestamp": 1635496054
    },
    {
        "content": "<p>Exciting! I would probably try to avoid spawning an extra concurrent thread if possible. Also would love to see this pushed over the finished line!</p>",
        "id": 259508052,
        "sender_full_name": "matklad",
        "timestamp": 1635504599
    },
    {
        "content": "<p>So one of my concerns with this is that we should only prime the caches for the workspace crates and probably their direct dependencies. Otherwise we're including in the analysis private dependencies, which seems wasteful. In practice it didn't seem to use a lot of extra memory, though.</p>",
        "id": 263181812,
        "sender_full_name": "Laurențiu",
        "timestamp": 1638294460
    },
    {
        "content": "<p>I actually disable cache priming now, not because it's slow (that doesn't matter), but because it tends to spin up the computer fans. And in Code, the syntax highlighting and inlay hints tend to have the same effect anyway.</p>",
        "id": 263182076,
        "sender_full_name": "Laurențiu",
        "timestamp": 1638294571
    },
    {
        "content": "<p>It probably still worth doing the topo sort and parallel thing, but only on the direct dependencies</p>",
        "id": 263197549,
        "sender_full_name": "Laurențiu",
        "timestamp": 1638301270
    },
    {
        "content": "<p>that's what <a href=\"https://github.com/rust-analyzer/rust-analyzer/pull/10743\">https://github.com/rust-analyzer/rust-analyzer/pull/10743</a> aimed to do, right? if so, my plan is to still only cache those subset of crates</p>",
        "id": 263240147,
        "sender_full_name": "jhgg",
        "timestamp": 1638329187
    },
    {
        "content": "<p>No, that one only excluded the examples and other targets:</p>\n<blockquote>\n<p>This PR instead makes us index only the transitive dependencies of all workspace crates.</p>\n</blockquote>",
        "id": 263254756,
        "sender_full_name": "Laurențiu",
        "timestamp": 1638345172
    },
    {
        "content": "<p>We still don't want the transitive dependencies</p>",
        "id": 263254772,
        "sender_full_name": "Laurențiu",
        "timestamp": 1638345184
    },
    {
        "content": "<p>ah yeah i see. so in this case, we'd want to resolve all local roots and then everything those local roots depend on?</p>",
        "id": 263261217,
        "sender_full_name": "jhgg",
        "timestamp": 1638349649
    },
    {
        "content": "<p>Yeah, that's what I was thinking</p>",
        "id": 263272907,
        "sender_full_name": "Laurențiu",
        "timestamp": 1638356182
    },
    {
        "content": "<p><a href=\"https://github.com/rust-analyzer/rust-analyzer/pull/11281\">https://github.com/rust-analyzer/rust-analyzer/pull/11281</a> !!</p>",
        "id": 267988504,
        "sender_full_name": "jhgg",
        "timestamp": 1642151725
    },
    {
        "content": "<p>It still touches 107 crates on RA itself, right?</p>",
        "id": 267988939,
        "sender_full_name": "Laurențiu",
        "timestamp": 1642152029
    },
    {
        "content": "<p>108 now since i guess i added one, but yes.</p>",
        "id": 267989154,
        "sender_full_name": "jhgg",
        "timestamp": 1642152173
    },
    {
        "content": "<p>8909ms 8823ms 8853ms before<br>\n5262ms 5222ms 5254ms after</p>",
        "id": 267989991,
        "sender_full_name": "Laurențiu",
        "timestamp": 1642152668
    },
    {
        "content": "<p>what cmd are u running to compute that</p>",
        "id": 267990112,
        "sender_full_name": "jhgg",
        "timestamp": 1642152734
    },
    {
        "content": "<p><code>\"RA_PROFILE\": \"*&gt;4000\"</code></p>",
        "id": 267990586,
        "sender_full_name": "Laurențiu",
        "timestamp": 1642153048
    },
    {
        "content": "<p>And <code>Developer: Reload Window</code></p>",
        "id": 267990621,
        "sender_full_name": "Laurențiu",
        "timestamp": 1642153063
    },
    {
        "content": "<p>I think we can remove the old version and use the number of CPUs</p>",
        "id": 267990874,
        "sender_full_name": "Laurențiu",
        "timestamp": 1642153225
    },
    {
        "content": "<p>on our workspace at work, which has many more crates...</p>\n<p>38953ms 38799ms ... before<br>\n15280ms 14077ms 13766ms after</p>",
        "id": 267991070,
        "sender_full_name": "jhgg",
        "timestamp": 1642153354
    },
    {
        "content": "<p>not bad at all...!</p>",
        "id": 267991102,
        "sender_full_name": "jhgg",
        "timestamp": 1642153375
    },
    {
        "content": "<p>I have a 16C32T</p>",
        "id": 267991520,
        "sender_full_name": "Laurențiu",
        "timestamp": 1642153595
    },
    {
        "content": "<p>But I generally keep cache priming disabled, it's not that noticeable.</p>",
        "id": 267991799,
        "sender_full_name": "Laurențiu",
        "timestamp": 1642153793
    }
]