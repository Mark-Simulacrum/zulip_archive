[
    {
        "content": "<p>It is quite hard to generate mbe benchmark tests which is  self contain: it means we cannot use any name resolution facility. I can find all <code>ast::macro_rules</code> and <code>ast::macro_call</code> inside RA project, but that's not that much (around 2k calls). <br>\nThe total macro invocations in <code>rust-analyzer analysis-stats .</code> is around 60k which include std and all dependencies, and recursive calls.</p>\n<p>If we want to benchmark the real world sitution, maybe only 2k calls are not enough IMO. On the other hand, that 2k call data size is around 100kb, we maybe can just keep it inside git, right?</p>\n<p>I feel stuck in both cases, <span class=\"user-mention\" data-user-id=\"133169\">@matklad</span>  what do you think about it ?</p>",
        "id": 225429489,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1612644473
    },
    {
        "content": "<blockquote>\n<p>If we want to benchmark the real world sitution, maybe only 2k calls are not enough IMO. On the other hand, that 2k call data size is around 100kb, we maybe can just keep it inside git, right?</p>\n</blockquote>\n<p>How long does it take to run for those?</p>",
        "id": 225455959,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612692518
    },
    {
        "content": "<p>Worst case, we can store the fixture in another repo, ignore the test, and clone it on-demand (on CI?). But I'm not sure it's worth the hassle?</p>",
        "id": 225455985,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612692577
    },
    {
        "content": "<blockquote>\n<p>How long does it take to run for those?</p>\n</blockquote>\n<p>For 60k one, it is around 15s in my low spec machine. I haven't testetd the 2k one yet, but I guess it is proportional to the 60k one.</p>",
        "id": 225460284,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1612698484
    },
    {
        "content": "<blockquote>\n<p>Worst case, we can store the fixture in another repo, ignore the test, and clone it on-demand (on CI?). But I'm not sure it's worth the hassle?</p>\n</blockquote>\n<p>Another workaround is adding new argument flags (e.g. dump-macro-rules, dump-macro-call, etc) in <code>rust-analyzer</code> cli and generate these test on CI. But I don't know whether it's worth or not.</p>",
        "id": 225460620,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1612698993
    },
    {
        "content": "<p>But I can imagine we can reuse these infrastructure to benchmark different part of RA , right ?</p>",
        "id": 225460638,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1612699048
    },
    {
        "content": "<p>Okay, I think I have an idea : We could just use a pre-generated macro-rules (which is not that big, around 640kb)  and generated macro invocations based on these rules. I will try to implement it this week to see how it is.</p>",
        "id": 225665028,
        "sender_full_name": "Edwin Cheng",
        "timestamp": 1612864239
    },
    {
        "content": "<p>I don't think we necessary should target \"real world\". I think it's better to have generated synthetic benchmarks. That way, we can easily change the generator to exercise particularly problematic spots</p>",
        "id": 225694093,
        "sender_full_name": "matklad",
        "timestamp": 1612879632
    },
    {
        "content": "<p>Let me maybe do this for accidentally quadratic benchmark, just to get the infra in place</p>",
        "id": 225694239,
        "sender_full_name": "matklad",
        "timestamp": 1612879697
    }
]