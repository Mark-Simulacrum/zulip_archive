[
    {
        "content": "<p>Hi</p>\n<p>DHAT shows the Rowan trees to be the most memory-hungry while running analysis-stats. AFAICT Rowan nodes are only cached for a file, but this cache is not shared across files. Is this correct?</p>\n<p>I would like to investigate ways to reduce Rowan memory usage. I have some ideas I would like to try, but unsure how to do it.</p>\n<ul>\n<li><em>Reuse the same NodeCache across many files.</em> Where would be the best place to put this cache?</li>\n<li><em>Allocate Rowan nodes in an arena (https://github.com/rust-analyzer/rowan/issues/15).</em> Does this mean not using thin-dst? It seems to be thin-dst that does the Box allocation. </li>\n</ul>\n<p>Any pointers? Ideas?</p>",
        "id": 197823879,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1589678560
    },
    {
        "content": "<p>Yeah, sharing node cache should be an easy win!</p>\n<p>The simplest thing to do would be to add a read-only cache with pre-interned Node, and than use it for all files</p>",
        "id": 197837818,
        "sender_full_name": "matklad",
        "timestamp": 1589704490
    },
    {
        "content": "<p>More complicated thing would be something like a thread-local cache, but I am afraid it can grow unbounded, so it also needs some kind of LRU...</p>",
        "id": 197837827,
        "sender_full_name": "matklad",
        "timestamp": 1589704551
    },
    {
        "content": "<p>Regarding sharing node cache. I see Roslyn simply makes SyntaxNodeCache be a static class with a bound of 65536 elements. <a href=\"https://github.com/dotnet/roslyn/blob/f03f87fbb636dfba824947c0e19e779ee9a39626/src/Compilers/Core/Portable/Syntax/InternalSyntax/SyntaxNodeCache.cs#L113\">https://github.com/dotnet/roslyn/blob/f03f87fbb636dfba824947c0e19e779ee9a39626/src/Compilers/Core/Portable/Syntax/InternalSyntax/SyntaxNodeCache.cs#L113</a></p>\n<p>I'll experiment with instantiating NodeCache thread locally statically. Let's try for now without any bounds</p>",
        "id": 197849315,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1589722288
    },
    {
        "content": "<p>Yeah, actually making it just a global should also work. The only thing I am worrying about is that this might make parsing slower, as you'd need synchronisation for looking up nodes</p>",
        "id": 197850183,
        "sender_full_name": "matklad",
        "timestamp": 1589723442
    },
    {
        "content": "<p>Yeah, I suspect the synchronization cost will be high (haven't benchmarked though). How parallel is rust analyzer currently? Analysis-stats seems serial AFAICT</p>",
        "id": 197851992,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1589725880
    },
    {
        "content": "<p>The only really parallel bit is this one: <a href=\"https://github.com/rust-analyzer/rust-analyzer/blob/71e94b1d0bf7c58aa377513f010fcb3f56081f5f/crates/ra_ide_db/src/symbol_index.rs#L113\">https://github.com/rust-analyzer/rust-analyzer/blob/71e94b1d0bf7c58aa377513f010fcb3f56081f5f/crates/ra_ide_db/src/symbol_index.rs#L113</a></p>",
        "id": 197852814,
        "sender_full_name": "matklad",
        "timestamp": 1589727218
    },
    {
        "content": "<p>It'd be good to expose it as <code>analysis-stats</code>/ <code>analysis-bench</code>, as it's a good measure of how fast we can parse files in parallel</p>",
        "id": 197852861,
        "sender_full_name": "matklad",
        "timestamp": 1589727264
    },
    {
        "content": "<p>I haven't looked at the rowan PR yet (hopefully do tat on monday), but I think it makes sense to remove the cache from the public API completely, and just use a global singleton, provided that the synchronization cost is not too bad</p>",
        "id": 197852908,
        "sender_full_name": "matklad",
        "timestamp": 1589727333
    },
    {
        "content": "<p>(now I'm wondering about trying to build a lru cache on a dashmap (sharded hashmap) instead of a plain hashmap and seeing how that behaves)</p>",
        "id": 197963267,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589820614
    },
    {
        "content": "<p>If this ends up positive, I'll at least need to consider what the global cache would look like for sorbus</p>",
        "id": 197963382,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589820690
    },
    {
        "content": "<p>One tricky thing I'm doing in sorbus right now is deduplicating based on subtree identity rather than subtree contents</p>",
        "id": 197963506,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589820745
    },
    {
        "content": "<p>This however becomes problematic (in terms of ideal deduplication) if small nodes exit the cache and thus any new parents of them stop getting deduplicated</p>",
        "id": 197963672,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589820820
    },
    {
        "content": "<p>There's still the comment in Rowan's cache to avoid re-hashing subtrees. That's what the identity-based cache in sorbus is doing</p>",
        "id": 197964110,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589821043
    },
    {
        "content": "<p>Plus \"how many direct children does this node have\" is a terrible heuristic for \"how deep is this node\"</p>",
        "id": 197964260,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589821097
    },
    {
        "content": "<p>Where the second is the more interesting one for caching</p>",
        "id": 197964276,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589821108
    },
    {
        "content": "<p>How deep a node is a decent heuristic for how likely it is to have the same node duplicated elsewhere in the compilation unit</p>",
        "id": 197964474,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589821198
    },
    {
        "content": "<p>(as the deeper a tree gets the more chances it has to diverge)</p>",
        "id": 197964627,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589821259
    },
    {
        "content": "<p>Ok, I thought of an interesting way to bound the cache compute cost while being resilient to some cache evictions</p>",
        "id": 197965212,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589821520
    },
    {
        "content": "<p>Basically, do the identity equivalence some number of levels down, rather than initially</p>",
        "id": 197965301,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589821555
    },
    {
        "content": "<p>The question, though, is if that even helps in any common cases</p>",
        "id": 197965370,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589821567
    },
    {
        "content": "<p>Seeing as if a shallow node is evicted from the cache, its ancestors are not unlikely to also be on the way out</p>",
        "id": 197965505,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589821634
    },
    {
        "content": "<p>I haven't looked into how sorbus does identity of a tree, so correct me where I am wrong in the following. To avoid hashing the complete subtree of a node(all descendants of a node), would it be enough to hash the kind + text size + child1 ptr... ChildN ptr?</p>",
        "id": 198045834,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1589883625
    },
    {
        "content": "<p>Actually if this child pointers are enough to give identity to a node, then text size is unnecessary as that information is already contained in the children</p>",
        "id": 198084886,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1589902799
    },
    {
        "content": "<p>Here it is in the code: <a href=\"https://github.com/CAD97/sorbus/blob/2b5fcd42808fb99f41170f575bddea9e0f873878/src/green/builder.rs#L30\">https://github.com/CAD97/sorbus/blob/2b5fcd42808fb99f41170f575bddea9e0f873878/src/green/builder.rs#L30</a></p>",
        "id": 198098548,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589908571
    },
    {
        "content": "<p>But yes, for the builder cache I'm hashing the node as the rough equivalent of <code>hash(kind), hash(text_len), for child in children ptr::hash(Arc::as_ptr(child))</code></p>",
        "id": 198098719,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589908665
    },
    {
        "content": "<p>It would be possible to skip hashing the text length for a small decrease in work, though, yes</p>",
        "id": 198098903,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589908739
    },
    {
        "content": "<p>(this is only for the builder; regular <code>PartialEq</code>/<code>Hash</code> does full tree equivalence)</p>",
        "id": 198099098,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589908808
    },
    {
        "content": "<p>AFAICT sorbus doesn’t have the condition to only cache nodes with &lt;= 3 children. How come? I would think would help only caching nodes with a high chance of being seen again</p>",
        "id": 198107383,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1589912705
    },
    {
        "content": "<p>The reason is that (so far) I don't think that's a good heuristic for what we actually want</p>",
        "id": 198225939,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589993343
    },
    {
        "content": "<p>Which would be the number of (transitive) children(?)</p>",
        "id": 198225976,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589993365
    },
    {
        "content": "<p>What we want is a decent heuristic for whether the node is likely to have duplicates that we can dedupe</p>",
        "id": 198226018,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589993391
    },
    {
        "content": "<p>It was important for Rowan to use some heuristic here because it recursively rehashed the entire subtree</p>",
        "id": 198226190,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589993459
    },
    {
        "content": "<p>Whereas sorbus from the start didn't do that, instead using child identity for deduping</p>",
        "id": 198226239,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589993489
    },
    {
        "content": "<p>It's important to note that Rowan's \"3 or fewer direct children\" would still rehash subtrees even if they had more than 3 direct children</p>",
        "id": 198226342,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589993535
    },
    {
        "content": "<p>So, abusing big O, Rowan's cache was roughly O(n) and sorbus's was O(log n)</p>",
        "id": 198226406,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589993575
    },
    {
        "content": "<p>(where N is the number of nodes in the tree, and log n is asymptotically the number of direct children of any one node)</p>",
        "id": 198226470,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589993617
    },
    {
        "content": "<p>Rowan's just had a gate on the O(n) that said you had to have 3 or fewer direct children to be part of that n</p>",
        "id": 198226695,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589993708
    },
    {
        "content": "<p>Basically, the fact that I no longer had to revisit subtrees to do the cache let me just always do node creation through the cache. But with a strong argument that some O(1) heuristic is a good approximation for \"does this node have duplicates\", I'd be perfectly happy to re-add that heuristic to sorbus</p>",
        "id": 198226983,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589993848
    },
    {
        "content": "<p>I'd have to double-check how exactly r-a parses them, but my gut says that some nodes that could definitely be cached end up not being cached because of this heuristic, like <code>#[rustfmt::skip]</code></p>",
        "id": 198227494,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589994087
    },
    {
        "content": "<p>If anything, I think textual length is probably a better heuristic than number of direct children</p>",
        "id": 198227693,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589994156
    },
    {
        "content": "<p>But also one that isn't available before iterating the children to sum their length, so that'd be an O(children) minimum heuristic</p>",
        "id": 198227836,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589994228
    },
    {
        "content": "<p>Plus <span class=\"user-mention\" data-user-id=\"139182\">@Simon Vandel Sillesen</span>, there's another big hole I just realized in caching based on identity while also caching based on direct children length:</p>\n<p>Your patch will now decrease the number of cached nodes, because nodes now can only be cached if their children were</p>",
        "id": 198228412,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589994464
    },
    {
        "content": "<p>Which actually was a key reason in sorbus caching everything now that I think about it</p>",
        "id": 198228485,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1589994489
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"132829\">@Christopher Durham</span> I have a few clarifying questions. </p>\n<p>I read <a href=\"https://github.com/rust-analyzer/rowan/blob/40fe55c450c88f4da7cf315bf027bec3250a7352/src/green/builder.rs#L29\">https://github.com/rust-analyzer/rowan/blob/40fe55c450c88f4da7cf315bf027bec3250a7352/src/green/builder.rs#L29</a> as Rowan only hashing if the direct children are &lt;= 3. What am I missing?</p>\n<p>Regarding caching based on identity with children count heuristic. I feel like I’m missing something obvious - my mental model is not yet accurate of parsing and trees. Why can nodes only be cached if the children were? I think an example would help me greatly.</p>\n<p>Finding a nice heuristic will however help reducing the number of hashes. I’ll benchmark no limitations on cache next week when I get to a computer.</p>",
        "id": 198265122,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590013522
    },
    {
        "content": "<p>So if I have node A 2+2 that is a node with 3 children (all tokens). In another branch of the tree i now want to build the node B 2+2 node. This second node B would have the same identity hash as the first node A, right? If token + of node A went out of cache, node A would still be in the cache .... ah okay, I think I realize ... so when constructing the token + for node B it will not be found in the cache and will therefore get a different ptr. So the identity of node A and B will not be the same</p>",
        "id": 198266628,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590014613
    },
    {
        "content": "<p>Idea to find heuristic: setup caching based on identity with no limitations. Instrument the hashmap access with cache miss/hit and eprtintln the miss/hit along with kind, Len of node and a print of each child. It would be interesting to see which of these, if any, are a good predictor of a cache hit. Run this data collection on representative code</p>",
        "id": 198267670,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590015393
    },
    {
        "content": "<p>A good predictor of a cache miss would be equally useful as those can be used to avoid hashing</p>",
        "id": 198267775,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590015477
    },
    {
        "content": "<p>Actually it should be caching based on subtree hashing. Else we get misleading information on cachability of a node</p>",
        "id": 198272974,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590019879
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"139182\">@Simon Vandel Sillesen</span> (sorry for slow replies, I'm trying to do too many things at the same time right now <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span> )</p>",
        "id": 198480951,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590173805
    },
    {
        "content": "<p>First off, something I just realized:</p>",
        "id": 198480966,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590173817
    },
    {
        "content": "<blockquote>\n<p>For example, all <code>#[inline]</code> in this file share the same green node!</p>\n</blockquote>",
        "id": 198481019,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590173842
    },
    {
        "content": "<p>This is actually not true if we gate caching to nodes that have &lt;= 3 direct children</p>",
        "id": 198481058,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590173864
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>    ATTR@25..34\n      POUND@25..26 &quot;#&quot;\n      L_BRACK@26..27 &quot;[&quot;\n      PATH@27..33\n        PATH_SEGMENT@27..33\n          NAME_REF@27..33\n            IDENT@27..33 &quot;inline&quot;\n      R_BRACK@33..34 &quot;]&quot;\n</code></pre></div>",
        "id": 198481087,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590173879
    },
    {
        "content": "<p>As <code>#[inline]</code> is a node with four direct children</p>",
        "id": 198481180,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590173909
    },
    {
        "content": "<p>If you do something like sorbus's cache where _every_ node is built with the cache and persisted in the cache, you know you have perfect deduplication and can still get perfect deduplication via identity</p>",
        "id": 198481489,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590174068
    },
    {
        "content": "<p>I just keep having more and more cursed ideas for how to optimize this.</p>\n<p>In sorbus, we're storing the node's children's textual offset from the parent in the children array, so that going from <code>&amp;green::Node, (kind, offset)</code> to <code>&amp;green::Node</code> is ~O(log n) rather than O(n)</p>",
        "id": 198844944,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590547693
    },
    {
        "content": "<p>The first node is obviously at offset 0, though, so that's a wasted u32!</p>",
        "id": 198844957,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590547725
    },
    {
        "content": "<p>So we <em>could</em> \"just\" stick a depth counter there and have our O(1) depth check</p>",
        "id": 198845024,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590547814
    },
    {
        "content": "<p>(Should we? Probably not, it'd add a very annoying edge case to child iteration.)</p>",
        "id": 198845040,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590547847
    },
    {
        "content": "<p>If we care a little less about performance of a cache and a little more about proper caching while retaining a bounded size, then <a href=\"https://medium.com/@bparli/enhancing-least-frequently-used-caches-with-dynamic-aging-64dc973d5857#9457\">LFUDA or GDS</a> caches sound enticing, especially for a shared cache</p>",
        "id": 198845537,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590548617
    },
    {
        "content": "<p>Other than stealing the first child's offset as space for storing depth, we could also just add another <code>u32</code> to <code>Node</code> and flip the parity of the children array</p>",
        "id": 198845746,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590548919
    },
    {
        "content": "<p>Shrinking node size would be a way to reduce the work that needs to be done on memory allocation. However a 4 byte decrease like your example might actually not be that significant. I guess it will depend on the memory allocator, and its bucket sizes.</p>\n<p>An orthogonal, and I think it might be more fruitful, idea would be to add arena allocation of nodes. If er can add an api to basically batch allocate the complete tree, that would work well for <a href=\"http://crates.io\">crates.io</a> dependencies that don't change.</p>",
        "id": 198900706,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590590403
    },
    {
        "content": "<p>I'm currently working on a patch for rust-analyzer to reduce the amount of events allocated in the parser. I don't have any numbers yet, but i believe it should be helpful in reducing memory usage of parsing  <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 198900966,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590590512
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"132829\">@Christopher Durham</span> i just read the article on dynamics cache aging. It seems like something that could be useful in case of a global cache. A good cache policy could definitely improve performance a bunch</p>",
        "id": 198903355,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590591340
    },
    {
        "content": "<p>Hey, it seems like I am not able to follow this work to closely. I hope to catch up with it once things calm down (hehe). If you need something from me to unblock experimentation (publishing crates or what not), feel free to agressively ping :)</p>",
        "id": 198903776,
        "sender_full_name": "matklad",
        "timestamp": 1590591472
    },
    {
        "content": "<p>No blockings on my side <span aria-label=\"slight smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"slight smile\">:slight_smile:</span></p>",
        "id": 198906455,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590592388
    },
    {
        "content": "<p>I am working on publishing crates. Chalk should start publishing soon and then I can move on to rust-analyzer</p>",
        "id": 198914545,
        "sender_full_name": "pksunkara",
        "timestamp": 1590595825
    },
    {
        "content": "<p>(I'm just dumping this cursed idea here so it's recorded, but I doubt it's actually a good idea:<br>\nIt's theoretically possible to have both thread-local shared builder caches and sharing between threads. \"Just\" have a cache miss look at the other thread's caches before making a new node and inserting it to the local cache.)</p>",
        "id": 198969211,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590624431
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"139182\">@Simon Vandel Sillesen</span> </p>\n<blockquote>\n<p>arena allocation</p>\n</blockquote>\n<p>Out of curiousity, do you know if sorbus's FAM-based nodes are inherently problematic for an arena?</p>",
        "id": 198969462,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590624616
    },
    {
        "content": "<p>From a theoretical point of view, arena-allocating nodes, at least for dependencies, makes sense</p>",
        "id": 198969511,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590624660
    },
    {
        "content": "<p>From a convenience point of view, owning trees makes everything _so much easier_.</p>",
        "id": 198969528,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590624679
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"132829\">@Christopher Durham</span> i believe roslyn uses this concept of a L1 and L2 cache system. Probably a good thing to write on that list of things to experiment with</p>",
        "id": 198994262,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590653987
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"132829\">@Christopher Durham</span> does Fam-based nodes mean nodes in a family tree? As in parents point to children, but not the opposite? I don't know on the top of my head, will need to research or think some time</p>",
        "id": 198994421,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590654119
    },
    {
        "content": "<p>I think the api and ownership semantics would need to be different for arena-trees. As in ownership is only possible at the root level of a tree (the complete syntax tree)</p>",
        "id": 198994673,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590654267
    },
    {
        "content": "<p>FAM = Flexible Array Member</p>",
        "id": 199048703,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590684727
    },
    {
        "content": "<p>(wait, I did actually get Rowan using an older version of my FAM infra)</p>",
        "id": 199048910,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590684842
    },
    {
        "content": "<p>Basically,<br>\nWithout FAM <code>Node = { kind, len, childs: Vec&lt;Child&gt; }</code><br>\nWith FAM <code>Node = { kind, len, childs: [Child] }</code></p>",
        "id": 199049030,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590684904
    },
    {
        "content": "<p>It means <code>Node</code> (well, the heap part) is dynamically sized. This means a simple typed arena doesn't work.</p>",
        "id": 199049201,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590684993
    },
    {
        "content": "<p>By the way, <code>AsRef&lt;[T]&gt;</code> for <code>vec::Drain</code> and <code>vec::IntoIter</code> (the API required for no-alloc cache hits) are FCP-merge. Once they're in nightly, I can make a PR to Rowan to use them; I already have <a href=\"https://github.com/CAD97/sorbus/pull/40\">a proof-of-concept patch for sorbus</a> (as well as a slightly less POC draft locally).</p>",
        "id": 199055949,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590687859
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"132829\">@Christopher Durham</span> </p>\n<blockquote>\n<p>no-alloc cache hits </p>\n</blockquote>\n<p>sounds good! Unfortunately rust analyzer is stable only, right? </p>\n<blockquote>\n<p>FAM</p>\n</blockquote>\n<p>Ah okay, I see. So Node is not Sized, right?<br>\nIn any case, we could have different sized bins ala <a href=\"https://citybound.github.io/citybound/chunky/struct.MultiArena.html\">https://citybound.github.io/citybound/chunky/struct.MultiArena.html</a></p>",
        "id": 199062665,
        "sender_full_name": "Simon Vandel Sillesen",
        "timestamp": 1590690545
    },
    {
        "content": "<p>Yeah, r-a compiles on stable and it'd be nice to keep it that way. The API in question is be stabilized by the PRs, though, so it'll probably be available in 1.46 (1.45 if it sneaks in before the beta cutoff next week)</p>",
        "id": 199091973,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590703409
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/pull/72584\">https://github.com/rust-lang/rust/pull/72584</a>, <a href=\"https://github.com/rust-lang/rust/pull/72584\">https://github.com/rust-lang/rust/pull/72584</a></p>",
        "id": 199091999,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590703436
    },
    {
        "content": "<p><code>vec::Drain</code> is the more important one, as that's how the <code>TreeBuilder</code> works.</p>",
        "id": 199092094,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1590703485
    },
    {
        "content": "<p>The necessary API for no-alloc cache hits are riding the trains to stable</p>",
        "id": 201093986,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592358349
    },
    {
        "content": "<p>I've done the requisite patch in sorbus: <a href=\"https://github.com/CAD97/sorbus/pull/45\">https://github.com/CAD97/sorbus/pull/45</a></p>",
        "id": 201093996,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592358362
    },
    {
        "content": "<p>If nobody has done the same for rowan by the time 1.46 hits stable I'll do the same for rowan</p>",
        "id": 201094051,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592358409
    },
    {
        "content": "<p>The <code>weak_as_ptr</code> feature is also stabilizing in 1.46</p>",
        "id": 201094069,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592358452
    },
    {
        "content": "<p>And that may potentially have an answer for the cache growth!</p>",
        "id": 201094086,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592358473
    },
    {
        "content": "<p>\"Just\" store weak pointers in the cache</p>",
        "id": 201094225,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592358625
    },
    {
        "content": "<p>And expose a <code>.gc(&amp;mut self)</code> to walk the cache and clear out any dangling pointers</p>",
        "id": 201094230,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592358648
    },
    {
        "content": "<p>Alternatively: be <del>even more</del> less clever, and expose the <code>.gc(&amp;mut self)</code> that just purges any cached element with <code>strong_count() == 1</code></p>",
        "id": 201094314,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592358807
    },
    {
        "content": "<p><a href=\"https://github.com/CAD97/sorbus/pull/46\">https://github.com/CAD97/sorbus/pull/46</a> is an implementation in sorbus of a cache GC.</p>",
        "id": 201179377,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592417372
    },
    {
        "content": "<p>upstream hashbrown prereqs merged, so we have GC for the sorbus caches exposed now.<br>\nI went to port this to rowan only to be foiled by myself; <code>thin_dst::ThinArc</code> doesn't expose any way to get at <code>Arc::strong_count</code>.</p>",
        "id": 201313254,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592507649
    },
    {
        "content": "<p>I guess I need to look into actually updating rowan from using <code>thin_dst</code> to <code>slice_dst</code> and <code>erasable</code> then...</p>",
        "id": 201313290,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592507676
    },
    {
        "content": "<p>Simple cache gc support for rowan: <a href=\"https://github.com/rust-analyzer/rowan/pull/63\">https://github.com/rust-analyzer/rowan/pull/63</a></p>",
        "id": 201346166,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592532884
    },
    {
        "content": "<p><a href=\"https://github.com/CAD97/sorbus/pull/53\">I'm doing evil things to hash tables</a></p>",
        "id": 201517815,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592717010
    },
    {
        "content": "<p>The current cache GC is roughly O(m log n)</p>",
        "id": 201517826,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592717023
    },
    {
        "content": "<p>That's why sorbus exposes a O(m) \"GC turn\" that only collects roughly one and a half \"layers\" of a freeable tree</p>",
        "id": 201517869,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592717057
    },
    {
        "content": "<p>But this is a way of getting a complete collection in just one pass</p>",
        "id": 201518000,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592717088
    },
    {
        "content": "<p>(m is the number of cached elements, n is the number of freeable elements)</p>",
        "id": 201518050,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592717180
    },
    {
        "content": "<p>(which isn't quite O(m), but should be O(m+n) (which... is O(m) because asymptotes and n⊂m))</p>",
        "id": 201518059,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592717215
    },
    {
        "content": "<p>But the evil part is it works via concurrent modification and iteration of the hash table</p>",
        "id": 201518065,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592717239
    },
    {
        "content": "<p>I think I found a way for it to actually be sound</p>",
        "id": 201518069,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592717278
    },
    {
        "content": "<p>But I'm <a href=\"https://github.com/rust-lang/hashbrown/issues/165\">asking hashbrown how abusive I'm allowed to be to their hash tables</a> to be safe.</p>",
        "id": 201518115,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1592717311
    },
    {
        "content": "<p>I am warming up to Roslyn/libsyntax idea of storing trivia in tokens more!</p>\n<p>The idea is that, instead of storing comments, white space and skipped tokens as nodes, we store them as leading/trailing fields of tokens.</p>\n<p>On the first sight, this seems like a royally bad idea, because <strong>most</strong> of the tree are tokens, and we are growing each token by two pointers.</p>\n<p>However, I think we’ll actually save on memory here. Most of the tokens are interned (even if we take trivia into account), so +two pointers is not such a big deal. OTOH, we now store twice as few pointers in internal nodes, which would seem like a bigger win.</p>",
        "id": 206986269,
        "sender_full_name": "matklad",
        "timestamp": 1597441956
    },
    {
        "content": "<p>Hmm</p>",
        "id": 206992224,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1597446781
    },
    {
        "content": "<p>That definitely does seem plausible</p>",
        "id": 206992231,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1597446792
    },
    {
        "content": "<p>The number of unique tokens would go up, ofc</p>",
        "id": 206992246,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1597446834
    },
    {
        "content": "<p>(as not every token with the same text would store the same lead/tail trivia)</p>",
        "id": 206992304,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1597446870
    },
    {
        "content": "<p>But it does seem like it could reduce the number of pointers to any given trivia node</p>",
        "id": 206992354,
        "sender_full_name": "Christopher Durham",
        "timestamp": 1597446956
    },
    {
        "content": "<p>It seems to me that before making any decisions regarding changes, we should develop a benchmark. Because maybe we will win some memory but lose execution speed or that don't give any benefits</p>",
        "id": 207023990,
        "sender_full_name": "Mr Smeet",
        "timestamp": 1597501393
    },
    {
        "content": "<blockquote>\n<p>I am warming up to Roslyn/libsyntax idea of storing trivia in tokens more!</p>\n</blockquote>\n<p>Ok, I've made up my mind, we should switch to roslyn's handling of trivia. It's way to easy to loose trivia during refactors otherwise.</p>\n<p>Now we need only to rewrite everything <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span></p>",
        "id": 216997634,
        "sender_full_name": "matklad",
        "timestamp": 1605617307
    },
    {
        "content": "<p><a href=\"https://github.com/rust-analyzer/rust-analyzer/issues/6584\">https://github.com/rust-analyzer/rust-analyzer/issues/6584</a></p>",
        "id": 217037464,
        "sender_full_name": "matklad",
        "timestamp": 1605635288
    }
]