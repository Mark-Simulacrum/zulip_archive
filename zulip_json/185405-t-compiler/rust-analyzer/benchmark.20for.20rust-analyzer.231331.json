[
    {
        "content": "<p>So <span class=\"user-mention\" data-user-id=\"133169\">@matklad</span> if I wanted to do some benchmarking here, what would you suggest? I saw the regression test you pointed at for adding a newline, should I just try to convert that to a benchmark?</p>",
        "id": 166903455,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559219727
    },
    {
        "content": "<p>yeah, that would be a good start!</p>",
        "id": 166903467,
        "sender_full_name": "matklad",
        "timestamp": 1559219747
    },
    {
        "content": "<p>I am not sure we are really ready to adding \"true\" benchmarks to rust-analyzer's CI, but we certantly can write benches for one-off profiling</p>",
        "id": 166903535,
        "sender_full_name": "matklad",
        "timestamp": 1559219791
    },
    {
        "content": "<p>Let me breifly explain how <a href=\"https://github.com/rust-analyzer/rust-analyzer/blob/6b88735fe6cd3b259816c7c90a2675ee057c9e4c/crates/ra_lsp_server/tests/heavy_tests/main.rs#L349-L411\" target=\"_blank\" title=\"https://github.com/rust-analyzer/rust-analyzer/blob/6b88735fe6cd3b259816c7c90a2675ee057c9e4c/crates/ra_lsp_server/tests/heavy_tests/main.rs#L349-L411\">https://github.com/rust-analyzer/rust-analyzer/blob/6b88735fe6cd3b259816c7c90a2675ee057c9e4c/crates/ra_lsp_server/tests/heavy_tests/main.rs#L349-L411</a> works.</p>",
        "id": 166903548,
        "sender_full_name": "matklad",
        "timestamp": 1559219820
    },
    {
        "content": "<p>it is an <strong>integration</strong> test, which creates a real instance of rust-analyzer, which reads real files from the hard drive</p>",
        "id": 166903580,
        "sender_full_name": "matklad",
        "timestamp": 1559219850
    },
    {
        "content": "<p>In particular, it reads sources of standard library from the sysroot, so it can be used for load testing</p>",
        "id": 166903658,
        "sender_full_name": "matklad",
        "timestamp": 1559219890
    },
    {
        "content": "<p><code>server</code> is basically a channel to which you can send LSP requests and read responses</p>",
        "id": 166903675,
        "sender_full_name": "matklad",
        "timestamp": 1559219917
    },
    {
        "content": "<p>for benchmarking salsa, I think what we should do is:</p>\n<ul>\n<li>open the proejct</li>\n<li>wait until it is loaded</li>\n<li>send <code>DidOpenTextDocument</code> request. This should trigger syntax highlighting and diagnosits</li>\n<li>recieve diagnostics notification (it will take a while to compute one, especially in debug mode)</li>\n<li>here's the point where we start profiling</li>\n<li>send <code>DidChangeTextDocument</code> notification, which changes whitespace in the document</li>\n<li>wait for the updated set of diagnostics.</li>\n</ul>",
        "id": 166903827,
        "sender_full_name": "matklad",
        "timestamp": 1559220062
    },
    {
        "content": "<p>The subtle bit here is that diagnostics computation is a side-effect of a change, it's not a direct response to some request. The logic which kicks diagnostic computation is here: <a href=\"https://github.com/rust-analyzer/rust-analyzer/blob/6b88735fe6cd3b259816c7c90a2675ee057c9e4c/crates/ra_lsp_server/src/main_loop.rs#L293-L301\" target=\"_blank\" title=\"https://github.com/rust-analyzer/rust-analyzer/blob/6b88735fe6cd3b259816c7c90a2675ee057c9e4c/crates/ra_lsp_server/src/main_loop.rs#L293-L301\">https://github.com/rust-analyzer/rust-analyzer/blob/6b88735fe6cd3b259816c7c90a2675ee057c9e4c/crates/ra_lsp_server/src/main_loop.rs#L293-L301</a></p>",
        "id": 166903956,
        "sender_full_name": "matklad",
        "timestamp": 1559220167
    },
    {
        "content": "<p>You also would probably want to look at the logs/profiling info. The relevan env-vars look like this:</p>\n<div class=\"codehilite\"><pre><span></span>env RA_PROFILE=&#39;*&gt;16&#39; RUST_LOG=ra_lsp_server=info\n</pre></div>\n\n\n<p>(more infor about cryptic <code>*&gt;16</code> syntax in the <code>docs/dev/README.md</code>)</p>",
        "id": 166904007,
        "sender_full_name": "matklad",
        "timestamp": 1559220233
    },
    {
        "content": "<p>You can set the logs in the tests:</p>\n<p><a href=\"https://github.com/rust-analyzer/rust-analyzer/blob/6b88735fe6cd3b259816c7c90a2675ee057c9e4c/crates/ra_lsp_server/tests/heavy_tests/main.rs#L20-L22\" target=\"_blank\" title=\"https://github.com/rust-analyzer/rust-analyzer/blob/6b88735fe6cd3b259816c7c90a2675ee057c9e4c/crates/ra_lsp_server/tests/heavy_tests/main.rs#L20-L22\">https://github.com/rust-analyzer/rust-analyzer/blob/6b88735fe6cd3b259816c7c90a2675ee057c9e4c/crates/ra_lsp_server/tests/heavy_tests/main.rs#L20-L22</a></p>",
        "id": 166904062,
        "sender_full_name": "matklad",
        "timestamp": 1559220256
    },
    {
        "content": "<p>Oh, and a good way to benchmark from-scratch analysis is this command <code>cargo run --release -p ra_cli -- analysis-stats ../chalk/</code>. But yeah, for salsa we specifically interested in <em>incremental</em> analysis, and that's harder to set up</p>",
        "id": 166905213,
        "sender_full_name": "matklad",
        "timestamp": 1559221340
    },
    {
        "content": "<p>that makes we wonder if we should add a <code>--diff</code> argument to the <code>analysis-stats</code> command, which allows one to check re-analysis as well?</p>",
        "id": 166905254,
        "sender_full_name": "matklad",
        "timestamp": 1559221385
    },
    {
        "content": "<blockquote>\n<p>that makes we wonder if we should add a <code>--diff</code> argument to the <code>analysis-stats</code> command, which allows one to check re-analysis as well?</p>\n</blockquote>\n<p>seems like a good idea, <span class=\"user-mention\" data-user-id=\"133169\">@matklad</span></p>",
        "id": 166980527,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559293612
    },
    {
        "content": "<p>I didn't get a chance to do much yesterday but i'm tinkering with this a bit now</p>",
        "id": 166980537,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559293623
    },
    {
        "content": "<p>I could imagine trying to add a <code>--diff</code> to that command -- but i'm first trying it out to see what it does =)</p>",
        "id": 166980553,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559293643
    },
    {
        "content": "<p>Have you seen the benchmark setup we have for <code>perf.rust-lang.org</code>?</p>",
        "id": 166980558,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559293656
    },
    {
        "content": "<p>(In particular, the patches)</p>",
        "id": 166980563,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559293663
    },
    {
        "content": "<p>Might be nice to be able to be compatible with that at <em>some point</em></p>",
        "id": 166980567,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559293672
    },
    {
        "content": "<p>No, I haven't seen that. I am somewhat skeptical that we can come up with really good benchmarks for latency of IDE features. </p>\n<p>testing rustc with patch is easy: you need to check that full compilation is fast.</p>\n<p>testing ra would be tricky, because you also have to select the subset of operations you want to do. The common scenario is something like \"user types something, it kicks diagnostic processing, user asks for completion, and diagnostics and completion now race against each other\". Setting up such scenarios is not just \"invoke all of the queries\".</p>\n<p>What we can and should cerntaly do is to check that diagnostics and highlights for a single file after update have reasonable performance. (as diagostics and highlihgts exercise most of the queries). However, while this is the easiest thing to test, it also is the least useful one, as diagnostics can be computed in the background totally fine. </p>\n<p>What we really interested in are real-time features, like completion or that dreaded on-enter handler. For those, I think investing in observability of the live server is more important than synthetic benchmarks (by the way, I <strong>love</strong> how <code>RA_PROFILE</code> shows real-time profiles, that's super useful)</p>",
        "id": 166985552,
        "sender_full_name": "matklad",
        "timestamp": 1559298713
    },
    {
        "content": "<p>Also, it should probably be called <code>--patch</code> and not <code>--diff</code></p>",
        "id": 166987063,
        "sender_full_name": "matklad",
        "timestamp": 1559300258
    },
    {
        "content": "<p>well I spent the morning just kind of reading into RA code and seeing how the pieces fit together</p>",
        "id": 166992736,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559306094
    },
    {
        "content": "<p>I didn't actually do any measuring yet</p>",
        "id": 166992744,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559306100
    },
    {
        "content": "<p>I certainly take your point that capturing the \"real world\" experience is complex</p>",
        "id": 166992761,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559306110
    },
    {
        "content": "<p>Though it still seems to me that you could measure a lot of individual things, the total of which are important -- e.g., how fast can we do completions with base X and edit Y</p>",
        "id": 166992794,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559306138
    },
    {
        "content": "<p>(and perhaps a stream of edits)</p>",
        "id": 166992802,
        "sender_full_name": "nikomatsakis",
        "timestamp": 1559306147
    },
    {
        "content": "<p>It should be possible to do a 'typing' benchmark where we e.g. add a single letter and then request diagnostics, code actions and a 'real-time' request like onEnter or completions, right? and e.g. simulate typing a whole function that way</p>",
        "id": 166993142,
        "sender_full_name": "Florian Diebold",
        "timestamp": 1559306461
    },
    {
        "content": "<p>Yeah, that should be possible.</p>",
        "id": 166993228,
        "sender_full_name": "matklad",
        "timestamp": 1559306532
    },
    {
        "content": "<p>I also wonder if we could capture a trace of real interraction, and have a benchmark for \"how fast can we replay the trace\"</p>",
        "id": 166993247,
        "sender_full_name": "matklad",
        "timestamp": 1559306565
    },
    {
        "content": "<p>Yeah, perhaps I should stop complaining that it's hard and .. just add a benchmark? Seems totally doable</p>",
        "id": 166993342,
        "sender_full_name": "matklad",
        "timestamp": 1559306648
    },
    {
        "content": "<p>I guess, my experience with IntelliJ is harming me in this case: I haven't sloved benchmarking of IntelliJ Rust at all, but a significant part of that was that</p>\n<ul>\n<li>it's hard to do macro-benchmarks on JVM (we run benchmark like 20 times, picked the minimum time, and still got the wild variance)</li>\n<li>IntelliJ platform has all sorts of caches you don't even know about, so it's hard to make sure you are benching the thing you <em>think</em> you are benching :)</li>\n</ul>",
        "id": 166993437,
        "sender_full_name": "matklad",
        "timestamp": 1559306759
    },
    {
        "content": "<p>Btw, <span class=\"user-mention\" data-user-id=\"129457\">@Florian Diebold</span> I must say that <code>analysis-stats</code> is an awesome tool!</p>",
        "id": 166993496,
        "sender_full_name": "matklad",
        "timestamp": 1559306777
    },
    {
        "content": "<p>Thanks!</p>",
        "id": 166993498,
        "sender_full_name": "matklad",
        "timestamp": 1559306781
    }
]