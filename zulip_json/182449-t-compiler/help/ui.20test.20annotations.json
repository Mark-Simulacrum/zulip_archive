[
    {
        "content": "<p>What is the point of machine-checked annotations in testcases when you <em>have</em> to capture all error output for the <code>ui</code> tests anyway?</p>",
        "id": 163145446,
        "sender_full_name": "ange",
        "timestamp": 1555024429
    },
    {
        "content": "<p>(I'm asking because getting the exact compiler output right for a testcase that cannot yet pass is much more involved than simply saying \"I'm expecting this error here\")</p>",
        "id": 163145653,
        "sender_full_name": "ange",
        "timestamp": 1555024604
    },
    {
        "content": "<p>If you pass <code>--bless</code> then it updates all the ui test output. It's easy to do this and miss that you've accidentally blessed a regression (a missing error, say) if there are a lot of changed tests. By also using annotations, you need to acknowledge that a test lost an error.</p>",
        "id": 163147362,
        "sender_full_name": "davidtwco",
        "timestamp": 1555026609
    },
    {
        "content": "<p>Thanks, that makes sense. So I guess the reason you can't just have an <code>annotations-cover-all-expected-error-output</code> header command is to make sure one doesn't miss subtle changes in the formatting of the output and instead needs to explicitly <code>--bless</code> them?</p>",
        "id": 163147628,
        "sender_full_name": "ange",
        "timestamp": 1555026990
    },
    {
        "content": "<p>Exactly, without stderr files weâ€™d miss small things like a span changing.</p>",
        "id": 163165456,
        "sender_full_name": "davidtwco",
        "timestamp": 1555052538
    }
]