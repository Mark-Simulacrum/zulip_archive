[
    {
        "content": "<p>FWIW, I've been doing some investigation on whether the crater boxes are currently 'well tuned' for our workload, though I admit it's been sort of inbetween other tasks and not particularly well researched. (Maybe folks have some pointers on good documentation here; I wasn't really able to find anything that looked recently updated and easy-ish to use).</p>\n<p>One thing I did notice is that the GCP machines are context switching a <em>lot</em> more - 3x - compared to azure machines. Azure has context-switches at 0.249 K/sec, according to perf, while the gcp ones are at 0.684 K/sec.</p>\n<p>I don't really know if there's an easy way to compare two systems - with different core counts, too - in a way that nicely tracks the cause of context switches. Obviously, the workloads in terms of what crates are getting scheduled and such are different too; this might be caused by that. I only measured across 5 minutes - a longer measurement might be in order to smooth out some of this, but I suspect it won't change anything.</p>\n<p>cc <span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> - I'm wondering if you happen to know knobs we should be tuning to optimize for crater-style workloads, or whether the current strategy of overloading the system (e.g., we have 60 rustc's running right now, with only 14 cores) is a good idea. Maybe there's some documentation I wasn't able to find.</p>\n<p>I've tried adjusting sched_min_granularity_ns up to 1s, but this seemed to not change anything that I could see; I don't know whether it just had no effect or if something else was causing the problem.</p>",
        "id": 234573959,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434013
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> Are you using the default kernels, or custom kernels? Ideally, I'd suggest using near-identical kernel images on both systems for a comparison like that.</p>",
        "id": 234574940,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618434196
    },
    {
        "content": "<p>Can you grab the kernel configurations from both, and I'll diff them and look for notable issues?</p>",
        "id": 234574976,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618434207
    },
    {
        "content": "<p>They are different kernel versions (one is 5.0.0, though customized by azure), the other is 5.4</p>",
        "id": 234575132,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434244
    },
    {
        "content": "<p>Regarding overloads, I personally would favor having only as many rustc instances as logical CPUs. (Threads, not cores.) I can imagine scenarios where having a few more would be useful, but not 4x as many.</p>",
        "id": 234575190,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618434259
    },
    {
        "content": "<p>in terms of grabbing configuration, I guess you mean sysctl -A or something?</p>",
        "id": 234575223,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434271
    },
    {
        "content": "<p>No, I mean the kconfig compile-time kernel configuration.</p>",
        "id": 234575260,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618434281
    },
    {
        "content": "<p>Commonly found in /boot/config-(thekernelversion), or potentially /proc/config.gz</p>",
        "id": 234575318,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618434302
    },
    {
        "content": "<p>All the <code>CONFIG_XYZ</code> options.</p>",
        "id": 234575381,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618434319
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/dnU5KH9hmqG261k3lXVp7frJ/gcp\">gcp</a> <a href=\"/user_uploads/4715/RhR9lm47y_lHlJKZS9z0DQN6/azure\">azure</a></p>",
        "id": 234575631,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434380
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20performance/near/234575190\">said</a>:</p>\n<blockquote>\n<p>Regarding overloads, I personally would favor having only as many rustc instances as logical CPUs. (Threads, not cores.) I can imagine scenarios where having a few more would be useful, but not 4x as many.</p>\n</blockquote>\n<p>so, the reason why crater currently overloads is to \"fill\" the downtime between rustc executions, as crater also does a lot of non-compilation</p>",
        "id": 234575662,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618434389
    },
    {
        "content": "<p>4x is probably too much indeed, but it's not like crater is compiling all the time</p>",
        "id": 234575791,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618434415
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"121055\">@Pietro Albini</span> Ah, interesting.</p>",
        "id": 234575799,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618434418
    },
    {
        "content": "<p>I wonder if we could measure the % of time spent in rustc/linkers vs. other stuff</p>",
        "id": 234575823,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434423
    },
    {
        "content": "<p>It's not like 4x is going to make it that much <em>worse</em>; the kernel is relatively good at context switching. We're talking a few percent potential performance loss, not an order of magnitude.</p>",
        "id": 234575979,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618434465
    },
    {
        "content": "<p>As long as you aren't overloading the amount of memory on the system.</p>",
        "id": 234576013,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618434473
    },
    {
        "content": "<p>Yeah, no, plenty of free memory.</p>",
        "id": 234576041,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434481
    },
    {
        "content": "<p>On average the azure machine has 20 jobs/minute over 48 hour period and gcp has 15 jobs/minute; gcp has 14 cores vs 16 on the azure ones (vCPUs, so, logical cores)</p>",
        "id": 234576265,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434525
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20performance/near/234575823\">said</a>:</p>\n<blockquote>\n<p>I wonder if we could measure the % of time spent in rustc/linkers vs. other stuff</p>\n</blockquote>\n<p>I guess? it'd be quite hard to measure that though, since for every rustc execution crater does the equivalent of <code>docker run --rm -it crates-build-env cargo build</code>, so we'd not be able to measure the time needed to spin up the docker container</p>",
        "id": 234576299,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618434534
    },
    {
        "content": "<p>So there's a 17% performance delta \"per core\" between them, looks like, roughly</p>",
        "id": 234576422,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434568
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20performance/near/234576265\">said</a>:</p>\n<blockquote>\n<p>On average the azure machine has 20 jobs/minute over 48 hour period and gcp has 15 jobs/minute; gcp has 14 cores vs 16 on the azure ones (vCPUs, so, logical cores)</p>\n</blockquote>\n<p>crater also does a bunch of IO, and in my experience locally disk performance impacted crater a bit</p>",
        "id": 234576487,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618434586
    },
    {
        "content": "<p>(running crater with the workspace on hdd vs ssd makes a huge difference, and even ssd vs nvme has an impact)</p>",
        "id": 234576643,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618434619
    },
    {
        "content": "<p>It's mostly writing based on our metrics</p>",
        "id": 234576662,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434623
    },
    {
        "content": "<p>\"plenty of free memory\" -- enough to give them ram-disks?</p>",
        "id": 234576847,
        "sender_full_name": "cuviper",
        "timestamp": 1618434673
    },
    {
        "content": "<p>not <em>that</em> much :)</p>",
        "id": 234576911,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618434695
    },
    {
        "content": "<p>another difference between azure and gcp is the disk space, with gcp having 300gb disks and azure having 2tb disks</p>",
        "id": 234576980,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618434714
    },
    {
        "content": "<p>and that means gcp has to erase the cached prebuilt dependencies more often</p>",
        "id": 234577129,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618434734
    },
    {
        "content": "<p>47 MB/s write / 15.5 MB/s read on average on azure</p>",
        "id": 234577247,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434753
    },
    {
        "content": "<p>Interestingly, GCP is 38 MB/s write and only 1.4 MB/s read</p>",
        "id": 234577453,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434815
    },
    {
        "content": "<p>are they built with debuginfo? could reduce that</p>",
        "id": 234577469,
        "sender_full_name": "cuviper",
        "timestamp": 1618434820
    },
    {
        "content": "<p>though full beta-crater should probably not cut corners</p>",
        "id": 234577531,
        "sender_full_name": "cuviper",
        "timestamp": 1618434838
    },
    {
        "content": "<p>most runs are check-only, so most of the times we shouldn't go to codegen</p>",
        "id": 234577625,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618434856
    },
    {
        "content": "<p>Should all be the same</p>",
        "id": 234577646,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434858
    },
    {
        "content": "<p>Right now a check run</p>",
        "id": 234577661,
        "sender_full_name": "simulacrum",
        "timestamp": 1618434863
    },
    {
        "content": "<p>ah, would only be build-deps then</p>",
        "id": 234577749,
        "sender_full_name": "cuviper",
        "timestamp": 1618434891
    },
    {
        "content": "<p>also, for context for who's not familiar with crater internals, to cache dependencies between crates each crater thread has a single <code>target</code> directory shared between all builds run by that thread</p>",
        "id": 234578045,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618434975
    },
    {
        "content": "<p>when the disk is close to full all those target directories are <code>rm -rf</code>'d, pruning the caches</p>",
        "id": 234578088,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618435002
    },
    {
        "content": "<p>so it wouldn't surprise me that having 300gb of scratch disk space compared to 2tb would have an impact on the overall number of jobs completed by crater</p>",
        "id": 234578143,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618435029
    },
    {
        "content": "<p>couldn't sccache handle that more gracefully, like LRU?</p>",
        "id": 234578194,
        "sender_full_name": "cuviper",
        "timestamp": 1618435058
    },
    {
        "content": "<p>I don't think we've tried doing sccache on crater, we could even do S3 with that in theory, though working out how to get credentials down there would be annoying</p>",
        "id": 234578322,
        "sender_full_name": "simulacrum",
        "timestamp": 1618435122
    },
    {
        "content": "<p>hmm, doesn't sccache not work that well when passing different build flags to it?</p>",
        "id": 234578429,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618435183
    },
    {
        "content": "<p>It doesn't support base directory, but I think otherwise works well?</p>",
        "id": 234578535,
        "sender_full_name": "simulacrum",
        "timestamp": 1618435222
    },
    {
        "content": "<p>And for our use case it's all in docker at a common directory</p>",
        "id": 234578602,
        "sender_full_name": "simulacrum",
        "timestamp": 1618435250
    },
    {
        "content": "<p>There are a few notable kernel configuration differences, but nothing worth building and maintaining a custom kernel for.</p>",
        "id": 234578909,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618435387
    },
    {
        "content": "<p>Yeah. I want to at least update them to be the same version, but I doubt that would be a major difference - especially because the older one is actually faster right now. It might well be just the disk size difference as Pietro notes, though.</p>",
        "id": 234579534,
        "sender_full_name": "simulacrum",
        "timestamp": 1618435645
    },
    {
        "content": "<p>a practical thing we could do to make slight improvements would be to add a <code>docker image prune</code> cronjob on the agents, to delete the old docker images</p>",
        "id": 234579766,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618435756
    },
    {
        "content": "<p>that command freed up 40gb of storage on gcp-1</p>",
        "id": 234579790,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618435770
    },
    {
        "content": "<p>(crates-build-env is around 7gb in size)</p>",
        "id": 234579809,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618435782
    },
    {
        "content": "<p>Hm, yes, that seems promising - I guess that storage should be used for caches now?</p>",
        "id": 234579959,
        "sender_full_name": "simulacrum",
        "timestamp": 1618435847
    },
    {
        "content": "<p>yep!</p>",
        "id": 234579974,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618435858
    },
    {
        "content": "<p>right now crater doesn't remove caches until the total filesystem utilization reaches 85%</p>",
        "id": 234580266,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618435997
    },
    {
        "content": "<p>(it might actually delete stuff a bit later than when it reaches 85%, depending on how long the current crates finishes building)</p>",
        "id": 234580300,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618436018
    },
    {
        "content": "<p>whelp on gcp-2 I free'd 90gb of stale images</p>",
        "id": 234580415,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618436053
    },
    {
        "content": "<p>Good to know. I can file a PR tomorrow, and we should probably add some monitoring for disk space used by the target directories (measured when deleting them).</p>",
        "id": 234580529,
        "sender_full_name": "simulacrum",
        "timestamp": 1618436108
    },
    {
        "content": "<p>It might be worth stopping things, clearing out the target directories like crater does, and then checking how much space we're using for other stuff</p>",
        "id": 234580781,
        "sender_full_name": "simulacrum",
        "timestamp": 1618436240
    },
    {
        "content": "<p>(maybe some of that base image load can be deleted)</p>",
        "id": 234580811,
        "sender_full_name": "simulacrum",
        "timestamp": 1618436255
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20performance/near/234580781\">said</a>:</p>\n<blockquote>\n<p>It might be worth stopping things, clearing out the target directories like crater does, and then checking how much space we're using for other stuff</p>\n</blockquote>\n<p>hmm, we already kinda know that</p>",
        "id": 234581037,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618436378
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/6i0ZnvG8AOO6-xDTUcx1sqBJ/2021-04-14-23-39-59.png\">2021-04-14-23-39-59.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/6i0ZnvG8AOO6-xDTUcx1sqBJ/2021-04-14-23-39-59.png\" title=\"2021-04-14-23-39-59.png\"><img src=\"/user_uploads/4715/6i0ZnvG8AOO6-xDTUcx1sqBJ/2021-04-14-23-39-59.png\"></a></div>",
        "id": 234581132,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618436412
    },
    {
        "content": "<p>the dips here should be as soon as crater finished purging the caches</p>",
        "id": 234581191,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618436437
    },
    {
        "content": "<p>(that was gcp-1)</p>",
        "id": 234581211,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618436446
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/3N-m7EyoFKRhYGFaUj1ZgMcz/2021-04-14-23-41-08.png\">2021-04-14-23-41-08.png</a> <br>\nwhile this is azure-1</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/3N-m7EyoFKRhYGFaUj1ZgMcz/2021-04-14-23-41-08.png\" title=\"2021-04-14-23-41-08.png\"><img src=\"/user_uploads/4715/3N-m7EyoFKRhYGFaUj1ZgMcz/2021-04-14-23-41-08.png\"></a></div>",
        "id": 234581255,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618436481
    },
    {
        "content": "<p>That seems most likely to be the culprit of performance differences.</p>",
        "id": 234581677,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618436695
    },
    {
        "content": "<p>Would it be worth testing a larger disk on GCP to see if it improves performance substantially?</p>",
        "id": 234581704,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618436709
    },
    {
        "content": "<p>iirc I chose 300gb because that was exactly within the budget we had on gcp</p>",
        "id": 234581898,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618436794
    },
    {
        "content": "<p>otherwise we'd have to remove some cores, or well, pay a bit more :)</p>",
        "id": 234581922,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618436810
    },
    {
        "content": "<p>Hm, why do we have 500gb left on the azure machines then? That seems really excessive after purging</p>",
        "id": 234582218,
        "sender_full_name": "simulacrum",
        "timestamp": 1618436969
    },
    {
        "content": "<p>running <code>docker purge</code> on azure-1</p>",
        "id": 234582248,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618436992
    },
    {
        "content": "<p>150gb on gcp also seems way too large</p>",
        "id": 234582252,
        "sender_full_name": "simulacrum",
        "timestamp": 1618436995
    },
    {
        "content": "<p>How big is the latest docker image? I recall us having a big one</p>",
        "id": 234582344,
        "sender_full_name": "simulacrum",
        "timestamp": 1618437022
    },
    {
        "content": "<p>7gb</p>",
        "id": 234582370,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618437031
    },
    {
        "content": "<p>Ah. Ok. So not that.</p>",
        "id": 234582389,
        "sender_full_name": "simulacrum",
        "timestamp": 1618437041
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"121055\">Pietro Albini</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20performance/near/234581922\">said</a>:</p>\n<blockquote>\n<p>otherwise we'd have to remove some cores, or well, pay a bit more :)</p>\n</blockquote>\n<p>Definitely not worth trading cores for storage, I think.</p>",
        "id": 234582398,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618437044
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20performance/near/234582398\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"121055\">Pietro Albini</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20performance/near/234581922\">said</a>:</p>\n<blockquote>\n<p>otherwise we'd have to remove some cores, or well, pay a bit more :)</p>\n</blockquote>\n<p>Definitely not worth trading cores for storage, I think.</p>\n</blockquote>\n<p>that was the call I made too</p>",
        "id": 234582419,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618437057
    },
    {
        "content": "<p>I'm curious, when you say \"budget we had on GCP\", is that donated by Google?</p>",
        "id": 234582423,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618437064
    },
    {
        "content": "<p>Yeah, we can likely increase that - it's mostly a matter of getting the most we can out of the machines we have, IMO</p>",
        "id": 234582455,
        "sender_full_name": "simulacrum",
        "timestamp": 1618437090
    },
    {
        "content": "<p>yes, they gave us a round amount of credits and I squeezed as much as I could there</p>",
        "id": 234582467,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618437093
    },
    {
        "content": "<p>One thought...</p>",
        "id": 234582559,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618437134
    },
    {
        "content": "<p>If there are two GCP systems, would it be feasible to switch to one with twice the cores and twice the disk?</p>",
        "id": 234582594,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618437149
    },
    {
        "content": "<p>That should perform comparably, but share more crate cache.</p>",
        "id": 234582608,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618437164
    },
    {
        "content": "<p>Crate cache isn't shared between workers</p>",
        "id": 234582629,
        "sender_full_name": "simulacrum",
        "timestamp": 1618437182
    },
    {
        "content": "<p>I don't think it would share more crates: we keep a target directory per thread, otherwise cargo would lock the build with \"waiting on lock on target directory\"</p>",
        "id": 234582664,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618437201
    },
    {
        "content": "<p>(so e.g. the azure machines have 16 distinct caches, gcp 14 distinct caches)</p>",
        "id": 234582674,
        "sender_full_name": "simulacrum",
        "timestamp": 1618437206
    },
    {
        "content": "<p>Oh, I didn't realize that we ran a worker per thread.</p>",
        "id": 234582680,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618437210
    },
    {
        "content": "<p>iirc we <em>also</em> keep different copies of another cargo directory to avoid another lock error</p>",
        "id": 234582705,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618437232
    },
    {
        "content": "<p>can't recall which off the top of my head</p>",
        "id": 234582716,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618437237
    },
    {
        "content": "<p>In theory, we could run a background deduplication mechanism that hardlinked identical files between the caches, but anything more than that would be complicated, yeah.</p>",
        "id": 234582787,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618437267
    },
    {
        "content": "<p>What might make sense is to switch to e.g. zfs or btrfs to have automatic content-based-deduplication of the cache, which might save space. But I'm not sure our builds are sufficiently identical for that to make sense.</p>",
        "id": 234582796,
        "sender_full_name": "simulacrum",
        "timestamp": 1618437273
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20performance/near/234582389\">said</a>:</p>\n<blockquote>\n<p>Ah. Ok. So not that.</p>\n</blockquote>\n<p><code>docker image prune</code> on <code>azure-1</code> did not exit yet, and it already removed 100gb</p>",
        "id": 234582832,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618437300
    },
    {
        "content": "<p>(Or maybe I'm remembering wrong, and those FSs don't do that)</p>",
        "id": 234582849,
        "sender_full_name": "simulacrum",
        "timestamp": 1618437306
    },
    {
        "content": "<p>We also have a bunch of (old) toolchains in the crater-agent-workspace, it looks like</p>",
        "id": 234584018,
        "sender_full_name": "simulacrum",
        "timestamp": 1618437849
    },
    {
        "content": "<p>57 GB of them on gcp-1</p>",
        "id": 234584348,
        "sender_full_name": "simulacrum",
        "timestamp": 1618438029
    },
    {
        "content": "<p>opened an issue for pruning docker stuff: <a href=\"https://github.com/rust-lang/crater/issues/571\">https://github.com/rust-lang/crater/issues/571</a></p>",
        "id": 234584555,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618438126
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"121055\">@Pietro Albini</span> didn't we have a plan at some point to shrink the image by separating it into layers? So all but the most recently updated layer could be cached?</p>",
        "id": 234607947,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1618454660
    },
    {
        "content": "<p>I think we did, but I'm not convinced it's a good idea nowadays</p>",
        "id": 234645250,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618479509
    },
    {
        "content": "<p>for production (<a href=\"http://docs.rs\">docs.rs</a> and crater) we want that image to be up to date, and download times don't matter much</p>",
        "id": 234645568,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618479672
    },
    {
        "content": "<p>and now rustwide supports arbitrary images, so for local development we could create a <code>crates-build-env-micro</code> with just <code>build-essential</code> and <code>libssl-dev</code></p>",
        "id": 234645649,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618479715
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"121055\">Pietro Albini</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20performance/near/234645568\">said</a>:</p>\n<blockquote>\n<p>for production (<a href=\"http://docs.rs\">docs.rs</a> and crater) we want that image to be up to date, and download times don't matter much</p>\n</blockquote>\n<p>well I'm not worried as much about download time, but it would save a lot on disk space instead of using 7 GB on each update</p>",
        "id": 234674056,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1618493166
    },
    {
        "content": "<p>I don't see what the image being up to date has to do with it? docker does the caching itself, it would still download any newer version</p>",
        "id": 234674132,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1618493199
    },
    {
        "content": "<p>oh, I meant having the latest version of packages</p>",
        "id": 234700560,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618501216
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> uh the experiment you tried needs to be tuned a bit better</p>",
        "id": 234835333,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618571741
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/XcmqaK3BmLYdjPN07uveWMvd/2021-04-16-13-20-43.png\">2021-04-16-13-20-43.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/XcmqaK3BmLYdjPN07uveWMvd/2021-04-16-13-20-43.png\" title=\"2021-04-16-13-20-43.png\"><img src=\"/user_uploads/4715/XcmqaK3BmLYdjPN07uveWMvd/2021-04-16-13-20-43.png\"></a></div>",
        "id": 234835925,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618572053
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/AXpvywiLD7KfFJJDOuOUMcDO/2021-04-16-13-21-47.png\">2021-04-16-13-21-47.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/AXpvywiLD7KfFJJDOuOUMcDO/2021-04-16-13-21-47.png\" title=\"2021-04-16-13-21-47.png\"><img src=\"/user_uploads/4715/AXpvywiLD7KfFJJDOuOUMcDO/2021-04-16-13-21-47.png\"></a></div>",
        "id": 234835994,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618572115
    },
    {
        "content": "<p>zooming in on a single agent it seems like the threads start, load skyrockets, then everything stops until the load goes back to a reasonable amount, then <code>goto 0</code></p>",
        "id": 234836129,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618572177
    },
    {
        "content": "<p>Oh, I was planning to be around to babysit it</p>",
        "id": 234836378,
        "sender_full_name": "simulacrum",
        "timestamp": 1618572315
    },
    {
        "content": "<p>But yeah that's not unexpected</p>",
        "id": 234836395,
        "sender_full_name": "simulacrum",
        "timestamp": 1618572322
    },
    {
        "content": "<p>do you want to check things in prod or should I revert?</p>",
        "id": 234836846,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618572520
    },
    {
        "content": "<p>Let's revert</p>",
        "id": 234837309,
        "sender_full_name": "simulacrum",
        "timestamp": 1618572769
    },
    {
        "content": "<p>I don't think there's any data to gain from prod</p>",
        "id": 234837332,
        "sender_full_name": "simulacrum",
        "timestamp": 1618572781
    },
    {
        "content": "<p>How difficult would it be to setup a staging branch for just one of the machines, so I could experiment a bit more easily?</p>",
        "id": 234837421,
        "sender_full_name": "simulacrum",
        "timestamp": 1618572817
    },
    {
        "content": "<p>hmm</p>",
        "id": 234837551,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618572858
    },
    {
        "content": "<p>we'd need to either create a second ECR repo and point one of the machines to it</p>",
        "id": 234837610,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618572884
    },
    {
        "content": "<p>or push the staging images to the same ECR repo but with a separate tag</p>",
        "id": 234837636,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618572900
    },
    {
        "content": "<p>(rolled back the image with <code>simpleinfra/aws-rollback.py</code>, will open a revert PR now)</p>",
        "id": 234837686,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618572926
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/crater/pull/574\">https://github.com/rust-lang/crater/pull/574</a></p>",
        "id": 234837837,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618572994
    },
    {
        "content": "<p>I think a separate tag feels good</p>",
        "id": 234838545,
        "sender_full_name": "simulacrum",
        "timestamp": 1618573369
    },
    {
        "content": "<p>separate tag is definitely the least impactful change</p>",
        "id": 234838602,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618573402
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/-UyRUIPv2bcJh8ZXK0yRi3Zs/2021-04-16-13-43-12.png\">2021-04-16-13-43-12.png</a> <br>\nbut I'm a bit worried then about losing the ability to easily rollback prod</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/-UyRUIPv2bcJh8ZXK0yRi3Zs/2021-04-16-13-43-12.png\" title=\"2021-04-16-13-43-12.png\"><img src=\"/user_uploads/4715/-UyRUIPv2bcJh8ZXK0yRi3Zs/2021-04-16-13-43-12.png\"></a></div>",
        "id": 234838633,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618573426
    },
    {
        "content": "<p>(ECR is configured to only keep 4 non-tagged images)</p>",
        "id": 234838693,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618573445
    },
    {
        "content": "<p>I guess... pros and cons for each approach</p>",
        "id": 234838773,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618573497
    },
    {
        "content": "<p>I don't think we'd be deploying to prod while this is being donr</p>",
        "id": 234839542,
        "sender_full_name": "simulacrum",
        "timestamp": 1618573902
    },
    {
        "content": "<p>I expect to finish over the weekend or so, at least with some initial work, and so I'm not too worried about the 4 item history</p>",
        "id": 234839642,
        "sender_full_name": "simulacrum",
        "timestamp": 1618573941
    },
    {
        "content": "<p>We could also just manually set that to 200 for now, but doesn't seem necessary</p>",
        "id": 234839669,
        "sender_full_name": "simulacrum",
        "timestamp": 1618573957
    },
    {
        "content": "<p>I can set this up if you're not opposed</p>",
        "id": 234839683,
        "sender_full_name": "simulacrum",
        "timestamp": 1618573968
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"121055\">@Pietro Albini</span> ^</p>",
        "id": 234839783,
        "sender_full_name": "simulacrum",
        "timestamp": 1618574040
    },
    {
        "content": "<p>yeah that works</p>",
        "id": 234841312,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618574847
    },
    {
        "content": "<p>at the end of the day it's not a huge deal if we can't rollback crater instantly</p>",
        "id": 234841330,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618574858
    },
    {
        "content": "<p>yeah</p>",
        "id": 234841404,
        "sender_full_name": "simulacrum",
        "timestamp": 1618574885
    },
    {
        "content": "<p>OK, I'll set that up then when I get a chance and try to drive this forward, still unclear if it'll actually help</p>",
        "id": 234841452,
        "sender_full_name": "simulacrum",
        "timestamp": 1618574908
    },
    {
        "content": "<p><span aria-label=\"thumbs up\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"thumbs up\">:thumbs_up:</span>.</p>",
        "id": 234841489,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618574919
    },
    {
        "content": "<p>You may want to sample /proc/pressure/cpu instead of load average. The load average also includes tasks waiting for IO which would incur a context switch anyway. And it has higher resolution</p>",
        "id": 234979816,
        "sender_full_name": "The 8472",
        "timestamp": 1618656698
    },
    {
        "content": "<blockquote>\n<p>What might make sense is to switch to e.g. zfs or btrfs to have automatic content-based-deduplication of the cache, which might save space. But I'm not sure our builds are sufficiently identical for that to make sense.</p>\n</blockquote>\n<p>btrfs doesn't have online deduplication. and offline deduplication via reflinks doesn't share the page cache afaik.<br>\nzfs has online deduplication but it eats tons of ram<br>\nhardlinks could work</p>",
        "id": 234980171,
        "sender_full_name": "The 8472",
        "timestamp": 1618657087
    },
    {
        "content": "<p>If writes are an issue there's also a bunch of filesystem durability features you can modify (e.g. noauto_da_alloc and data=writeback on ext4), <br>\nAnd if docker uses overlayfs as storage driver then the <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=c86243b090bc25f81abe33ddfa9fe833e02c4d64\">new volatile mount option in 5.10</a> might help.<br>\nNot sure though if can actually be enabled via remount or has to be there from the first mount.</p>",
        "id": 234980488,
        "sender_full_name": "The 8472",
        "timestamp": 1618657425
    },
    {
        "content": "<p>no, doesn't look like it can be added with a remount. so another layer of overlayfs on top would be needed to use it</p>",
        "id": 234981816,
        "sender_full_name": "The 8472",
        "timestamp": 1618658874
    },
    {
        "content": "<p>Thanks - that's all very helpful</p>",
        "id": 235034844,
        "sender_full_name": "simulacrum",
        "timestamp": 1618711179
    },
    {
        "content": "<p>I've deployed to gcp-1 usage of the jobserver that's already supported by cargo, which has reduced the load on the system; it's not clear yet (too short sample size) if this is a net improvement in throughput though. No excellent results either, though.</p>",
        "id": 235034925,
        "sender_full_name": "simulacrum",
        "timestamp": 1618711224
    },
    {
        "content": "<p>we'll see how it does over the next some hours - if it seems to break, then the config for the systemd job merely needs to be edited to :latest and that should fix it.</p>\n<p>My guess is it'll run into the typical jobserver bug of tokens not getting returned if something exits abnormally, but we'll see if that poses a problem in practice. It's also quite possible this is ~useless and we already work well as-is, without these changes</p>",
        "id": 235035606,
        "sender_full_name": "simulacrum",
        "timestamp": 1618712055
    },
    {
        "content": "<p>cargo's jobserver doesn't benefit from recent kernel optimizations though since it uses polling instead of blocking reads. so more context switches than necessary. I already filed an issue for that... maybe I should just write a PR.</p>",
        "id": 235062696,
        "sender_full_name": "The 8472",
        "timestamp": 1618742413
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"330154\">@The 8472</span> If there are optimizations you could offer there, that'd be highly welcome.</p>",
        "id": 235459584,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1618984730
    },
    {
        "content": "<p>They're already in jobserver-rs 0.1.22. <a href=\"https://github.com/alexcrichton/jobserver-rs/commits/master\">https://github.com/alexcrichton/jobserver-rs/commits/master</a><br>\nThat's a reader side optimization. I assume cargo just passes the file descriptors it through to the rustc instances? If so the dependency in rustc needs to be bumped.</p>",
        "id": 235475795,
        "sender_full_name": "The 8472",
        "timestamp": 1618995193
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/issues/84407\">#84407</a></p>",
        "id": 235572625,
        "sender_full_name": "The 8472",
        "timestamp": 1619035436
    },
    {
        "content": "<p>Yes, and regardless, the Cargo lockfile is either ignored or doesn't exist (I forget) for the cargo's we build.</p>",
        "id": 235573959,
        "sender_full_name": "simulacrum",
        "timestamp": 1619035944
    },
    {
        "content": "<blockquote>\n<p>My guess is it'll run into the typical jobserver bug of tokens not getting returned if something exits abnormally, but we'll see if that poses a problem in practice. </p>\n</blockquote>\n<p>This could be solved by throwing some extra tokens into the pipe, then discarding it and replacing it with a new one whenever a child doesn't exit properly. Old jobs will run on whatever tokens are left in the pipe and among each other and new ones will have a clean pool. It'll lead to temporary oversubscription but prevent permanent underutilization.</p>",
        "id": 235575488,
        "sender_full_name": "The 8472",
        "timestamp": 1619036709
    },
    {
        "content": "<p>Yeah, ultimately, it didn't really seem to hit that (at least how I'd expect it to if it happened), not sure whether that's due to a) my actually getting the implementation wrong somehow or b) few crates getting killed in a way that caused token loss. The load on the server definitely dropped, but this had no observable effect on actual throughput in terms of crates per minute or whatever. We can try again once the jobserver patch you posted lands and propagates enough, but that will also need a bump on the kernel for that machine (shouldn't be a problem, in theory, just needs some rebooting and such). We have discussion of those bumps scheduled for next week.</p>",
        "id": 235576008,
        "sender_full_name": "simulacrum",
        "timestamp": 1619036972
    },
    {
        "content": "<p>FWIW currently experimenting with just reducing the thread count on the machines (i.e. how many parallel cargos etc we run), and at least initial results suggest that there's no difference between 10 and 5, and potentially no difference between 5 and 3 in terms of throughput, though average CPU usage definitely goes down.</p>\n<p>That seems to suggest that throughput is blocked on some other resource, one we've not yet determined, but it seems hard to say definitively.</p>",
        "id": 235721244,
        "sender_full_name": "simulacrum",
        "timestamp": 1619114280
    },
    {
        "content": "<p>(We do see larger caches, by virtue of splitting the disk amongst less workers, so maybe that is the tradeoff we're addressing)</p>",
        "id": 235721367,
        "sender_full_name": "simulacrum",
        "timestamp": 1619114318
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"121055\">@Pietro Albini</span> I'm curious if you know whether we had throughput metrics dictating the thread=10 count on the GCP workers, or if it was intended simply to ensure we were constantly at 100% CPU?</p>",
        "id": 235721463,
        "sender_full_name": "simulacrum",
        "timestamp": 1619114359
    },
    {
        "content": "<p>that was a <em>long</em> time ago</p>",
        "id": 235726053,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1619116183
    },
    {
        "content": "<p>I think I guesstimated two years ago what would 100% the CPU and then did proportions since then</p>",
        "id": 235726189,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1619116219
    },
    {
        "content": "<p>ok</p>",
        "id": 235727582,
        "sender_full_name": "simulacrum",
        "timestamp": 1619116746
    },
    {
        "content": "<p><a href=\"#narrow/stream/242791-t-infra/topic/playground.20issues.3F/near/234586123\">https://rust-lang.zulipchat.com/#narrow/stream/242791-t-infra/topic/playground.20issues.3F/near/234586123</a><br>\nnagisa suggested there's a prometheus exporter for pressure stall information. if you haven't looked at memory/IO pressure yet.</p>",
        "id": 235728263,
        "sender_full_name": "The 8472",
        "timestamp": 1619117014
    },
    {
        "content": "<p>Both seem ~zero, based on manual inspection on all of the runners.</p>",
        "id": 235730105,
        "sender_full_name": "simulacrum",
        "timestamp": 1619117821
    },
    {
        "content": "<p>odd, why wouldn't it scale with cpu utilization then?</p>",
        "id": 235730986,
        "sender_full_name": "The 8472",
        "timestamp": 1619118191
    },
    {
        "content": "<p>how much is spent in the kernel?</p>",
        "id": 235731154,
        "sender_full_name": "The 8472",
        "timestamp": 1619118255
    },
    {
        "content": "<p>My guess is we're burning CPU, e.g., we have memory left (so no memory pressure) but are still hitting a bunch of overhead from context switching or whatever. But I don't know how to prove that.</p>",
        "id": 235731156,
        "sender_full_name": "simulacrum",
        "timestamp": 1619118257
    },
    {
        "content": "<p>~10% 'sys' cpu usage</p>",
        "id": 235731199,
        "sender_full_name": "simulacrum",
        "timestamp": 1619118284
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code># on 3-worker node\n$ tail /proc/pressure/*\n==&gt; /proc/pressure/cpu &lt;==\nsome avg10=5.87 avg60=11.80 avg300=13.44 total=803265577317\n\n==&gt; /proc/pressure/io &lt;==\nsome avg10=0.38 avg60=0.80 avg300=0.70 total=15699602295\nfull avg10=0.26 avg60=0.66 avg300=0.54 total=2398199866\n\n==&gt; /proc/pressure/memory &lt;==\nsome avg10=0.00 avg60=0.00 avg300=0.00 total=2619255506\nfull avg10=0.00 avg60=0.00 avg300=0.00 total=444665793\n\n# on a 10-worker node\n$ tail /proc/pressure/*\n==&gt; /proc/pressure/cpu &lt;==\nsome avg10=97.21 avg60=93.02 avg300=91.75 total=1463334511136\n\n==&gt; /proc/pressure/io &lt;==\nsome avg10=0.90 avg60=1.34 avg300=1.31 total=27106684130\nfull avg10=0.00 avg60=0.00 avg300=0.00 total=3621447640\n\n==&gt; /proc/pressure/memory &lt;==\nsome avg10=0.17 avg60=0.12 avg300=0.15 total=4874537937\nfull avg10=0.00 avg60=0.00 avg300=0.00 total=805547034\n</code></pre></div>",
        "id": 235731817,
        "sender_full_name": "simulacrum",
        "timestamp": 1619118568
    },
    {
        "content": "<p>well, that's still oversubscribing the CPU. CPU pressure should be 0 if there are exactly as many runnable threads as cores. you said you added a jobserver, so something isn't under jobserver control.</p>",
        "id": 235732323,
        "sender_full_name": "The 8472",
        "timestamp": 1619118803
    },
    {
        "content": "<p>this is without the jobserver</p>",
        "id": 235732367,
        "sender_full_name": "simulacrum",
        "timestamp": 1619118825
    },
    {
        "content": "<p>I don't think I collected these stats with the jobserver</p>",
        "id": 235732389,
        "sender_full_name": "simulacrum",
        "timestamp": 1619118833
    },
    {
        "content": "<p>we could of course try it again, I suppose</p>",
        "id": 235732614,
        "sender_full_name": "simulacrum",
        "timestamp": 1619118923
    },
    {
        "content": "<p>slight oversubscription to keep the CPUs fed + jobserver to keep contention down seems like it would provide good throughput in theory</p>",
        "id": 235733070,
        "sender_full_name": "The 8472",
        "timestamp": 1619119129
    },
    {
        "content": "<p>ah, does that also run tests?</p>",
        "id": 235733130,
        "sender_full_name": "The 8472",
        "timestamp": 1619119150
    },
    {
        "content": "<p>the current jobs are just cargo check</p>",
        "id": 235733141,
        "sender_full_name": "simulacrum",
        "timestamp": 1619119159
    },
    {
        "content": "<p>I don't think libtest uses jobserver today, so that's a valid point</p>",
        "id": 235733156,
        "sender_full_name": "simulacrum",
        "timestamp": 1619119169
    },
    {
        "content": "<p>An interesting observation, though I'm not sure how reliable, is that it seems like the crater-azure instances <em>sped up</em> after gcp instances went offline. Not really sure what to make of this; I guess one possibility is we actually have a bottleneck in the rust-bots machine (or network to it, whatever).</p>\n<p><a href=\"/user_uploads/4715/yIjl6ZQZJwUsCaWPOUciUJ58/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/yIjl6ZQZJwUsCaWPOUciUJ58/image.png\" title=\"image.png\"><img src=\"/user_uploads/4715/yIjl6ZQZJwUsCaWPOUciUJ58/image.png\"></a></div>",
        "id": 238916702,
        "sender_full_name": "simulacrum",
        "timestamp": 1621103423
    },
    {
        "content": "<p>it's possible we switched to a different job, too, I guess... not sure how reasonable that is</p>",
        "id": 238916773,
        "sender_full_name": "simulacrum",
        "timestamp": 1621103480
    }
]