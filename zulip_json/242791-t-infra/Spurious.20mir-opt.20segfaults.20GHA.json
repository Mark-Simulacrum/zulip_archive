[
    {
        "content": "<p>I've recently seen a lot of spurious segfaults for x86_64-gnu-llvm-12 GitHub Action runs. (It can be frustrating for people that don't have permissions to rerun them) Can anyone take a look? Here's a recent run: <a href=\"https://github.com/rust-lang/rust/runs/4977289409?check_suite_focus=true\">https://github.com/rust-lang/rust/runs/4977289409?check_suite_focus=true</a>.</p>",
        "id": 269699225,
        "sender_full_name": "fee1-dead",
        "timestamp": 1643357008
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/issues/93384\">#93384</a></p>",
        "id": 269699351,
        "sender_full_name": "Hans Kratz",
        "timestamp": 1643357070
    },
    {
        "content": "<p>Thanks for the pointer.</p>",
        "id": 269699747,
        "sender_full_name": "fee1-dead",
        "timestamp": 1643357340
    },
    {
        "content": "<p>I've been trying to reproduce it locally without success.</p>",
        "id": 269735465,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643376661
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120518\">@Eric Huss</span> is that with llvm 12? If we've not seen it elsewhere it seems plausible to be related</p>",
        "id": 269738198,
        "sender_full_name": "simulacrum",
        "timestamp": 1643377848
    },
    {
        "content": "<p>Yea, I'm running <code>src/ci/docker/run.sh x86_64-gnu-llvm-12</code></p>",
        "id": 269738293,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643377892
    },
    {
        "content": "<p>I'm trying now on a real linux box, I was doing it via Windows WSL before which isn't quite the same.</p>",
        "id": 269738320,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643377912
    },
    {
        "content": "<p>The same x86_64-gnu-llvm-12 job runs on pull request CI where it consistently fails, and during merge process where it consistently succeeds, right? Is there anything else that differs between those two runs?</p>",
        "id": 269740000,
        "sender_full_name": "tm",
        "timestamp": 1643378551
    },
    {
        "content": "<p>I recall we have had segfaults before (or some other mysterious library issues, not sure) on GHA that went away after a few days, so it might be specific to their system</p>",
        "id": 269740002,
        "sender_full_name": "The 8472",
        "timestamp": 1643378551
    },
    {
        "content": "<p>I'm having a hard time reproducing it, even on the rust-lang account.  I ran the mir-opt tests in a loop 200 times without an error.  I'm running low on ideas on how to reproduce it.</p>",
        "id": 269795339,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643400746
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120518\">@Eric Huss</span> I suspect our emitted IR may be non-deterministic? Maybe worth dumping that from src/bootstrap/bin/rustc.rs?</p>\n<p>Or, perhaps more likely that it's another 'random segfaults in github actions' thing</p>",
        "id": 269806191,
        "sender_full_name": "simulacrum",
        "timestamp": 1643405861
    },
    {
        "content": "<p>Yea.  I'm just having a hard time getting it to fail at all, even re-running the container from scratch.  If done that hundreds of times locally without error.</p>\n<p>I was thinking maybe it was the particular docker cache used on CI, but I've been testing that too without error.  I was thinking maybe the version of llvm-12 in that cache was maybe a little out-of-date.</p>",
        "id": 269806503,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643406014
    },
    {
        "content": "<p>It's hard to imagine how this could be specific to GitHub Actions since it always fails during the mir-opt tests.  It is also random how many mir-opt tests pass.</p>\n<p>It is also strange that it is <code>compiletest</code> itself which is failing.  All it is doing is spawning some processes and comparing their output.</p>",
        "id": 269806599,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643406081
    },
    {
        "content": "<p>yeah, it does seem odd -- it's worth noting I guess that compiletest shouldn't be affected by the recent mir-opt change landing, right? Since it's built by beta rustc and beta std, iirc?</p>",
        "id": 269806723,
        "sender_full_name": "simulacrum",
        "timestamp": 1643406142
    },
    {
        "content": "<p>Right, it is a stage0 tool.</p>",
        "id": 269806814,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643406193
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"120518\">Eric Huss</span> <a href=\"#narrow/stream/242791-t-infra/topic/Spurious.20mir-opt.20segfaults.20GHA/near/269806599\">said</a>:</p>\n<blockquote>\n<p>It's hard to imagine how this could be specific to GitHub Actions since it always fails during the mir-opt tests.  It is also random how many mir-opt tests pass.</p>\n<p>It is also strange that it is <code>compiletest</code> itself which is failing.  All it is doing is spawning some processes and comparing their output.</p>\n</blockquote>\n<p>Could it be the same test which is consistently failing, and the different number of passing tests just being the result of the test execution getting re-ordered by the OS (I believe they run in parallel right?)</p>",
        "id": 269807231,
        "sender_full_name": "Jake",
        "timestamp": 1643406433
    },
    {
        "content": "<p>It could be.  The tests run very fast, and it runs 16 in parallel.</p>\n<p>One thing I was going to try was to enable verbose tests to at least see which ones started but didn't finish.</p>",
        "id": 269807599,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643406646
    },
    {
        "content": "<p>For the time being, it may also be worth disabling mir-opt tests in PR CI, to avoid the constant noise for contributors -- not sure how quickly we get to that point, though. Obviously not an ideal state.</p>",
        "id": 269807717,
        "sender_full_name": "simulacrum",
        "timestamp": 1643406711
    },
    {
        "content": "<p>Just on the off chance that it was related to the CPU, I scanned all the errors, but they hit pretty much every CPU type in the pool.</p>",
        "id": 269818244,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643412740
    },
    {
        "content": "<p>I've hit this twice now on <a href=\"https://github.com/rust-lang/rust/issues/93351\">#93351</a> (out of like, four check builds?)</p>",
        "id": 269821151,
        "sender_full_name": "tmandry",
        "timestamp": 1643414605
    },
    {
        "content": "<p>Yea, I just scanned the builds since it started, and it is about a 50% failure rate.</p>",
        "id": 269821184,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643414631
    },
    {
        "content": "<p>at a glance that builder looks to be responsible for roughly half or more of the failures on <a href=\"https://github.com/rust-lang/rust/actions/workflows/ci.yml\">this page</a></p>",
        "id": 269821185,
        "sender_full_name": "tmandry",
        "timestamp": 1643414635
    },
    {
        "content": "<p>ah you beat me to it :)</p>",
        "id": 269821241,
        "sender_full_name": "tmandry",
        "timestamp": 1643414653
    },
    {
        "content": "<p>Just from my notes:</p>\n<p><a href=\"https://github.com/rust-lang/rust/runs/4958862214?check_suite_focus=true\">https://github.com/rust-lang/rust/runs/4958862214?check_suite_focus=true</a> is the first failure starting at 2022-01-26T22:45:32</p>\n<p>It is conspicuous that it started just after <a href=\"https://github.com/rust-lang/rust/issues/91840\">#91840</a> landed.  I'm still not clear how a test failure would cause compiletest itself to crash.</p>",
        "id": 269821452,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643414777
    },
    {
        "content": "<p>I'm gonna get a PR with a revert for those changes up in a couple minutes and we can see if the issue still shows up</p>",
        "id": 269821960,
        "sender_full_name": "Jake",
        "timestamp": 1643415113
    },
    {
        "content": "<p>Running at <a href=\"https://github.com/rust-lang/rust/issues/93446\">#93446</a></p>",
        "id": 269822928,
        "sender_full_name": "Jake",
        "timestamp": 1643415750
    },
    {
        "content": "<p>That still failed, so probably that change is unrelated? This whole situation is weird though, so its very difficult to be sure</p>",
        "id": 269826884,
        "sender_full_name": "Jake",
        "timestamp": 1643418895
    },
    {
        "content": "<p>That's so weird, thanks for trying <span class=\"user-mention\" data-user-id=\"310518\">@Jake</span>!</p>",
        "id": 269826909,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643418938
    },
    {
        "content": "<p>I posted <a href=\"https://github.com/rust-lang/rust/issues/93447\">#93447</a> to disable the tests until we can better understand what is going on.</p>",
        "id": 269827287,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643419283
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120518\">@Eric Huss</span> do we know if retrying will work? (i.e. running them again if we see a segfault?)</p>",
        "id": 269829113,
        "sender_full_name": "simulacrum",
        "timestamp": 1643421184
    },
    {
        "content": "<p>I don't know.  I have not really had much luck reproducing it (I only triggered it once).  </p>\n<p>I'm trying to run it with <code>catchsegv</code> to get a backtrace, but I'm wondering if that maybe perturbs it too much.</p>",
        "id": 269832191,
        "sender_full_name": "Eric Huss",
        "timestamp": 1643424460
    },
    {
        "content": "<p>Ok</p>",
        "id": 269832726,
        "sender_full_name": "simulacrum",
        "timestamp": 1643425083
    },
    {
        "content": "<p>I'd probably prefer a retry loop (up to 5 times say) as a first step before disabling (r=me on that) so we can get data on whether it's deterministic or not, at least</p>",
        "id": 269832768,
        "sender_full_name": "simulacrum",
        "timestamp": 1643425139
    },
    {
        "content": "<p>Why not change the CI in a PR and use <code>catchsegv</code>?</p>",
        "id": 269863807,
        "sender_full_name": "fee1-dead",
        "timestamp": 1643462305
    }
]