[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116155\">@Jake Goulding</span> To a first approximation, it might be worth just giving every job both cores on the system, and relying on the scheduler when you have multiple jobs running.</p>",
        "id": 271033606,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644263899
    },
    {
        "content": "<p>I don't know if that would be a net win in the conflicting case, but I wouldn't expect it to be a substantial loss if it was a loss, and it'd be a win in the single-job case.</p>",
        "id": 271033668,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644263929
    },
    {
        "content": "<p>Seems worth testing at least.</p>",
        "id": 271033705,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644263952
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116155\">@Jake Goulding</span> Do you have numbers on what percentage of jobs run uncontended?</p>",
        "id": 271033719,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644263965
    },
    {
        "content": "<p>Not directly. If you have access â€” <a href=\"https://grafana.rust-lang.org/d/BoFKl4CGk/playground-timeouts?orgId=1\">https://grafana.rust-lang.org/d/BoFKl4CGk/playground-timeouts?orgId=1</a></p>",
        "id": 271040664,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1644267450
    },
    {
        "content": "<p>Notably: </p>\n<p><a href=\"/user_uploads/4715/-iP3ypoNq0gi0VUf9hDomXxj/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/-iP3ypoNq0gi0VUf9hDomXxj/image.png\" title=\"image.png\"><img src=\"/user_uploads/4715/-iP3ypoNq0gi0VUf9hDomXxj/image.png\"></a></div>",
        "id": 271040709,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1644267477
    },
    {
        "content": "<p>I don't (and don't have a need-to-know).</p>",
        "id": 271040711,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644267478
    },
    {
        "content": "<p>The playground is a 2-core EC2 server, so AFAIK load average should be &lt; 2 ;-)</p>",
        "id": 271040807,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1644267519
    },
    {
        "content": "<p>22 runnable threads on a 2 core machine? and that's not even peak but smoothed?</p>",
        "id": 271040828,
        "sender_full_name": "The 8472",
        "timestamp": 1644267530
    },
    {
        "content": "<p>we should probably just bump the specs to alleviate the problem</p>",
        "id": 271041436,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1644267810
    },
    {
        "content": "<p>From my miscellaneous quotes file:</p>\n<blockquote>\n<p>We learned that the Linux load average rolls over at 1024. And we actually found this out empirically. -- H. Peter Anvin, about rsync on <a href=\"http://kernel.org\">kernel.org</a></p>\n</blockquote>",
        "id": 271042708,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644268463
    },
    {
        "content": "<p>(That was a long time ago, I'm sure it doesn't wrap anymore.)</p>",
        "id": 271042762,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644268496
    },
    {
        "content": "<p>Part of me is morbidly curious how much more it would cost to run playground on lambda, and whether the scale-up-and-down-with-load would make up for the increased cost per CPU on lambda.</p>",
        "id": 271042852,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644268556
    },
    {
        "content": "<p>(But if we <em>did</em> do that, it should still have a hard peak limit, and a queue past that point.)</p>",
        "id": 271042922,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644268581
    },
    {
        "content": "<p>it's way harder to sandbox on lambda unfortunately</p>",
        "id": 271043352,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1644268814
    },
    {
        "content": "<p>(I'm pretty sure if the lambda is warm it's executed in the same container, so a job could affect the other)</p>",
        "id": 271043413,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1644268851
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"121055\">Pietro Albini</span> <a href=\"#narrow/stream/242791-t-infra/topic/playground.20jobserver/near/271041436\">said</a>:</p>\n<blockquote>\n<p>we should probably just bump the specs to alleviate the problem</p>\n</blockquote>\n<p><a href=\"https://github.com/rust-lang/simpleinfra/pull/78\">https://github.com/rust-lang/simpleinfra/pull/78</a></p>",
        "id": 271045830,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1644270057
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"330154\">The 8472</span> <a href=\"#narrow/stream/242791-t-infra/topic/playground.20jobserver/near/271040828\">said</a>:</p>\n<blockquote>\n<p>22 runnable threads on a 2 core machine? and that's not even peak but smoothed?</p>\n</blockquote>\n<p>Just as a double-check, <code>top</code> reports it right now as <code>load average: 2.15, 2.48, 2.24</code>.</p>",
        "id": 271047879,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1644270847
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"121055\">Pietro Albini</span> <a href=\"#narrow/stream/242791-t-infra/topic/playground.20jobserver/near/271043413\">said</a>:</p>\n<blockquote>\n<p>(I'm pretty sure if the lambda is warm it's executed in the same container, so a job could affect the other)</p>\n</blockquote>\n<p>I believe that's addressable with a custom container, but yeah, it's a pain to deal with.</p>",
        "id": 271048030,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644270932
    },
    {
        "content": "<p>(I have that problem as well, where I need lambdas to run <em>one</em> job and then exit.)</p>",
        "id": 271048067,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1644270956
    },
    {
        "content": "<p>Yeah, bumping it is <em>largely</em> waiting on <span class=\"user-mention\" data-user-id=\"121055\">@Pietro Albini</span> your input on the security groups/subnets in AWS (or I can sit down and spend a few hours on figuring it out). I didn't really manage to parse out intent last time I tried (a month ish back now)</p>",
        "id": 271062288,
        "sender_full_name": "simulacrum",
        "timestamp": 1644278757
    },
    {
        "content": "<p>(saw the ping, will respond soonishish)</p>",
        "id": 271177190,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1644347812
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116155\">@Jake Goulding</span> ok merged the ansible/terraform config and deployed all of it</p>",
        "id": 272697800,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1645455048
    },
    {
        "content": "<p>it's currently failing to start with:</p>\n<div class=\"codehilite\"><pre><span></span><code>Feb 21 14:47:09 play-next.infra.rust-lang.org systemd[1]: Started The Rust Playground.\nFeb 21 14:47:09 play-next.infra.rust-lang.org systemd[441]: playground.service: Changing to the requested working directory failed: No such file or directory\nFeb 21 14:47:09 play-next.infra.rust-lang.org systemd[441]: playground.service: Failed at step CHDIR spawning /home/playground/playground-artifacts/ui: No such file or directory\nFeb 21 14:47:09 play-next.infra.rust-lang.org systemd[1]: playground.service: Main process exited, code=exited, status=200/CHDIR\nFeb 21 14:47:09 play-next.infra.rust-lang.org systemd[1]: playground.service: Failed with result &#39;exit-code&#39;.\n</code></pre></div>",
        "id": 272697816,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1645455055
    },
    {
        "content": "<p>you should have sudo access to the server, please try to make the changes directly with ansible instead of manually on the server, and then open a PR with the changes you made <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 272697866,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1645455082
    }
]