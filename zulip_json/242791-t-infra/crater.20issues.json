[
    {
        "content": "<p>A Crater job just failed, and three agents are marked as 'Online' instead of 'Working': <a href=\"https://crater.rust-lang.org/ex/pr-76219\">https://crater.rust-lang.org/ex/pr-76219</a></p>",
        "id": 209851525,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1599872283
    },
    {
        "content": "<p>One agent is currently marked 'Online' instead of 'Working'</p>",
        "id": 209908766,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1599961279
    },
    {
        "content": "<p>They are marked as 'Online' and 'Unreachable' respectively</p>",
        "id": 231150327,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1616251352
    },
    {
        "content": "<p>The <a href=\"https://crater.rust-lang.org/\">crater queue</a> shows <a href=\"https://github.com/rust-lang/rust/issues/82565\">#82565</a> as 'running', but it also already continued with the second one in the queue. It should've already been finished. Any idea what happened here?</p>\n<p><a href=\"https://crater.rust-lang.org/ex/pr-82565\">https://crater.rust-lang.org/ex/pr-82565</a></p>\n<blockquote>\n<p>Estimated end:    22 minutes</p>\n</blockquote>",
        "id": 231213500,
        "sender_full_name": "Mara",
        "timestamp": 1616329622
    },
    {
        "content": "<p>Hmm, crater website is down for me...</p>",
        "id": 231245558,
        "sender_full_name": "Noah Lev",
        "timestamp": 1616365806
    },
    {
        "content": "<p>nvm, it's just loading really slowly</p>",
        "id": 231245615,
        "sender_full_name": "Noah Lev",
        "timestamp": 1616365890
    },
    {
        "content": "<p>oh looks like the other PR it is 'running' is also stuck. that one just permanently displays <code>76%</code> it seems.</p>",
        "id": 231356936,
        "sender_full_name": "Mara",
        "timestamp": 1616436222
    },
    {
        "content": "<p>all of this was due to an underlying problem with the crater agents on gcp</p>",
        "id": 231485119,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1616511811
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> restarted them, so hopefully everything should start working again</p>",
        "id": 231485169,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1616511830
    },
    {
        "content": "<p><a href=\"https://crater.rust-lang.org/\">https://crater.rust-lang.org/</a> is now giving a 502</p>",
        "id": 231690098,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1616612407
    },
    {
        "content": "<p>gah the server oom'd</p>",
        "id": 231691524,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1616612951
    },
    {
        "content": "<p>(also, I'm wondering why our alerting for crater stopped working...)</p>",
        "id": 231691540,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1616612960
    },
    {
        "content": "<p>crater is stuck again: <a href=\"https://crater.rust-lang.org/ex/pr-82781\">https://crater.rust-lang.org/ex/pr-82781</a></p>",
        "id": 232084258,
        "sender_full_name": "Mara",
        "timestamp": 1616845094
    },
    {
        "content": "<p>Any updates? Would like to have a look at those results :)</p>",
        "id": 232754841,
        "sender_full_name": "Dirkjan Ochtman",
        "timestamp": 1617283271
    },
    {
        "content": "<p>None of the agents are running except for <code>azure-1</code></p>",
        "id": 232940588,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1617391286
    },
    {
        "content": "<p>It seems like there have been many more Crater issues than usual recently - is there a common cause to any of them? At this rate, it's going to take over a month to get through the queue, which is going to hold up many prs</p>",
        "id": 232978154,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1617422422
    },
    {
        "content": "<p>I'm planning to try to fit some time in to investigate this problem this weekend, IIRC there's the normal problem of crater machines getting into a deadlock, but it's recently been exacerbated by our alerting going down for unknown reasons.</p>",
        "id": 232979237,
        "sender_full_name": "simulacrum",
        "timestamp": 1617423633
    },
    {
        "content": "<p>(as may have been expected I did not find the time. Maybe this week)</p>",
        "id": 233126612,
        "sender_full_name": "simulacrum",
        "timestamp": 1617586068
    },
    {
        "content": "<p><a href=\"https://crater.rust-lang.org/\">https://crater.rust-lang.org/</a> is now showing a 502 error</p>",
        "id": 233199990,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1617642385
    },
    {
        "content": "<p>hm, investigating this</p>",
        "id": 233204115,
        "sender_full_name": "simulacrum",
        "timestamp": 1617644390
    },
    {
        "content": "<p>ok, not sure why it was down, but crater.r-l.o is back up</p>",
        "id": 233204949,
        "sender_full_name": "simulacrum",
        "timestamp": 1617644800
    },
    {
        "content": "<p>Two jobs are now marked as 'Generating report ' - I didn't think that was possible</p>",
        "id": 233205487,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1617645065
    },
    {
        "content": "<p>Trying to look into those</p>",
        "id": 233205980,
        "sender_full_name": "simulacrum",
        "timestamp": 1617645283
    },
    {
        "content": "<p>seems like the retry-report should make this work</p>",
        "id": 233207655,
        "sender_full_name": "simulacrum",
        "timestamp": 1617645979
    },
    {
        "content": "<p>though I have a suspected cause - looks like writing out full.html used roughly 80% of memory on the machine</p>",
        "id": 233207684,
        "sender_full_name": "simulacrum",
        "timestamp": 1617646000
    },
    {
        "content": "<p>ok, stopped all the agents</p>",
        "id": 233209609,
        "sender_full_name": "simulacrum",
        "timestamp": 1617646936
    },
    {
        "content": "<p>my current belief is that the problem we have is this:</p>\n<ul>\n<li>generating a report is done on the same thread as everything else, and essentially doesn't yield</li>\n</ul>",
        "id": 233209677,
        "sender_full_name": "simulacrum",
        "timestamp": 1617646973
    },
    {
        "content": "<p>and this particular server only has one cpu, so the webserver and that thread are the same thread</p>",
        "id": 233209715,
        "sender_full_name": "simulacrum",
        "timestamp": 1617646998
    },
    {
        "content": "<p>i'm going to see if that is accurate (i.e. that the webserver runs in the same thread)</p>",
        "id": 233209740,
        "sender_full_name": "simulacrum",
        "timestamp": 1617647014
    },
    {
        "content": "<p>it's possible that sqlite locks are the actual problem</p>",
        "id": 233209757,
        "sender_full_name": "simulacrum",
        "timestamp": 1617647023
    },
    {
        "content": "<p>aha, ok, so we do in fact have several threads</p>",
        "id": 233210277,
        "sender_full_name": "simulacrum",
        "timestamp": 1617647264
    },
    {
        "content": "<p>I am struggling on this relatively old environment to get a concise listing, but it seems like at least there's two tokio-runtime threads, one of which is using ~100% cpu</p>",
        "id": 233210909,
        "sender_full_name": "simulacrum",
        "timestamp": 1617647579
    },
    {
        "content": "<p>hm, it <em>seems</em> like for whatever reason the usage of that one tokio thread is causing requests to timeout to the server, which seems unexpected</p>",
        "id": 233212994,
        "sender_full_name": "simulacrum",
        "timestamp": 1617648625
    },
    {
        "content": "<p>unfortunately we don't have tokio logs at all</p>",
        "id": 233213242,
        "sender_full_name": "simulacrum",
        "timestamp": 1617648745
    },
    {
        "content": "<p>I'm not sure if there's something we can do to inspect state at runtime, I don't really want to restart this log pushing</p>",
        "id": 233213274,
        "sender_full_name": "simulacrum",
        "timestamp": 1617648765
    },
    {
        "content": "<p>it does look like we're doing a ton of disk i/o (lseek + read syscalls)</p>",
        "id": 233214072,
        "sender_full_name": "simulacrum",
        "timestamp": 1617649136
    },
    {
        "content": "<p>it looks like we're writing ~15k crate logs in ~20 minutes, which  is roughly 12.5 logs / second... that feels maybe slow?</p>",
        "id": 233215439,
        "sender_full_name": "simulacrum",
        "timestamp": 1617649734
    },
    {
        "content": "<p>perf top seems to point at ~20% being in sqlite exec and ~20% in kernel copy_user_enhanced_fast_string (plus 10% read/llseek)</p>",
        "id": 233215511,
        "sender_full_name": "simulacrum",
        "timestamp": 1617649779
    },
    {
        "content": "<p>so I'll let that run, because it doesn't look like it's feasible to concurrently run that with a crater build, even though I can't fully tell why we're not serving requests nicely on the other thread</p>",
        "id": 233216769,
        "sender_full_name": "simulacrum",
        "timestamp": 1617650418
    },
    {
        "content": "<p>needs another hour or so</p>",
        "id": 233216800,
        "sender_full_name": "simulacrum",
        "timestamp": 1617650431
    },
    {
        "content": "<p>I'm wondering if this might be a symptom of using tokio 0.1 still</p>",
        "id": 233217700,
        "sender_full_name": "simulacrum",
        "timestamp": 1617650898
    },
    {
        "content": "<p>but in any case, not diving into updating that now</p>",
        "id": 233217768,
        "sender_full_name": "simulacrum",
        "timestamp": 1617650947
    },
    {
        "content": "<p>FWIW <a href=\"http://docs.rs\">docs.rs</a> has had issues with FD leaks in the past, I would be surprised if those fixes were backported</p>",
        "id": 233217784,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1617650955
    },
    {
        "content": "<p>but it sounds like this is CPU related, not a resource leak</p>",
        "id": 233217801,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1617650964
    },
    {
        "content": "<p>I don't think that's at all related to the problems we're seeing here</p>",
        "id": 233217813,
        "sender_full_name": "simulacrum",
        "timestamp": 1617650971
    },
    {
        "content": "<p>I'm going to try enabling logs before kicking off the next report</p>",
        "id": 233218360,
        "sender_full_name": "simulacrum",
        "timestamp": 1617651204
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"121055\">@Pietro Albini</span> can you comment on your availability for reviews etc on crater? It feels important to me to get it to a point where we don't need to baby sit it as much, and depending on if you're available I can see us doing that via code improvement (e.g., trying to upgrade tokio, which likely has some code changes required) or by resizing the machine things run on, if not</p>",
        "id": 233221952,
        "sender_full_name": "simulacrum",
        "timestamp": 1617653011
    },
    {
        "content": "<p>I'm also happy to own reviewing and merging things, but don't know how comfortable you are with that</p>",
        "id": 233221978,
        "sender_full_name": "simulacrum",
        "timestamp": 1617653033
    },
    {
        "content": "<p>and.... it seems like the majority of the slowness came from metrics queries?</p>",
        "id": 233235490,
        "sender_full_name": "simulacrum",
        "timestamp": 1617658965
    },
    {
        "content": "<p>or that's my operating theory, which is downright weird</p>",
        "id": 233235506,
        "sender_full_name": "simulacrum",
        "timestamp": 1617658975
    },
    {
        "content": "<p>I'm going to disable those temporarily and see if that helps</p>",
        "id": 233235559,
        "sender_full_name": "simulacrum",
        "timestamp": 1617659012
    },
    {
        "content": "<p>definitely seeing some 500-900ms queries with trace logs enabled, though it's not obvious why</p>",
        "id": 233235861,
        "sender_full_name": "simulacrum",
        "timestamp": 1617659171
    },
    {
        "content": "<p>SELECT * FROM experiments INNER JOIN experiment_crates ON experiment_crates.experiment = <a href=\"http://experiments.name\">experiments.name</a> WHERE experiment_crates.assigned_to = ?1 AND experiment_crates.status = ?2 AND experiments.status = ?2 AND experiment_crates.skipped = 0 LIMIT 1 seems particularly slow</p>",
        "id": 233235922,
        "sender_full_name": "simulacrum",
        "timestamp": 1617659216
    },
    {
        "content": "<p>ok, I've disabled metrics collection for now, and restarted the 4 crater instances as well as report generatino</p>",
        "id": 233237695,
        "sender_full_name": "simulacrum",
        "timestamp": 1617660235
    },
    {
        "content": "<p>things seem to be largely healthy</p>",
        "id": 233237713,
        "sender_full_name": "simulacrum",
        "timestamp": 1617660242
    },
    {
        "content": "<p>going to see how cpu usage does now that we're not hitting the metrics endpoint constantly</p>",
        "id": 233237786,
        "sender_full_name": "simulacrum",
        "timestamp": 1617660270
    },
    {
        "content": "<p>so far seems to be doing good, will check back in an hour (well, actually, sit here staring at graphs in all probability because I want to refresh constantly now)</p>",
        "id": 233238683,
        "sender_full_name": "simulacrum",
        "timestamp": 1617660792
    },
    {
        "content": "<p>unfortunately this means I didn't get to investigating the crater stoppage (i.e. when crater agents, not the webserver, fail), but my hope is that I can dedicate time to this soon.</p>",
        "id": 233240089,
        "sender_full_name": "simulacrum",
        "timestamp": 1617661688
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/YLmW8_8jqJiWJKNb8WtwGn_p/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/YLmW8_8jqJiWJKNb8WtwGn_p/image.png\" title=\"image.png\"><img src=\"/user_uploads/4715/YLmW8_8jqJiWJKNb8WtwGn_p/image.png\"></a></div>",
        "id": 233253008,
        "sender_full_name": "simulacrum",
        "timestamp": 1617671799
    },
    {
        "content": "<p>Looking much better after monitoring was turned off; still some spikes in CPU usage which are concerning, but overall no longer seeing the near-100% constant usage. I suspect the spikes might be the zulip archiving.</p>",
        "id": 233253036,
        "sender_full_name": "simulacrum",
        "timestamp": 1617671842
    },
    {
        "content": "<p>Monitoring became what it swore to destroy...</p>",
        "id": 233253109,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1617671904
    },
    {
        "content": "<p>FWIW, I'm pretty sure that the monitoring lacks some kind of exponential backoff or whatever, so when queries started failing due to high load (around the 2pm mark in that graph) I suspect it was just disconnecting before actually getting a response back - I saw 499 status in the logs</p>",
        "id": 233253160,
        "sender_full_name": "simulacrum",
        "timestamp": 1617672001
    },
    {
        "content": "<p>(and likely retrying sooner than might be desired, but I don't know about that)</p>",
        "id": 233253212,
        "sender_full_name": "simulacrum",
        "timestamp": 1617672020
    },
    {
        "content": "<p>catching up with the thread</p>",
        "id": 233295694,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617704126
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20issues/near/233253160\">said</a>:</p>\n<blockquote>\n<p>FWIW, I'm pretty sure that the monitoring lacks some kind of exponential backoff or whatever, so when queries started failing due to high load (around the 2pm mark in that graph) I suspect it was just disconnecting before actually getting a response back - I saw 499 status in the logs</p>\n</blockquote>\n<p>prometheus is currently configured to scrape the metrics every 5 seconds</p>",
        "id": 233295785,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617704168
    },
    {
        "content": "<p>we could add a <code>scrape_interval: 30s</code> or even <code>scrape_interval: 1m</code> to just <a href=\"https://github.com/rust-lang/simpleinfra/blob/fefff4d492c02388091c543ef8921c4fd98e0fb0/ansible/playbooks/monitoring.yml#L76\">the crater monitoring job</a></p>",
        "id": 233295885,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617704238
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20issues/near/233221952\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"121055\">Pietro Albini</span> can you comment on your availability for reviews etc on crater? It feels important to me to get it to a point where we don't need to baby sit it as much, and depending on if you're available I can see us doing that via code improvement (e.g., trying to upgrade tokio, which likely has some code changes required) or by resizing the machine things run on, if not</p>\n</blockquote>\n<p>I can definitely review crater PRs unless they're absolutely huge to review</p>",
        "id": 233296001,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617704302
    },
    {
        "content": "<p>for the slowness and crashes in generating the reports, my understanding is that it's due to a couple of issues:</p>\n<ul>\n<li>we generate the log archives and <code>full.html</code> loading everything in memory, which can OOM if we have too many things to put there</li>\n<li>uploading to s3 is currently single-thread, we don't do concurrent uploads</li>\n</ul>",
        "id": 233296368,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617704558
    },
    {
        "content": "<p>I think the easiest temporary fix for the reports is to just discard all the logs for the categories not included in <code>summary.html</code></p>",
        "id": 233296428,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617704604
    },
    {
        "content": "<p>actually, let me dump all this info in github issues</p>",
        "id": 233296518,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617704643
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> did a dump of the issues I see in <a href=\"https://github.com/rust-lang/crater/labels/reliability\">https://github.com/rust-lang/crater/labels/reliability</a></p>",
        "id": 233302538,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617708482
    },
    {
        "content": "<p>need to eat something, will check back later</p>",
        "id": 233302586,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617708490
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"121055\">Pietro Albini</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20issues/near/233295885\">said</a>:</p>\n<blockquote>\n<p>we could add a <code>scrape_interval: 30s</code> or even <code>scrape_interval: 1m</code> to just <a href=\"https://github.com/rust-lang/simpleinfra/blob/fefff4d492c02388091c543ef8921c4fd98e0fb0/ansible/playbooks/monitoring.yml#L76\">the crater monitoring job</a></p>\n</blockquote>\n<p>I think for now I'd like to hold off on adding this -- we should look at the queries needed to fulfill that request, as they're also used for e.g. the agents page, and optimize those. I don't think the metrics are super critical at this juncture.</p>",
        "id": 233317803,
        "sender_full_name": "simulacrum",
        "timestamp": 1617715553
    },
    {
        "content": "<p>I think for now I'm guessing that we should be stable-ish as is, primarily want to get the crater agent stalls fixed, but whether that's with the replacement with a more minimal graph (or, well, more of a list) or something more targeted, I think we'll see</p>",
        "id": 233318013,
        "sender_full_name": "simulacrum",
        "timestamp": 1617715626
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20issues/near/233317803\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"121055\">Pietro Albini</span> <a href=\"#narrow/stream/242791-t-infra/topic/crater.20issues/near/233295885\">said</a>:</p>\n<blockquote>\n<p>we could add a <code>scrape_interval: 30s</code> or even <code>scrape_interval: 1m</code> to just <a href=\"https://github.com/rust-lang/simpleinfra/blob/fefff4d492c02388091c543ef8921c4fd98e0fb0/ansible/playbooks/monitoring.yml#L76\">the crater monitoring job</a></p>\n</blockquote>\n<p>I think for now I'd like to hold off on adding this -- we should look at the queries needed to fulfill that request, as they're also used for e.g. the agents page, and optimize those. I don't think the metrics are super critical at this juncture.</p>\n</blockquote>\n<p>well, the metrics are useful to get alerts when an agent deadlocks</p>",
        "id": 233330187,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617720137
    },
    {
        "content": "<p>if we scrape them with a long frequency we'll be able to detect whether the agents deadlocked or not without impacting the server too much</p>",
        "id": 233330455,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617720263
    },
    {
        "content": "<p>I think we could bump granularity to something like 5 minutes maybe</p>",
        "id": 233334252,
        "sender_full_name": "simulacrum",
        "timestamp": 1617721388
    },
    {
        "content": "<p>at least one minute</p>",
        "id": 233334294,
        "sender_full_name": "simulacrum",
        "timestamp": 1617721394
    },
    {
        "content": "<p>I think one minute is fine</p>",
        "id": 233341561,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617723267
    },
    {
        "content": "<p>I'll enable those in a bit at 1 minute then</p>",
        "id": 233341625,
        "sender_full_name": "simulacrum",
        "timestamp": 1617723300
    },
    {
        "content": "<p>thanks <span aria-label=\"heart\" class=\"emoji emoji-2764\" role=\"img\" title=\"heart\">:heart:</span></p>",
        "id": 233341691,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617723335
    },
    {
        "content": "<p>seems to be doing ok:</p>\n<p><a href=\"/user_uploads/4715/2k9se66bLbkd2dhHwSpBPBpe/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/2k9se66bLbkd2dhHwSpBPBpe/image.png\" title=\"image.png\"><img src=\"/user_uploads/4715/2k9se66bLbkd2dhHwSpBPBpe/image.png\"></a></div>",
        "id": 233345415,
        "sender_full_name": "simulacrum",
        "timestamp": 1617724768
    },
    {
        "content": "<p>but definitely not insignificant addition to our load</p>",
        "id": 233345457,
        "sender_full_name": "simulacrum",
        "timestamp": 1617724786
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"121055\">@Pietro Albini</span> next time crater stalls out, can you ping me and not reboot it? I'd like to try to capture a backtrace and such</p>",
        "id": 233489893,
        "sender_full_name": "simulacrum",
        "timestamp": 1617803182
    },
    {
        "content": "<p>sure!</p>",
        "id": 233489920,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617803194
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"121055\">@Pietro Albini</span> okay, gcp-1 is stalled out right now, and I think I found the bug - <a href=\"https://github.com/rust-lang/crater/pull/569\">https://github.com/rust-lang/crater/pull/569</a></p>",
        "id": 233853414,
        "sender_full_name": "simulacrum",
        "timestamp": 1617986052
    },
    {
        "content": "<p>I can restart it but was going to ping in case you wanted to investigate as well, I think that PR should hopefully fix at least one case of stalled workers though.</p>",
        "id": 233853475,
        "sender_full_name": "simulacrum",
        "timestamp": 1617986078
    },
    {
        "content": "<p>(to avoid slowing down our queue since it's pretty long and since this happens not infrequently I'll restart in an hour or two regardless)</p>",
        "id": 233853546,
        "sender_full_name": "simulacrum",
        "timestamp": 1617986107
    },
    {
        "content": "<p>please restart, I'll be able to take a look at that PR later today or early tomorrow</p>",
        "id": 233853774,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617986197
    },
    {
        "content": "<p>thanks for drilling down into this <span aria-label=\"heart\" class=\"emoji emoji-2764\" role=\"img\" title=\"heart\">:heart:</span></p>",
        "id": 233853804,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1617986210
    },
    {
        "content": "<p>sounds good</p>",
        "id": 233853842,
        "sender_full_name": "simulacrum",
        "timestamp": 1617986229
    },
    {
        "content": "<p>it seems <a href=\"https://crater.rust-lang.org/\">https://crater.rust-lang.org/</a> is showing 502s again</p>",
        "id": 233958360,
        "sender_full_name": "lqd",
        "timestamp": 1618064138
    },
    {
        "content": "<p>yeah, fixing it</p>",
        "id": 233965195,
        "sender_full_name": "simulacrum",
        "timestamp": 1618068948
    },
    {
        "content": "<p>awesome, thanks a bunch :)</p>",
        "id": 233965760,
        "sender_full_name": "lqd",
        "timestamp": 1618069459
    },
    {
        "content": "<p>ok, seems like there's another deadlock - crater-azure-2 has my patch but stalled out on all workers in the blocked state as far as I can tell</p>",
        "id": 234049881,
        "sender_full_name": "simulacrum",
        "timestamp": 1618151915
    },
    {
        "content": "<p>trying to investigate before restarting</p>",
        "id": 234050587,
        "sender_full_name": "simulacrum",
        "timestamp": 1618152480
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>ure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z ERROR crater::runner::worker] task failed, marking childs as failed too: doc beta-2021-03-27 of crate sheesy-cli-4.0.11\nure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z ERROR crater::utils] No such file or directory (os error 2)\nure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z ERROR crater::utils] note: run with `RUST_BACKTRACE=1` to display a backtrace.\nure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z INFO  rustwide::cmd] [stdout] 26f27f650d6db6f9a05caab6fb26b2b64aa53b47709d67f161ee63d189a5bd3b\nure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z TRACE crater::runner::graph] worker-11 | NodeIndex(3754) prevented recursive mark_as_failed as it has other parents\nure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z DEBUG crater::runner::graph] marking task doc beta-2021-03-27 of crate sheesy-cli-4.0.11 as failed\nure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z ERROR crater::runner::tasks] this task or one of its parent failed!\nure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z ERROR crater::utils] No such file or directory (os error 2)\nure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z ERROR crater::utils] note: run with `RUST_BACKTRACE=1` to display a backtrace.\nure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z INFO  crater::agent::results] sending results to the crater server...\n</code></pre></div>",
        "id": 234052145,
        "sender_full_name": "simulacrum",
        "timestamp": 1618153345
    },
    {
        "content": "<p>is the interesting piece of the log</p>",
        "id": 234052196,
        "sender_full_name": "simulacrum",
        "timestamp": 1618153361
    },
    {
        "content": "<p>as far as I can tell, that unit in the task graph was not marked as completed by this line <a href=\"https://github.com/rust-lang/crater/blob/e8f8ace1476107d4bd01df73f0645bba1b2451c0/src/runner/graph.rs#L241\">https://github.com/rust-lang/crater/blob/e8f8ace1476107d4bd01df73f0645bba1b2451c0/src/runner/graph.rs#L241</a></p>",
        "id": 234052242,
        "sender_full_name": "simulacrum",
        "timestamp": 1618153381
    },
    {
        "content": "<p>but since the log shows the marking task as failed line, that implies that we early-exited from that function in the ? on task.mark_as_failed</p>",
        "id": 234052545,
        "sender_full_name": "simulacrum",
        "timestamp": 1618153491
    },
    {
        "content": "<p>I'm trying to verify that theory now</p>",
        "id": 234052558,
        "sender_full_name": "simulacrum",
        "timestamp": 1618153496
    },
    {
        "content": "<p>according to the logs on the coordination server, two record-progress requests at 03:50:28Z came in from this machine; the first took 3.648 seconds due to a concurrent metrics request, and the second took 0.113 seconds (if I'm interpreting the log right). The returned status on both is 200.</p>",
        "id": 234053008,
        "sender_full_name": "simulacrum",
        "timestamp": 1618153725
    },
    {
        "content": "<p>I'm not sure why there's two requests though, as the log on the crater machine seems to only indicate one</p>",
        "id": 234053039,
        "sender_full_name": "simulacrum",
        "timestamp": 1618153752
    },
    {
        "content": "<p>database doesn't seem to have a record of these requests, at least as far as I can tell- </p>\n<div class=\"codehilite\"><pre><span></span><code>sqlite&gt; select * from results where experiment = &#39;beta-1.52-rustdoc-1&#39; and crate like &#39;%sheesy-cli%&#39;;\nexperiment,crate,toolchain,result,log,encoding\nbeta-1.52-rustdoc-1,reg/sheesy-cli/4.0.11,1.51.0,error,&quot;,gzip\n</code></pre></div>",
        "id": 234053761,
        "sender_full_name": "simulacrum",
        "timestamp": 1618154346
    },
    {
        "content": "<p>(that's the run of this crate from the before version, not the beta-... version seen failing above)</p>",
        "id": 234053783,
        "sender_full_name": "simulacrum",
        "timestamp": 1618154374
    },
    {
        "content": "<p>I guess I should look at the API endpoint to see if it can return 200 while failing</p>",
        "id": 234054634,
        "sender_full_name": "simulacrum",
        "timestamp": 1618155034
    },
    {
        "content": "<p>er, I misread the HTTP log - there were <em>3</em> requests, not 2, from the azure-2 machine</p>",
        "id": 234054860,
        "sender_full_name": "simulacrum",
        "timestamp": 1618155222
    },
    {
        "content": "<p>and the crater server log also has the same</p>",
        "id": 234054870,
        "sender_full_name": "simulacrum",
        "timestamp": 1618155236
    },
    {
        "content": "<p>which is pretty weird</p>",
        "id": 234054977,
        "sender_full_name": "simulacrum",
        "timestamp": 1618155286
    },
    {
        "content": "<p>aha, I have an idea now</p>",
        "id": 234055293,
        "sender_full_name": "simulacrum",
        "timestamp": 1618155568
    },
    {
        "content": "<p>the timestamp in the nginx logs is the <em>end</em> of the request most likely</p>",
        "id": 234055301,
        "sender_full_name": "simulacrum",
        "timestamp": 1618155577
    },
    {
        "content": "<p>and indeed both of the first two took ~3 seconds to process, so they're unrelated to this, likely issued ~3 seconds earlier</p>",
        "id": 234055389,
        "sender_full_name": "simulacrum",
        "timestamp": 1618155629
    },
    {
        "content": "<p>and the agent log has two progress's sent at :24, so those are probably these</p>",
        "id": 234055413,
        "sender_full_name": "simulacrum",
        "timestamp": 1618155656
    },
    {
        "content": "<p>ok, so that means we sent a single progress report, no retries</p>",
        "id": 234055424,
        "sender_full_name": "simulacrum",
        "timestamp": 1618155669
    },
    {
        "content": "<p>there's a bunch of early exits but I don't really know if they could be responsible for the failure to record the result in the db, or maybe I'm even reading the db wrong</p>",
        "id": 234055599,
        "sender_full_name": "simulacrum",
        "timestamp": 1618155803
    },
    {
        "content": "<p>ok, so we think the crate is running</p>",
        "id": 234056555,
        "sender_full_name": "simulacrum",
        "timestamp": 1618156592
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>sqlite&gt; select * from experiment_crates where experiment = &#39;beta-1.52-rustdoc-1&#39; and crate like &#39;%sheesy-cli%&#39;;\nexperiment,crate,skipped,status,assigned_to\nbeta-1.52-rustdoc-1,reg/sheesy-cli/4.0.11,0,running,agent:azure-2\n</code></pre></div>",
        "id": 234056652,
        "sender_full_name": "simulacrum",
        "timestamp": 1618156699
    },
    {
        "content": "<p>I'm going to see if I can find the other half of the crate in the agent logs</p>",
        "id": 234056674,
        "sender_full_name": "simulacrum",
        "timestamp": 1618156735
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>Apr 11 03:50:28 crater-azure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z ERROR crater::runner::worker] task failed, marking childs as failed too: doc beta-2021-03-27 of crate sheesy-cli-4.0.11\nApr 11 03:50:28 crater-azure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:28Z DEBUG crater::runner::graph] marking task doc beta-2021-03-27 of crate sheesy-cli-4.0.11 as failed\nApr 11 03:50:30 crater-azure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:30Z ERROR crater::runner::worker] task failed, marking childs as failed too: doc 1.51.0 of crate sheesy-cli-4.0.11\nApr 11 03:50:30 crater-azure-2.infra.rust-lang.org docker[57987]: [2021-04-11T03:50:30Z DEBUG crater::runner::graph] marking task doc 1.51.0 of crate sheesy-cli-4.0.11 as failed\n</code></pre></div>",
        "id": 234056705,
        "sender_full_name": "simulacrum",
        "timestamp": 1618156781
    },
    {
        "content": "<p>ok so both parts failed</p>",
        "id": 234056708,
        "sender_full_name": "simulacrum",
        "timestamp": 1618156794
    },
    {
        "content": "<p>and we seem to have the http request on the nginx and app logs on the server</p>",
        "id": 234056903,
        "sender_full_name": "simulacrum",
        "timestamp": 1618156940
    },
    {
        "content": "<p>but neither has recorded a result?</p>",
        "id": 234056909,
        "sender_full_name": "simulacrum",
        "timestamp": 1618156946
    },
    {
        "content": "<p>oh, I guess no, the second one did</p>",
        "id": 234057201,
        "sender_full_name": "simulacrum",
        "timestamp": 1618157244
    },
    {
        "content": "<p>but the first one didn't</p>",
        "id": 234057211,
        "sender_full_name": "simulacrum",
        "timestamp": 1618157265
    },
    {
        "content": "<p>and I guess the second one is not present in the later logs, so it presumably successfully deleted itself from the task graph as well</p>",
        "id": 234057261,
        "sender_full_name": "simulacrum",
        "timestamp": 1618157292
    },
    {
        "content": "<p>oh, I have a thought - if I get the threads to exit, the logs should get the error message logged</p>",
        "id": 234057988,
        "sender_full_name": "simulacrum",
        "timestamp": 1618158091
    },
    {
        "content": "<p>hm ok I think I failed to do that</p>",
        "id": 234058108,
        "sender_full_name": "simulacrum",
        "timestamp": 1618158194
    },
    {
        "content": "<p>oh well</p>",
        "id": 234058181,
        "sender_full_name": "simulacrum",
        "timestamp": 1618158267
    },
    {
        "content": "<p>anyway, restarted azure-2, will post a PR shortly that should help with this</p>",
        "id": 234058188,
        "sender_full_name": "simulacrum",
        "timestamp": 1618158279
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/crater/pull/570\">https://github.com/rust-lang/crater/pull/570</a>, also adds some logging to help track down the cause (which I still don't know)</p>",
        "id": 234059471,
        "sender_full_name": "simulacrum",
        "timestamp": 1618159571
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"121055\">@Pietro Albini</span> do you mind if I deploy the logging change (in the second commit in this case, but also more generally in the future to help debug future cases) without review by you?</p>",
        "id": 234070712,
        "sender_full_name": "simulacrum",
        "timestamp": 1618170463
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> thanks for the ping, approved</p>",
        "id": 234070909,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1618170641
    },
    {
        "content": "<p>FWIW I have not seen further stalls after these two bugs were fixed</p>",
        "id": 234508794,
        "sender_full_name": "simulacrum",
        "timestamp": 1618410823
    },
    {
        "content": "<p>which seems positive!</p>",
        "id": 234508818,
        "sender_full_name": "simulacrum",
        "timestamp": 1618410832
    },
    {
        "content": "<p>otoh, I'm a bit surprised, as my guess is we didn't fix <em>everything</em></p>",
        "id": 234508915,
        "sender_full_name": "simulacrum",
        "timestamp": 1618410858
    },
    {
        "content": "<p>so I think the old policy of \"don't restart if it goes down\" should continue to be the case <span class=\"user-mention\" data-user-id=\"121055\">@Pietro Albini</span> -- I want to iron out any remaining bugs as they arise</p>",
        "id": 234508998,
        "sender_full_name": "simulacrum",
        "timestamp": 1618410885
    },
    {
        "content": "<p>gcp-1 and gcp-2 are marked as unreachable</p>",
        "id": 238182067,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1620668286
    },
    {
        "content": "<p>Known, thanks - it'll likely be some time before they're brought back up at this point</p>",
        "id": 238182230,
        "sender_full_name": "simulacrum",
        "timestamp": 1620668365
    },
    {
        "content": "<p>One <em>could</em> say they... cratered</p>",
        "id": 238182771,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1620668570
    },
    {
        "content": "<p>The job <a href=\"https://crater.rust-lang.org/ex/pr-84920\">https://crater.rust-lang.org/ex/pr-84920</a> has no agents assigned, but isn't complete</p>",
        "id": 238608920,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1620905079
    },
    {
        "content": "<p>hmm</p>",
        "id": 238618777,
        "sender_full_name": "simulacrum",
        "timestamp": 1620911581
    },
    {
        "content": "<p>I wonder if that's because of the crater stuff</p>",
        "id": 238618788,
        "sender_full_name": "simulacrum",
        "timestamp": 1620911587
    },
    {
        "content": "<p>let me see if prioritizing that run will let it finish up</p>",
        "id": 238618802,
        "sender_full_name": "simulacrum",
        "timestamp": 1620911594
    },
    {
        "content": "<p>ok, prioritized, will check back in later</p>",
        "id": 238619068,
        "sender_full_name": "simulacrum",
        "timestamp": 1620911724
    },
    {
        "content": "<p>oh I think why this is happening</p>",
        "id": 238619084,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620911734
    },
    {
        "content": "<p>some chunks of the distributed experiments are still assigned to the gcp agents</p>",
        "id": 238619114,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620911748
    },
    {
        "content": "<p>oh, perhaps</p>",
        "id": 238619131,
        "sender_full_name": "simulacrum",
        "timestamp": 1620911760
    },
    {
        "content": "<p>yep</p>",
        "id": 238619577,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912000
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>sqlite&gt; select assigned_to, count(*) from experiment_crates where status = &#39;running&#39; group by assigned_to;\nagent:azure-1|12\nagent:azure-2|299\nagent:gcp-1|324\nagent:gcp-2|954\nsqlite&gt;\n</code></pre></div>",
        "id": 238619590,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912003
    },
    {
        "content": "<p>ran</p>\n<div class=\"codehilite\" data-code-language=\"SQL\"><pre><span></span><code><span class=\"k\">update</span> <span class=\"n\">experiment_crates</span> <span class=\"k\">set</span> <span class=\"n\">status</span> <span class=\"o\">=</span> <span class=\"s1\">'queued'</span><span class=\"p\">,</span> <span class=\"n\">assigned_to</span> <span class=\"o\">=</span> <span class=\"k\">null</span> <span class=\"k\">where</span> <span class=\"n\">status</span> <span class=\"o\">=</span> <span class=\"s1\">'running'</span> <span class=\"k\">and</span> <span class=\"n\">assigned_to</span> <span class=\"o\">=</span> <span class=\"s1\">'agent:gcp-1'</span><span class=\"p\">;</span>\n<span class=\"k\">update</span> <span class=\"n\">experiment_crates</span> <span class=\"k\">set</span> <span class=\"n\">status</span> <span class=\"o\">=</span> <span class=\"s1\">'queued'</span><span class=\"p\">,</span> <span class=\"n\">assigned_to</span> <span class=\"o\">=</span> <span class=\"k\">null</span> <span class=\"k\">where</span> <span class=\"n\">status</span> <span class=\"o\">=</span> <span class=\"s1\">'running'</span> <span class=\"k\">and</span> <span class=\"n\">assigned_to</span> <span class=\"o\">=</span> <span class=\"s1\">'agent:gcp-2'</span><span class=\"p\">;</span>\n</code></pre></div>",
        "id": 238619742,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912074
    },
    {
        "content": "<p>hopefully this will fix it</p>",
        "id": 238619764,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912077
    },
    {
        "content": "<p>the next time one of the azure agents finishes a chunk they should pick some of those crates up</p>",
        "id": 238619833,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912110
    },
    {
        "content": "<p>btw <span class=\"user-mention\" data-user-id=\"125294\">@Aaron Hill</span>, could we keep a single topic for crater issues to keep the topics manageable?</p>",
        "id": 238620091,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912234
    },
    {
        "content": "<p>Sure :)</p>",
        "id": 238620146,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1620912248
    },
    {
        "content": "<p>thanks <span aria-label=\"heart\" class=\"emoji emoji-2764\" role=\"img\" title=\"heart\">:heart:</span></p>",
        "id": 238620162,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912255
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>sqlite&gt; select experiment, assigned_to, count(*) from experiment_crates where status = &#39;running&#39; group by assigned_to, experiment;\nbeta-1.53-1|agent:azure-1|1013\nbeta-1.53-1|agent:azure-2|251\nsqlite&gt;\n</code></pre></div>",
        "id": 238620180,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912259
    },
    {
        "content": "<p>current status</p>",
        "id": 238620193,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912261
    },
    {
        "content": "<p>once azure-2 goes to zero it should pick up the older run</p>",
        "id": 238620238,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912279
    },
    {
        "content": "<p>I'll file an issue for not hitting this in the future (we should deschedule things from unreachable nodes or something)</p>",
        "id": 238620321,
        "sender_full_name": "simulacrum",
        "timestamp": 1620912319
    },
    {
        "content": "<p>definitely, thanks for doing so mark!</p>",
        "id": 238620379,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620912352
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/crater/issues/577\">https://github.com/rust-lang/crater/issues/577</a></p>",
        "id": 238620634,
        "sender_full_name": "simulacrum",
        "timestamp": 1620912481
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>sqlite&gt; select experiment, assigned_to, count(*) from experiment_crates where status = &#39;running&#39; group by assigned_to, experiment;\nbeta-1.53-1|agent:azure-1|297\npr-84920|agent:azure-2|712\nsqlite&gt;\n</code></pre></div>",
        "id": 238628498,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620916066
    },
    {
        "content": "<p>that seems to have worked <span class=\"user-mention\" data-user-id=\"125294\">@Aaron Hill</span>!</p>",
        "id": 238628573,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1620916089
    },
    {
        "content": "<p>How long are the two gcp agents expected to remain down for? I have a PR at the end of the queue, and at this rate, it's going to be a very long time before it gets a chance to run</p>",
        "id": 239302247,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1621362354
    },
    {
        "content": "<p>We don't currently have a timeline, but I suspect that we'll arrange for more capacity somehow if the queue gets (even) longer.</p>",
        "id": 239303901,
        "sender_full_name": "simulacrum",
        "timestamp": 1621362945
    },
    {
        "content": "<p>The queue now has 6 jobs, three of which are <code>cargo build</code> / <code>cargo test</code></p>",
        "id": 240002477,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1621829992
    },
    {
        "content": "<p>yeah :(</p>",
        "id": 240080923,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1621876179
    },
    {
        "content": "<p>we're figuring out what to do on this topic</p>",
        "id": 240080937,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1621876187
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"125294\">@Aaron Hill</span> we temporarily added an aws agent while we figure out a more permanent solution <span aria-label=\"tada\" class=\"emoji emoji-1f389\" role=\"img\" title=\"tada\">:tada:</span></p>",
        "id": 240445981,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1622111476
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/2TC-mqWgIWhF-oEp0eUnohGm/Screenshot-from-2021-05-27-12-30-31.png\">Screenshot-from-2021-05-27-12-30-31.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/2TC-mqWgIWhF-oEp0eUnohGm/Screenshot-from-2021-05-27-12-30-31.png\" title=\"Screenshot-from-2021-05-27-12-30-31.png\"><img src=\"/user_uploads/4715/2TC-mqWgIWhF-oEp0eUnohGm/Screenshot-from-2021-05-27-12-30-31.png\"></a></div>",
        "id": 240445990,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1622111482
    },
    {
        "content": "<p>note that the agent might go down for a bit as it's an aws spot instance</p>",
        "id": 240446110,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1622111542
    },
    {
        "content": "<p>we have monitoring in place so we shouldn't need a ping when it's down :)</p>",
        "id": 240446136,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1622111562
    },
    {
        "content": "<p><a href=\"https://crater.rust-lang.org/\">https://crater.rust-lang.org/</a> is giving a 502</p>",
        "id": 265500560,
        "sender_full_name": "Aaron Hill",
        "timestamp": 1639948680
    },
    {
        "content": "<p>restarted</p>",
        "id": 265501955,
        "sender_full_name": "simulacrum",
        "timestamp": 1639950674
    }
]