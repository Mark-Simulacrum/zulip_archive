[
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/issues/82832\">#82832</a> timed out. Today there way at least one other timeout on the i686-gnu job.</p>",
        "id": 229112443,
        "sender_full_name": "bjorn3",
        "timestamp": 1615050034
    },
    {
        "content": "<p>I ran the core tests on i686 in a loop for an hour, and wasn't able to reproduce any hangs. Hopefully it is a CI fluke.  There is a concerning miscompilation in <a href=\"https://github.com/rust-lang/rust/issues/82833\">#82833</a>, but that appears to require a specific collection of flags not used in normal doctests.</p>",
        "id": 229137488,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615072598
    },
    {
        "content": "<p>It hangs quite consistently <a href=\"https://github.com/rust-lang/rust/issues/82221\">#82221</a>. See also a list of cancelled jobs that took longer than 4h in <a href=\"https://github.com/rust-lang-ci/rust/actions?query=is%3Acancelled\">https://github.com/rust-lang-ci/rust/actions?query=is%3Acancelled</a></p>",
        "id": 229167981,
        "sender_full_name": "tm",
        "timestamp": 1615104637
    },
    {
        "content": "<p>I couldn't reproduce this locally neither. On CI, all hangs seems to be tests that spawn rustc. Usually doc tests, but also occasionally UI tests (presuming the issue is the same).</p>",
        "id": 229168162,
        "sender_full_name": "tm",
        "timestamp": 1615104836
    },
    {
        "content": "<p>Thanks @tm for the link. I think I saw that issue when you opened it, but I forgot about it.</p>\n<p>I tried running again using Docker, and was able to reproduce:</p>\n<div class=\"codehilite\"><pre><span></span><code>test src/char/methods.rs - char::methods::char::escape_debug (line 455) has been running for over 60 seconds\ntest src/char/methods.rs - char::methods::char::escape_unicode (line 371) has been running for over 60 seconds\ntest src/char/methods.rs - char::methods::char::from_digit (line 205) has been running for over 60 seconds\ntest src/char/methods.rs - char::methods::char::from_digit (line 220) has been running for over 60 seconds\ntest src/char/methods.rs - char::methods::char::from_digit (line 230) has been running for over 60 seconds\ntest src/char/methods.rs - char::methods::char::is_alphabetic (line 689) has been running for over 60 seconds\ntest src/char/methods.rs - char::methods::char::is_alphanumeric (line 807) has been running for over 60 seconds\ntest src/char/methods.rs - char::methods::char::is_ascii (line 1054) has been running for over 60 seconds\ntest src/char/methods.rs - char::methods::char::is_ascii_alphabetic (line 1213) has been running for over 60 seconds\n</code></pre></div>",
        "id": 229206591,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615140837
    },
    {
        "content": "<p>All of the rustc processes are hung on reading stdin (on <a href=\"https://github.com/rust-lang/rust/blob/66ec64ccf31883cd2c28d045912a76179c0c6ed2/compiler/rustc_driver/src/lib.rs#L493\">this line</a>).</p>",
        "id": 229207545,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615141695
    },
    {
        "content": "<p>Does the container mount all the filesystems it should?</p>",
        "id": 229207827,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615141978
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120518\">@Eric Huss</span> If you still have the container running, can you check the <code>/proc/$PID/fd</code> directories for those rustc processes to see what fd 0 refers to?</p>",
        "id": 229207968,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615142059
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span>  I was able to run the tests many times without error, so I assume everything is mounted correctly.  Also, this is an extremely simple docker setup, with very few options.</p>\n<p>The fd for stdin of rustc is a pipe to rustdoc: <code>0 -&gt; pipe:[11708854]</code></p>",
        "id": 229208295,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615142387
    },
    {
        "content": "<p>Can you check lsof to figure out who has the other end open?</p>",
        "id": 229208376,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615142432
    },
    {
        "content": "<p>Yea, rustdoc is hung in spawn:</p>\n<div class=\"codehilite\"><pre><span></span><code>#0  0xf7f70569 in __kernel_vsyscall ()\n#1  0xeee29be2 in __lll_lock_wait () at ../sysdeps/unix/sysv/linux/i386/lowlevellock.S:144\n#2  0xeee26571 in __GI___pthread_rwlock_unlock (rwlock=0xef01a124 &lt;_ZN3std3sys4unix2os8ENV_LOCK17h937e2955b3dbd3d2E.llvm.373065096730365974&gt;)\n    at pthread_rwlock_unlock.c:42\n#3  0xeeee3b4f in std::sys::unix::process::process_inner::_$LT$impl$u20$std..sys..unix..process..process_common..Command$GT$::spawn::h61a7f768f84ef457 ()\n   from /checkout/obj/build/i686-unknown-linux-gnu/stage1/bin/../lib/libstd-901ad3224252f4d9.so\n</code></pre></div>",
        "id": 229208405,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615142466
    },
    {
        "content": "<p>If it's hanging, that suggests that someone has the pipe write end open and hasn't closed it.</p>",
        "id": 229208419,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615142471
    },
    {
        "content": "<p>No line numbers, but my first guess is that rwlock is the env_lock.</p>",
        "id": 229208422,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615142478
    },
    {
        "content": "<p>Interesting!</p>",
        "id": 229208427,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615142492
    },
    {
        "content": "<p>The unix spawn code is a little tricky, in the way it handles locks around <code>fork()</code>.</p>",
        "id": 229208439,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615142521
    },
    {
        "content": "<p>Oh, to be clear, there are two rustdoc processes.</p>\n<p>The second one has a single thread, which is blocked on that call to unlock.  So I assume that is just after the call to <code>fork</code>.</p>",
        "id": 229208538,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615142608
    },
    {
        "content": "<p>That's very strange. I'm wondering why it can't just snapshot the environment and set it post-fork.</p>",
        "id": 229209238,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615143102
    },
    {
        "content": "<p>There is some pretty gnarly interaction of posix rwlock and threads and forking.  I ran across <a href=\"https://stackoverflow.com/questions/61976745/why-does-rust-rwlock-behave-unexpectedly-with-fork\">https://stackoverflow.com/questions/61976745/why-does-rust-rwlock-behave-unexpectedly-with-fork</a> which explains some of it.</p>",
        "id": 229209488,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615143352
    },
    {
        "content": "<p>I notice that it is not using <code>posix_spawn</code>, and instead is using fork/exec. I wonder why...</p>",
        "id": 229209538,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615143376
    },
    {
        "content": "<p>And yeah, dropping env_read_lock in the child when it wasn't taken in the child seems like a bug, unless env_read_lock is something really unusual.</p>",
        "id": 229209662,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615143535
    },
    {
        "content": "<p>Ah, glibc is 2.23, and it requires &gt;=2.24.</p>",
        "id": 229209667,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615143541
    },
    {
        "content": "<p>Which it doesn't appear to be...</p>",
        "id": 229209799,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615143640
    },
    {
        "content": "<p>This is still a bug in the case where fork is used, even if using posix_spawn would work around it for rustdoc.</p>",
        "id": 229209944,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615143747
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> If I'm reading this correctly, it is wrong for the child process to call <code>pthread_rwlock_unlock</code> because its thread number will be different from where it was acquired, right?  Like, it should only call <code>pthread_rwlock_unlock</code> in the parent?</p>",
        "id": 229210126,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615143955
    },
    {
        "content": "<p>Right, but the lock may still appear held in the child.</p>",
        "id": 229210191,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615143988
    },
    {
        "content": "<p>It just can't be unlocked safely.</p>",
        "id": 229210199,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615144000
    },
    {
        "content": "<p>Oh, this is a <em>recent</em> change.</p>",
        "id": 229210899,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615144565
    },
    {
        "content": "<p>Commit <a href=\"https://github.com/rust-lang/rust/commit/55ca27faa78434d81d3f59535c96bbb2a08f0d5c\">55ca27faa78434d81d3f59535c96bbb2a08f0d5c</a> last month switched this to an rwlock.</p>",
        "id": 229210927,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615144591
    },
    {
        "content": "<p>I'm guessing that we didn't have this CI issue before that commit.</p>",
        "id": 229210950,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615144650
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/pull/81850\">https://github.com/rust-lang/rust/pull/81850</a></p>",
        "id": 229211028,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615144722
    },
    {
        "content": "<p>Yeah, that makes sense. Unlocking a mutex post-fork probably works (though I don't know if it's defined behavior or not).</p>",
        "id": 229211167,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615144847
    },
    {
        "content": "<p>Unlocking an rwlock post-fork doesn't, because of the thread-ID.</p>",
        "id": 229211192,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615144858
    },
    {
        "content": "<p>There may be ways to improve this further, but for the moment, I think the right approach may be to revert that series of commits (including on stable if it's been in a stable release).</p>",
        "id": 229211318,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615144979
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/242791-t-infra/topic/i686-gnu.20timeouts/near/229211318\">said</a>:</p>\n<blockquote>\n<p>There may be ways to improve this further, but for the moment, I think the right approach may be to revert that series of commits (including on stable if it's been in a stable release).</p>\n</blockquote>\n<p>fortunately, it's on nightly (according to the milestone)</p>",
        "id": 229211457,
        "sender_full_name": "Yuki Okushi",
        "timestamp": 1615145132
    },
    {
        "content": "<p>Ah, good.</p>",
        "id": 229211462,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615145144
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120518\">@Eric Huss</span> Do you want to prepare a revert and I'll review, or vice versa?</p>",
        "id": 229211465,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615145157
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> Sure</p>",
        "id": 229211564,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615145233
    },
    {
        "content": "<p>(which way around?)</p>",
        "id": 229211572,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615145243
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> haha, I'll post a PR soon.</p>",
        "id": 229211653,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615145325
    },
    {
        "content": "<p>Alright. I can write a commit message if that helps:</p>",
        "id": 229211655,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615145337
    },
    {
        "content": "<p>I want to run a stress test first just to ensure this actually solves the problem.</p>",
        "id": 229211677,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615145352
    },
    {
        "content": "<p><span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 229211684,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615145366
    },
    {
        "content": "<p>Commit message:</p>\n<p>Revert switch of env locking to rwlock, to fix deadlock in process spawning</p>\n<p>PR <a href=\"https://github.com/rust-lang/rust/pull/81850\">https://github.com/rust-lang/rust/pull/81850</a> switched the environment lock from a mutex to an rwlock. However, process spawning (when not able to use posix_spawn) locks the environment before forking, and unlocks it after forking (in both the parent and the child). With a mutex, this works. With an rwlock, on at least some targets, unlocking in the child does not work correctly, resulting in a deadlock.</p>\n<p>This has manifested as CI hangs on i686 Linux; that target doesn't use posix_spawn in the CI environment due to the age of the installed C library. (Switching to posix_spawn would just mask this issue, though, which would still arise in any case that can't use posix_spawn.)</p>\n<p>Some additional cleanup of environment handling around process spawning may help, but for now, revert the PR and go back to a standard mutex.</p>",
        "id": 229212283,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615145887
    },
    {
        "content": "<p>While neither rwlock nor mutex is correct here (corresponding functions are not async-signal-safe, so cannot be used after fork), I wonder what breaks exactly. The rwlock stores a thread id of a writer, while this code obtains read lock.</p>",
        "id": 229215509,
        "sender_full_name": "tm",
        "timestamp": 1615148895
    },
    {
        "content": "<p>In 2.23, to unlock an rwlock the implementation obtains an internal lock. It is possible that another thread obtained the lock at fork time, so it will remain locked. The implementation looks quite differently now, which is probably why I couldn't reproduce this.</p>",
        "id": 229217453,
        "sender_full_name": "tm",
        "timestamp": 1615150841
    },
    {
        "content": "<p>Yea, I was just taking a look, and the lll_lock code is in assembly in that version.</p>",
        "id": 229217727,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615151091
    },
    {
        "content": "<p>Posted <a href=\"https://github.com/rust-lang/rust/issues/82877\">#82877</a></p>",
        "id": 229218304,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615151606
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"120518\">@Eric Huss</span> LGTM, r+ed.</p>",
        "id": 229219084,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615152344
    },
    {
        "content": "<p>This topic was moved by <span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> to <a class=\"stream-topic\" data-stream-id=\"219381\" href=\"/#narrow/stream/219381-t-libs/topic/environment.2C.20locking.2C.20fork.2C.20and.20Command\">#t-libs &gt; environment, locking, fork, and Command</a></p>",
        "id": 229225574,
        "sender_full_name": "Notification Bot",
        "timestamp": 1615158636
    },
    {
        "content": "<p>Getting spurious CI failures trying to merge the commit that'll fix unrelated spurious CI failures...</p>",
        "id": 229344908,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615226331
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/pBJx1WKVJ05l2JtLM6m_mwWX/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/pBJx1WKVJ05l2JtLM6m_mwWX/image.png\" title=\"image.png\"><img src=\"/user_uploads/4715/pBJx1WKVJ05l2JtLM6m_mwWX/image.png\"></a></div>",
        "id": 229344929,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1615226347
    },
    {
        "content": "<p>Maybe attempt <a href=\"https://github.com/rust-lang/rust/issues/6\">#6</a> will be more lucky. <span aria-label=\"stuck out tongue wink\" class=\"emoji emoji-1f61c\" role=\"img\" title=\"stuck out tongue wink\">:stuck_out_tongue_wink:</span></p>",
        "id": 229358166,
        "sender_full_name": "Eric Huss",
        "timestamp": 1615231171
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/issues/88379\">#88379</a> seems likely to time out again, this time with <code>i686-gnu</code> (<code>s390x-linux</code> did pass though!). I noticed this thread from several months ago, so I figured I'd post here since the case I ran into in my PR looks similar. It could be a different issue though, given that the one discussed here seems to have been fixed.</p>",
        "id": 256836833,
        "sender_full_name": "Noah Lev",
        "timestamp": 1633752028
    },
    {
        "content": "<p>Strangely, there are no build logs visible (this is the <a href=\"https://github.com/rust-lang-ci/rust/runs/3843901917\">builder run</a> that will likely time out), which makes me think this could be related to <a href=\"#narrow/stream/242791-t-infra/topic/i686-gnu*.20jobs.20keep.20timing.20out/near/227119644\">this thread</a>.</p>",
        "id": 256837005,
        "sender_full_name": "Noah Lev",
        "timestamp": 1633752209
    },
    {
        "content": "<p>Yep, bors timed out. The builder is still running; probably it'll stop soon. And then hopefully there'll be a build log.</p>",
        "id": 256838397,
        "sender_full_name": "Noah Lev",
        "timestamp": 1633753609
    },
    {
        "content": "<p>Ok, build logs are now available: <a href=\"https://github.com/rust-lang-ci/rust/runs/3843901917\">https://github.com/rust-lang-ci/rust/runs/3843901917</a></p>",
        "id": 256881992,
        "sender_full_name": "Noah Lev",
        "timestamp": 1633797573
    },
    {
        "content": "<p>This is the last bit of the log:</p>\n<div class=\"codehilite\"><pre><span></span><code>Sat, 09 Oct 2021 00:45:24 GMT test src/time.rs - time::FromSecsError (line 1219) ... ok\nSat, 09 Oct 2021 00:45:24 GMT test src/time.rs - time::Duration::try_from_secs_f32 (line 772) ... ok\nSat, 09 Oct 2021 00:45:24 GMT test src/time.rs - time::Duration::try_from_secs_f64 (line 711) ... ok\nSat, 09 Oct 2021 00:45:24 GMT test src/unit.rs - unit::() (line 8) ... ok\nSat, 09 Oct 2021 05:25:25 GMT Error: The operation was canceled.\n</code></pre></div>\n<p>So it looks like whatever it started doing after running <code>src/unit.rs</code> is what timed out. Any idea what it might have been doing?</p>",
        "id": 256882089,
        "sender_full_name": "Noah Lev",
        "timestamp": 1633797655
    },
    {
        "content": "<p>Is it worth retrying the PR to see if that was a spurious timeout?</p>",
        "id": 256882109,
        "sender_full_name": "Noah Lev",
        "timestamp": 1633797675
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"307537\">@Noah Lev</span> Have you tried testing core with your PR locally?</p>",
        "id": 256886122,
        "sender_full_name": "Eric Huss",
        "timestamp": 1633801319
    },
    {
        "content": "<p>I did a quick diff, and I noticed that it had not yet printed <code>AtomicI64::from_mut</code> in the output.</p>",
        "id": 256886182,
        "sender_full_name": "Eric Huss",
        "timestamp": 1633801349
    },
    {
        "content": "<p>Normally when a test hangs, libtest will print a notice, so it is strange not to see that.</p>",
        "id": 256886192,
        "sender_full_name": "Eric Huss",
        "timestamp": 1633801361
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"120518\">Eric Huss</span> <a href=\"#narrow/stream/242791-t-infra/topic/i686-gnu.20timeouts/near/256886122\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"307537\">Noah Lev</span> Have you tried testing core with your PR locally?</p>\n</blockquote>\n<p>I have not, but every other builder passed, so it seems specific to this target.</p>",
        "id": 256886496,
        "sender_full_name": "Noah Lev",
        "timestamp": 1633801664
    },
    {
        "content": "<p>hm, good point.  I would just retry it and keep an eye out for similar timeouts.</p>",
        "id": 256888239,
        "sender_full_name": "Eric Huss",
        "timestamp": 1633803320
    },
    {
        "content": "<p>Will do, thanks!</p>",
        "id": 256890276,
        "sender_full_name": "Noah Lev",
        "timestamp": 1633805248
    },
    {
        "content": "<p><code>i686-gnu</code> has already finished on the new run, so I think it might have just been spurious before <span aria-label=\"tada\" class=\"emoji emoji-1f389\" role=\"img\" title=\"tada\">:tada:</span></p>",
        "id": 256894855,
        "sender_full_name": "Noah Lev",
        "timestamp": 1633809559
    },
    {
        "content": "<p>It succeeded! <span aria-label=\"tada\" class=\"emoji emoji-1f389\" role=\"img\" title=\"tada\">:tada:</span></p>",
        "id": 256903000,
        "sender_full_name": "Noah Lev",
        "timestamp": 1633817763
    }
]