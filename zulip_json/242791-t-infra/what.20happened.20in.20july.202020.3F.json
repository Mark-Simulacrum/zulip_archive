[
    {
        "content": "<p>looking a the prs-merged graph, we used to be able to merge 4-5 PRs a day for a long time (kinda like nowadays), then there was a huge speedup in end of july 2020 where the numbers went up to 8-12 prs a day (!!) and since then it has been slowly degrading back to 5-6 prs a day.</p>\n<p>Does anyone remember what happened in july back then and if we could so something  like that again? <span aria-label=\"big smile\" class=\"emoji emoji-1f604\" role=\"img\" title=\"big smile\">:big_smile:</span> </p>\n<p><a href=\"/user_uploads/4715/i2qZwtcd-LQ_TiQKOx-Zpxmo/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/i2qZwtcd-LQ_TiQKOx-Zpxmo/image.png\" title=\"image.png\"><img src=\"/user_uploads/4715/i2qZwtcd-LQ_TiQKOx-Zpxmo/image.png\"></a></div>",
        "id": 267975624,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1642140052
    },
    {
        "content": "<p>iirc that was when we moved from travis to github actions</p>",
        "id": 267983078,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642148224
    },
    {
        "content": "<p>the easiest way to speed CI up right now would be to split the apple jobs into multiple ones</p>",
        "id": 267983518,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642148528
    },
    {
        "content": "<p>so for example create <code>n</code> actual jobs for the dist-x86_64-apple and only create <code>1/n</code> of the dist tarballs in each builder</p>",
        "id": 267983608,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642148610
    },
    {
        "content": "<p>or splitting tests</p>",
        "id": 267983612,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642148616
    },
    {
        "content": "<p>CI stores a CPU utilization logfile, right? maybe we should check if that got worse over time, some sequential bottelneck somewhere.</p>",
        "id": 267983613,
        "sender_full_name": "The 8472",
        "timestamp": 1642148616
    },
    {
        "content": "<p>yeah, but like, there's no way the mac builders will be ever as fast as the x86_64 ones</p>",
        "id": 267983686,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642148645
    },
    {
        "content": "<p>they have 3 cores vs 8 cores</p>",
        "id": 267983696,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642148652
    },
    {
        "content": "<p>ouch</p>",
        "id": 267983717,
        "sender_full_name": "The 8472",
        "timestamp": 1642148670
    },
    {
        "content": "<p>oh yeah <span class=\"user-mention\" data-user-id=\"217864\">@matthiaskrgr</span>, at some point the gha macos builders went from 4 cores to 3 cores</p>",
        "id": 267983738,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642148686
    },
    {
        "content": "<p>which explains the recent drop</p>",
        "id": 267983748,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642148698
    },
    {
        "content": "<p>anyway, what would help right now is someone trying to rebalance those tasks across multiple parallel builders</p>",
        "id": 267985258,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642149656
    },
    {
        "content": "<p>last time I checked for the apple builders only an hour and something or so was spent building the host compiler</p>",
        "id": 267985287,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642149682
    },
    {
        "content": "<p>all the rest of the things can be probably split in multiple jobs</p>",
        "id": 267985303,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642149694
    },
    {
        "content": "<p>do we have some commits where this has been done in the past as cheatsheet?</p>",
        "id": 267986060,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1642150124
    },
    {
        "content": "<p>I don't think we have a split builder anywhere today</p>",
        "id": 267986381,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642150332
    },
    {
        "content": "<p>we had a few in the past though</p>",
        "id": 267986386,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642150337
    },
    {
        "content": "<p>I think a good plan would be to first try to identify <em>how</em> we could do the split</p>",
        "id": 267986490,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642150414
    },
    {
        "content": "<p>aka which jobs should be split and which tasks should run in each job</p>",
        "id": 267986503,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642150427
    },
    {
        "content": "<p>and then at monday's infra meeting we can decide how to best handle the split</p>",
        "id": 267986526,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642150439
    },
    {
        "content": "<p>could cargo be taught to give priority boosts to crate-rustcs that are on the critical path? That'd require some some hint file based on previous timings to identify the critical path in advance.</p>",
        "id": 268000045,
        "sender_full_name": "The 8472",
        "timestamp": 1642158831
    },
    {
        "content": "<p>hmm what do you mean?</p>",
        "id": 268002219,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642160150
    },
    {
        "content": "<p>different steps are not executed in parallel (while they might have internal parallelism)</p>",
        "id": 268002278,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642160169
    },
    {
        "content": "<p>I mean the rustc instances spawned by cargo within a step, e.g. when building std or rustc. If macos runners are that cpu-starved then getting the crate order wrong might lead to the critical path taking longer than necessary because non-critical crates take up cpu time first</p>",
        "id": 268002920,
        "sender_full_name": "The 8472",
        "timestamp": 1642160584
    },
    {
        "content": "<p>I think there are way easier wins</p>",
        "id": 268004349,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642161565
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/4tudxgovFx-SclO3cnXZXVDg/dist-x86_64-apple.html\">dist-x86_64-apple.html</a> this for example is the <code>dist-x86_64-apple</code> step</p>",
        "id": 268004439,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642161604
    },
    {
        "content": "<p>the \"serial\" (building stage1 + stage2) part ends at around 1.3hr</p>",
        "id": 268004564,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642161668
    },
    {
        "content": "<p>the remaining 1.7hr part could easily be split into three other builder, turning <code>dist-x86_64-apple</code> into a 2hr job</p>",
        "id": 268004638,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642161713
    },
    {
        "content": "<p>and same for all the other macos jobs (which have the same stage1+stage2 build at the start)</p>",
        "id": 268004760,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642161796
    },
    {
        "content": "<p>yeah, I guess that's easier. still, we're leaving 30% cpu time on the table. somewhat consistently somehow.</p>",
        "id": 268004934,
        "sender_full_name": "The 8472",
        "timestamp": 1642161888
    },
    {
        "content": "<p>I'm tempted to ask if it runs with -j2 on a 3 core machine.</p>",
        "id": 268004999,
        "sender_full_name": "The 8472",
        "timestamp": 1642161946
    },
    {
        "content": "<p>I don't think we configured that</p>",
        "id": 268005024,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642161962
    },
    {
        "content": "<p>anyway, <a href=\"https://gist.github.com/pietroalbini/5be40788e4d7c8e44c06121b4da0b0ab\">the script I used to generate that report</a></p>",
        "id": 268005073,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642161970
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/2rbLJCBRSs8lY0c4zGfXyg_g/dist-x86_64-apple.png\">dist-x86_64-apple.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/2rbLJCBRSs8lY0c4zGfXyg_g/dist-x86_64-apple.png\" title=\"dist-x86_64-apple.png\"><img src=\"/user_uploads/4715/2rbLJCBRSs8lY0c4zGfXyg_g/dist-x86_64-apple.png\"></a></div>",
        "id": 268005428,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642162236
    },
    {
        "content": "<p>the graph is pretty suspicious though, I give you that</p>",
        "id": 268005433,
        "sender_full_name": "Pietro Albini",
        "timestamp": 1642162244
    },
    {
        "content": "<p>I think it's worth to check what <code>num_cpus</code> thinks the thread count is. And if that reports 3 then run a small test program that burns CPU cycles on three threads for a few seconds and check the reported utilization on that.</p>",
        "id": 268005935,
        "sender_full_name": "The 8472",
        "timestamp": 1642162620
    },
    {
        "content": "<div class=\"message_inline_image\"><a href=\"https://user-images.githubusercontent.com/5047365/148451942-3c38d235-26f5-41ee-a20e-2b40c74260c9.png\"><img src=\"https://uploads.zulipusercontent.net/3f388888ce47408ac5951f0fc22c81f9a433237a/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f353034373336352f3134383435313934322d33633338643233352d323666352d343165652d613230652d3262343063373432363063392e706e67\"></a></div>",
        "id": 268012800,
        "sender_full_name": "simulacrum",
        "timestamp": 1642166795
    },
    {
        "content": "<p>Those are the graphs for apple builders over the last several months, sourced precisely from those cpu utilization files</p>",
        "id": 268012861,
        "sender_full_name": "simulacrum",
        "timestamp": 1642166844
    },
    {
        "content": "<p><a href=\"https://user-images.githubusercontent.com/5047365/148460936-776b85ea-5b59-439a-8bd7-d1cec906ab4e.png\">https://user-images.githubusercontent.com/5047365/148460936-776b85ea-5b59-439a-8bd7-d1cec906ab4e.png</a> for comparison with a Linux builder included</p>\n<div class=\"message_inline_image\"><a href=\"https://user-images.githubusercontent.com/5047365/148460936-776b85ea-5b59-439a-8bd7-d1cec906ab4e.png\"><img src=\"https://uploads.zulipusercontent.net/0df3dc0036867219eaa0ca3b253d1c03e27d28ae/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f353034373336352f3134383436303933362d37373662383565612d356235392d343339612d386264372d6431636563393036616234652e706e67\"></a></div>",
        "id": 268012895,
        "sender_full_name": "simulacrum",
        "timestamp": 1642166869
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"330154\">@The 8472</span> <a href=\"https://github.com/rust-lang/cargo/issues/7396\">https://github.com/rust-lang/cargo/issues/7396</a></p>",
        "id": 268013949,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1642167417
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> huh... what's that jump on the linux runner?<br>\nosx utilization being higher makes sense if it has fewer cores. at least I'm not seeing any big negatively correlated events where build time went up because utilization got worse.</p>",
        "id": 268015339,
        "sender_full_name": "The 8472",
        "timestamp": 1642168057
    },
    {
        "content": "<p>The jump is us starting to build llvm twice(?), for every build, because sccache doesn't support the pgo options yet.</p>",
        "id": 268015487,
        "sender_full_name": "simulacrum",
        "timestamp": 1642168109
    },
    {
        "content": "<p>Maybe once. Not sure.</p>",
        "id": 268015590,
        "sender_full_name": "simulacrum",
        "timestamp": 1642168172
    },
    {
        "content": "<p>So the averages don't change all that much over time, maybe 0.4% so </p>\n<p><span class=\"user-mention silent\" data-user-id=\"121055\">Pietro Albini</span> <a href=\"#narrow/stream/242791-t-infra/topic/what.20happened.20in.20july.202020.3F/near/268005428\">said</a>:</p>\n<blockquote>\n<p><a href=\"/user_uploads/4715/2rbLJCBRSs8lY0c4zGfXyg_g/dist-x86_64-apple.png\">dist-x86_64-apple.png</a></p>\n</blockquote>\n<p>And the way it bottoms out in those more dense sample clusters. That <em>looks</em> suspiciously close to 66.6% (+ a little system load or something). I may just be looking for patterns in clouds, but that's a really weird cloud...</p>\n<p>The averages might just change a bit due to the non-bottlenecked parts, so they don't tell us too much.</p>",
        "id": 268017322,
        "sender_full_name": "The 8472",
        "timestamp": 1642169051
    },
    {
        "content": "<p>Well, maybe that's just situations where 2 crates building in parallel and they're each stuck on a single-threaded bottleneck and that kind of pattern is common or something? But then shouldn't we see things dropping down to 33% utilization too?</p>\n<p>Wait, are those 3C6T or 3C3T?</p>",
        "id": 268017878,
        "sender_full_name": "The 8472",
        "timestamp": 1642169314
    },
    {
        "content": "<p>I see at least one <code>2022-01-14T06:40:20.4313850Z running: \"cmake\" \"--build\" \".\" \"--target\" \"install\" \"--config\" \"Release\" \"--\" \"-j\" \"3\"</code></p>",
        "id": 268017916,
        "sender_full_name": "simulacrum",
        "timestamp": 1642169339
    },
    {
        "content": "<p>3c3t, I believe</p>",
        "id": 268017968,
        "sender_full_name": "simulacrum",
        "timestamp": 1642169357
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>2022-01-14T06:36:50.5397260Z     Hardware Overview:\n2022-01-14T06:36:50.5397420Z\n2022-01-14T06:36:50.5397600Z       Model Name: Mac\n2022-01-14T06:36:50.5397850Z       Model Identifier: VMware7,1\n2022-01-14T06:36:50.5398910Z       Processor Name: Unknown\n2022-01-14T06:36:50.5399300Z       Processor Speed: 3.33 GHz\n2022-01-14T06:36:50.5399550Z       Number of Processors: 1\n2022-01-14T06:36:50.5399800Z       Total Number of Cores: 3\n2022-01-14T06:36:50.5400100Z       L2 Cache (per Core): 256 KB\n2022-01-14T06:36:50.5400420Z       L3 Cache: 12 MB\n2022-01-14T06:36:50.5400640Z       Memory: 14 GB\n2022-01-14T06:36:50.5400920Z       System Firmware Version: VMW71.00V.13989454.B64.1906190538\n2022-01-14T06:36:50.5401320Z       Apple ROM Info: [MS_VM_CERT/SHA1/27d66596a61c48dd3dc7216fd715126e33f59ae7]Welcome to the Virtual Machine\n2022-01-14T06:36:50.5401670Z       SMC Version (system): 2.8f0\n2022-01-14T06:36:50.5401950Z       Serial Number (system): VMh6VWuHw3oI\n2022-01-14T06:36:50.5402690Z       Hardware UUID: 4203018E-580F-C1B5-9525-B745CECA79EB\n2022-01-14T06:36:50.5403370Z       Provisioning UDID: 4203018E-580F-C1B5-9525-B745CECA79EB\n</code></pre></div>",
        "id": 268018020,
        "sender_full_name": "simulacrum",
        "timestamp": 1642169388
    },
    {
        "content": "<blockquote>\n<p>If macos runners are that cpu-starved then getting the crate order wrong might lead to the critical path taking longer than necessary because non-critical crates take up cpu time first</p>\n</blockquote>\n<p>Can you run a <code>-Ztimings</code> to demonstrate this behavior?</p>",
        "id": 268025791,
        "sender_full_name": "Eh2406",
        "timestamp": 1642173060
    },
    {
        "content": "<p>That'd need a reference point to compare to, how fast each crate would be under ideal conditions (-j1 I guess? Assuming cargo only spawns rustc instances when job tokens are available)</p>",
        "id": 268026194,
        "sender_full_name": "The 8472",
        "timestamp": 1642173219
    },
    {
        "content": "<p>FWIW, I recently collected these timings for building rustc_query_impl specifically (extracted from CI logs):</p>\n<ul>\n<li>85 seconds &lt;- ryzen 3600x -j1 + 2 rustc threads (perf.rlo)</li>\n<li>153 seconds &lt;- ryzen 3600x -j1 (my local machine)</li>\n<li>150 seconds &lt;- ryzen 3950x -j1 (server)</li>\n<li>303 seconds &lt;- macbook pro 2015 (2.5GHz -j1, for x86_64-apple-darwin)</li>\n<li>640 seconds &lt;- github ci (dist-aarch64-apple, compiling for x86_64-apple-darwin, part of a -j3 build)</li>\n</ul>\n<p>I'm not sure the extent to which this is helpful/significant though.</p>",
        "id": 268027594,
        "sender_full_name": "simulacrum",
        "timestamp": 1642173781
    },
    {
        "content": "<p>What's \"+ 2 rustc threads\"?</p>",
        "id": 268028245,
        "sender_full_name": "The 8472",
        "timestamp": 1642174037
    },
    {
        "content": "<p>I thought we're not building parallel rustc</p>",
        "id": 268028285,
        "sender_full_name": "The 8472",
        "timestamp": 1642174055
    },
    {
        "content": "<p>Anyway, being slower than a 2015 laptop despite more threads seems wrong. Are those VMs oversubscribed? Or on slow storage?</p>",
        "id": 268028510,
        "sender_full_name": "The 8472",
        "timestamp": 1642174147
    },
    {
        "content": "<p>well, one is just building that crate, the other is building other crates in parallel</p>",
        "id": 268028912,
        "sender_full_name": "simulacrum",
        "timestamp": 1642174296
    },
    {
        "content": "<p>the 2 threads is that we allocate 2 jobserver tokens per rustc (one implicit, one extra)</p>",
        "id": 268029171,
        "sender_full_name": "simulacrum",
        "timestamp": 1642174413
    },
    {
        "content": "<p>do you happen to know why dist-x86_64-apple appears to be building a bunch of tools twice?  like cargo, clippy, etc?</p>",
        "id": 268029769,
        "sender_full_name": "Eric Huss",
        "timestamp": 1642174680
    },
    {
        "content": "<p>hm</p>",
        "id": 268030141,
        "sender_full_name": "simulacrum",
        "timestamp": 1642174834
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/cargo/issues/7437#issuecomment-535543233\">https://github.com/rust-lang/cargo/issues/7437#issuecomment-535543233</a></p>\n<blockquote>\n<p>If I run cargo clean then cargo build -p script, the total time is 319 seconds whereas in the full build graph the script crate finishes after second 560. So as far as I can tell thatâ€™s ~240 seconds worth of work that is not a dependency of script, and that could (with different scheduling) be done in parallel with it.</p>\n</blockquote>\n<p>script is in the critical path in that case.</p>",
        "id": 268030720,
        "sender_full_name": "The 8472",
        "timestamp": 1642175044
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"120518\">Eric Huss</span> <a href=\"#narrow/stream/242791-t-infra/topic/what.20happened.20in.20july.202020.3F/near/268029769\">said</a>:</p>\n<blockquote>\n<p>do you happen to know why dist-x86_64-apple appears to be building a bunch of tools twice?  like cargo, clippy, etc?</p>\n</blockquote>\n<p>I think we're <em>running</em> that step multiple times due to <code>./x.py dist --exclude rust-docs --exclude extended &amp;&amp; ./x.py dist --target=x86_64-apple-darwin rust-docs &amp;&amp; ./x.py dist extended</code> being the build command</p>",
        "id": 268031719,
        "sender_full_name": "simulacrum",
        "timestamp": 1642175438
    },
    {
        "content": "<p>where both dist::Cargo and dist::Extended are DEFAULT, so the first --exclude doesn't really skip it</p>",
        "id": 268031820,
        "sender_full_name": "simulacrum",
        "timestamp": 1642175483
    },
    {
        "content": "<p>why that leads to a re-compile of cargo instead of reusing cached artifacts, not sure</p>",
        "id": 268031851,
        "sender_full_name": "simulacrum",
        "timestamp": 1642175496
    },
    {
        "content": "<p>it might make sense to just x.py dist and not bother with the excludes</p>",
        "id": 268031896,
        "sender_full_name": "simulacrum",
        "timestamp": 1642175519
    },
    {
        "content": "<p>I can probably take a look later today or this weekend to come up with a better strategy there.  That could save a significant amount of time.</p>\n<p>If we decide to go with <a href=\"https://github.com/rust-lang/rust/issues/92800\">#92800</a>, I'll also follow up afterwards to remove docs from aarch64-apple which I think should save another 5 minutes or so.</p>",
        "id": 268032937,
        "sender_full_name": "Eric Huss",
        "timestamp": 1642175961
    },
    {
        "content": "<p>FWIW, this was 'changed' fairly recently, in <a href=\"https://github.com/rust-lang/rust/pull/90100\">https://github.com/rust-lang/rust/pull/90100</a></p>",
        "id": 268032999,
        "sender_full_name": "simulacrum",
        "timestamp": 1642176000
    },
    {
        "content": "<p>and looking at the graph it's not obvious that had any impact</p>",
        "id": 268033053,
        "sender_full_name": "simulacrum",
        "timestamp": 1642176007
    },
    {
        "content": "<p>(it also didn't touch the slowest builder, dist-aarch64-apple, at all)</p>",
        "id": 268033210,
        "sender_full_name": "simulacrum",
        "timestamp": 1642176059
    },
    {
        "content": "<p>yea, that makes sense.   </p>\n<p>I can also comb over dist-aarch64-apple to see if there is anything interesting. My instinct is that it shouldn't be that slow.</p>",
        "id": 268033485,
        "sender_full_name": "Eric Huss",
        "timestamp": 1642176172
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/sziAhgrfmYRCRgSUY5Ot0QSq/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/sziAhgrfmYRCRgSUY5Ot0QSq/image.png\" title=\"image.png\"><img src=\"/user_uploads/4715/sziAhgrfmYRCRgSUY5Ot0QSq/image.png\"></a></div>",
        "id": 268037857,
        "sender_full_name": "simulacrum",
        "timestamp": 1642177947
    },
    {
        "content": "<p>alt builder is much faster, though it obviously is building less</p>",
        "id": 268037877,
        "sender_full_name": "simulacrum",
        "timestamp": 1642177957
    },
    {
        "content": "<p>Taking a closer look, it seems like docs are taking about 15 minutes on dist-aarch64-apple.  I'd need to do some real testing, but that would be a good chunk of time.</p>",
        "id": 268091998,
        "sender_full_name": "Eric Huss",
        "timestamp": 1642206389
    },
    {
        "content": "<p>Looking over the log for dist-aarch64-apple, some other thoughts about improving it:</p>\n<ul>\n<li>LLVM takes 23 minutes. I've never really understood why it uses sccache instead of just downloading an archive.</li>\n<li>I'm uncertain if it is necessary to build rustc 3 times. I would expect that it should be sufficient to build for x86_64 once, and then use that to build the final aarch64 rustc. That would cut 24 minutes.</li>\n<li>Sanitizers get built for x86_64, cutting that would be about 2.5 minutes.</li>\n<li>Building the combined installer takes 7.7 minutes. That seems exceedingly long, and probably worth more investigation.</li>\n<li>rustbuild itself could be more parallel. There are a bunch of projects that don't depend on one another that are built serially. Since there are huge chunks of time where they are using just 1 cpu, that's a lot of parallelism wasted. Of course that would make rustbuild substantially more complex. We could entertain using a better build system. <span aria-label=\"wink\" class=\"emoji emoji-1f609\" role=\"img\" title=\"wink\">:wink:</span></li>\n</ul>",
        "id": 268103061,
        "sender_full_name": "Eric Huss",
        "timestamp": 1642219429
    },
    {
        "content": "<p>I offered the last point before but there wasn't much appetite for it</p>",
        "id": 268115373,
        "sender_full_name": "The 8472",
        "timestamp": 1642237276
    },
    {
        "content": "<p>Did a dumb CPU load test under gnu <code>time</code> on a <code>macos-latest</code> runner</p>\n<p>run1:</p>\n<div class=\"codehilite\"><pre><span></span><code>Thread 0 finished in 12697 ms\nThread 2 finished in 12695 ms\nThread 1 finished in 12709 ms\nMain finished in 12709 ms\n37.54user 0.06system 0:12.84elapsed 292%CPU (0avgtext+0avgdata 848maxresident)k\n0inputs+0outputs (0major+7493minor)pagefaults 0swaps\n</code></pre></div>\n<p>run2:</p>\n<div class=\"codehilite\"><pre><span></span><code>Thread 1 finished in 28701 ms\nThread 0 finished in 28826 ms\nThread 2 finished in 28890 ms\nMain finished in 28891 ms\n37.95user 0.40system 0:29.16elapsed 131%CPU (0avgtext+0avgdata 844maxresident)k\n0inputs+0outputs (0major+7216minor)pagefaults 0swaps\n</code></pre></div>\n<p>I guess some machines are oversubscribed? Or maybe the test is too short and there's some background stuff running.<br>\nIs there a way to print the equivalent of <em>steal time</em> on macos?</p>",
        "id": 268121852,
        "sender_full_name": "The 8472",
        "timestamp": 1642247378
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"120518\">Eric Huss</span> <a href=\"#narrow/stream/242791-t-infra/topic/what.20happened.20in.20july.202020.3F/near/268029769\">said</a>:</p>\n<blockquote>\n<p>do you happen to know why dist-x86_64-apple appears to be building a bunch of tools twice?  like cargo, clippy, etc?</p>\n</blockquote>\n<p>It seems like dependency vendoring changes some dep paths which invalidates fingerpints <span aria-label=\"thinking\" class=\"emoji emoji-1f914\" role=\"img\" title=\"thinking\">:thinking:</span> <br>\n<a href=\"https://github.com/rust-lang/rust/issues/93033\">https://github.com/rust-lang/rust/issues/93033</a></p>",
        "id": 268404026,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1642518303
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>Building stage1 tool cargo (x86_64-unknown-linux-gnu)\n[2022-01-18T14:45:58Z INFO  cargo::core::compiler::fingerprint] stale: changed &quot;/home/matthias/.cargo/registry/src/github.com-1ecc6299db9ec823/curl-sys-0.4.51+curl-7.80.0/curl&quot;\n[2022-01-18T14:45:58Z INFO  cargo::core::compiler::fingerprint]           (vs) &quot;/home/matthias/vcs/github/rust/build/x86_64-unknown-linux-gnu/stage1-tools/x86_64-unknown-linux-gnu/release/build/curl-sys-92c84ce359fc681e/output&quot;\n[2022-01-18T14:45:58Z INFO  cargo::core::compiler::fingerprint]                FileTime { seconds: 1642516164, nanos: 851099564 } != FileTime { seconds: 1642516869, nanos: 931076291 }\n</code></pre></div>",
        "id": 268404113,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1642518341
    },
    {
        "content": "<p>or at least this is what I suspect..</p>",
        "id": 268404277,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1642518395
    },
    {
        "content": "<p>yeah, so it seems that <code>dist::PlainSourceTarball</code> somehow changes how all the the following <code>dist::${tool}</code> steps perceive their deps.<br>\nWhen I comment out the PlainSourceTarball step, tools are not rebuilt a second time.</p>\n<p>I'm wondering if we can simply move the PlainSourceTarball step to  when tools are built already or if that would break something.</p>",
        "id": 268434902,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1642530276
    },
    {
        "content": "<p>opened <a href=\"https://github.com/rust-lang/rust/pull/93047\">https://github.com/rust-lang/rust/pull/93047</a> to move the PlainSourceTarball lower in the dist pipeline</p>",
        "id": 268452798,
        "sender_full_name": "matthiaskrgr",
        "timestamp": 1642538655
    }
]