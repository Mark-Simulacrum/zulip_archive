[
    {
        "content": "<p>I'm afraid I caused the perfbot to hang (see <a href=\"https://perf.rust-lang.org/status.html\">https://perf.rust-lang.org/status.html</a>). It's chewing on a 30s test since 90 minutes</p>",
        "id": 221243200,
        "sender_full_name": "oli",
        "timestamp": 1609359499
    },
    {
        "content": "<p>can someone kill it?</p>",
        "id": 221243214,
        "sender_full_name": "oli",
        "timestamp": 1609359506
    },
    {
        "content": "<p>Taking a look</p>",
        "id": 221243440,
        "sender_full_name": "simulacrum",
        "timestamp": 1609359718
    },
    {
        "content": "<p>my guess is your code is has like n^3 or something</p>",
        "id": 221243571,
        "sender_full_name": "simulacrum",
        "timestamp": 1609359841
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"124288\">@oli</span> killed</p>",
        "id": 221243781,
        "sender_full_name": "simulacrum",
        "timestamp": 1609360024
    },
    {
        "content": "<p>thanks</p>",
        "id": 221243790,
        "sender_full_name": "oli",
        "timestamp": 1609360032
    },
    {
        "content": "<p>I think my code is stuck in an infinite loop</p>",
        "id": 221243797,
        "sender_full_name": "oli",
        "timestamp": 1609360040
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/242791-t-infra/topic/perfbot/near/221243571\">said</a>:</p>\n<blockquote>\n<p>my guess is your code is has like n^3 or something</p>\n</blockquote>\n<p>better than rustdoc's n^4 for blanket impls <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span></p>",
        "id": 221243808,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1609360054
    },
    {
        "content": "<p>I've had that before <span aria-label=\"grinning\" class=\"emoji emoji-1f600\" role=\"img\" title=\"grinning\">:grinning:</span></p>",
        "id": 221243812,
        "sender_full_name": "oli",
        "timestamp": 1609360055
    },
    {
        "content": "<p>big-o notation is not to be treated as a <em>high score</em> <span aria-label=\"eyes\" class=\"emoji emoji-1f440\" role=\"img\" title=\"eyes\">:eyes:</span></p>",
        "id": 221243920,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1609360148
    },
    {
        "content": "<p>just pretend this is golf, small numbers are better</p>",
        "id": 221243947,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1609360181
    },
    {
        "content": "<p>looks like it got stuck again</p>",
        "id": 221246298,
        "sender_full_name": "oli",
        "timestamp": 1609362183
    },
    {
        "content": "<p>I promise, no more perf runs on the inliner opt without first having done it locally</p>",
        "id": 221246333,
        "sender_full_name": "oli",
        "timestamp": 1609362207
    },
    {
        "content": "<p>Maybe there should be a max time per benchmark? If your code takes 300x as long you probably don't want to merge it anyway lol</p>",
        "id": 221246839,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1609362623
    },
    {
        "content": "<p>Yeah we should do it timeouts are hard</p>",
        "id": 221247346,
        "sender_full_name": "simulacrum",
        "timestamp": 1609363117
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"124288\">@oli</span> where are you seeing it again?</p>",
        "id": 221247382,
        "sender_full_name": "simulacrum",
        "timestamp": 1609363168
    },
    {
        "content": "<p>if you mean the rustc line that's just because we apparently don't check the \"success\" when recording previous time</p>",
        "id": 221247411,
        "sender_full_name": "simulacrum",
        "timestamp": 1609363201
    },
    {
        "content": "<p>ah it just moved on... rustc took like 40x as much time</p>",
        "id": 221247430,
        "sender_full_name": "oli",
        "timestamp": 1609363202
    },
    {
        "content": "<p>i.e. that's from a failing compile</p>",
        "id": 221247457,
        "sender_full_name": "simulacrum",
        "timestamp": 1609363216
    },
    {
        "content": "<p>stealing this topic to discuss a suggestion I made on twitter: We should add a suite of UI tests to the perf bot. For now, the speed of <em>failing</em> builds isn't tracked at all, and we should have UI tests for most if not all possible errors. I could imagine that we may want to track the different output formats as well as possibly check vs. buid, but I'm unsure iff the latter really makes any difference.</p>",
        "id": 229630033,
        "sender_full_name": "llogiq",
        "timestamp": 1615370016
    },
    {
        "content": "<p>Why would we care about the speed of a failing build?</p>",
        "id": 229668169,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1615386312
    },
    {
        "content": "<p>Because while developing, builds fail more often than not, and thus, the time saved with each perf boost could amortize even more than with succeeding ones.</p>",
        "id": 229713497,
        "sender_full_name": "llogiq",
        "timestamp": 1615399444
    },
    {
        "content": "<p>Hmm, I think the metric there should be \"time to first error\", not the total time to run</p>",
        "id": 229715115,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1615400086
    },
    {
        "content": "<p>And that varies wildly depending on what the error is</p>",
        "id": 229715243,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1615400132
    },
    {
        "content": "<p>E.g., name resolution errors show up <em>really</em> fastâ€”in just 5 minutes for <a href=\"https://github.com/rust-lang/rust/pull/82993/checks?check_run_id=2081786831\">https://github.com/rust-lang/rust/pull/82993/checks?check_run_id=2081786831</a> (which had an error late in the build, in rustdoc).</p>",
        "id": 229766456,
        "sender_full_name": "Noah Lev",
        "timestamp": 1615419107
    },
    {
        "content": "<p>Well, there's also \"time to last error\", which may also be of interest, but both aren't that easy to measure. So I'll take \"time to run\" as a workable approximation.</p>",
        "id": 229813034,
        "sender_full_name": "llogiq",
        "timestamp": 1615451806
    },
    {
        "content": "<p>To clarify, we may well want to get all those timings (time to first output, time to last output, time to completion), but time to completion is the easiest to get, so I propose we start there.</p>",
        "id": 229990932,
        "sender_full_name": "llogiq",
        "timestamp": 1615536638
    }
]