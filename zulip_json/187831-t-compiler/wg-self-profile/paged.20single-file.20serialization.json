[
    {
        "content": "<p>Hey everyone, over the last couple of days I've looking into implementing a single-file serialization format roughly as described in <a href=\"https://github.com/rust-lang/measureme/issues/128\">https://github.com/rust-lang/measureme/issues/128</a>.</p>",
        "id": 208941816,
        "sender_full_name": "mw",
        "timestamp": 1599125472
    },
    {
        "content": "<p>I have a working proof of concept implementation here: <a href=\"https://github.com/michaelwoerister/measureme/tree/paged_sinks\">https://github.com/michaelwoerister/measureme/tree/paged_sinks</a></p>",
        "id": 208941854,
        "sender_full_name": "mw",
        "timestamp": 1599125503
    },
    {
        "content": "<p>it implements two variants: </p>\n<ul>\n<li>a simpler one that writes pages to disk on the thread that does the recording (<a href=\"https://github.com/michaelwoerister/measureme/blob/paged_sinks/measureme/src/paged_serialization_sink2.rs\">https://github.com/michaelwoerister/measureme/blob/paged_sinks/measureme/src/paged_serialization_sink2.rs</a>), and </li>\n<li>a more sophisticated one that lets a background thread do the writing and clearing of pages (<a href=\"https://github.com/michaelwoerister/measureme/blob/paged_sinks/measureme/src/paged_serialization_sink.rs\">https://github.com/michaelwoerister/measureme/blob/paged_sinks/measureme/src/paged_serialization_sink.rs</a>).</li>\n</ul>",
        "id": 208942132,
        "sender_full_name": "mw",
        "timestamp": 1599125670
    },
    {
        "content": "<p>The background thread implementation is up to 5% faster than the simpler implementation in the micro benchmarks. But it is also more complicated.</p>",
        "id": 208942207,
        "sender_full_name": "mw",
        "timestamp": 1599125748
    },
    {
        "content": "<p>I'm not sure how valid the micro benchmarks are for assessing real-world performance in the compiler.</p>",
        "id": 208942296,
        "sender_full_name": "mw",
        "timestamp": 1599125795
    },
    {
        "content": "<p>Overall I think the paging approach works pretty well.</p>",
        "id": 208942980,
        "sender_full_name": "mw",
        "timestamp": 1599126248
    },
    {
        "content": "<p>(Credit goes to <span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span> for coming up with the idea)</p>",
        "id": 208943103,
        "sender_full_name": "mw",
        "timestamp": 1599126315
    },
    {
        "content": "<p>We can probably evaluate pretty well by running the perf suite and seeing how much time it takes - less or more, once we get to integration point.</p>\n<p>I am a bit worried about a background thread when we eventually look at parallel rustc more - but I suspect ultimately we can tackle that bridge then. Did we consider buffering everything in memory and then writing at the end? IIRC on perf.rlo we never ended up with more than a few hundred megabytes of data</p>",
        "id": 208951668,
        "sender_full_name": "simulacrum",
        "timestamp": 1599131945
    },
    {
        "content": "<p>I guess maybe a scheme that buffers say 5 million events or so and then does a small burst on the separate thread to record them feels like it might be a bit better</p>",
        "id": 208951788,
        "sender_full_name": "simulacrum",
        "timestamp": 1599132005
    },
    {
        "content": "<p>Yes, I'm not a big fan of libraries spawning off background threads either. OTOH, this kind of background worker would spend most of its time blocked waiting for new worked, or blocked waiting for the data to make it to the disk...</p>",
        "id": 208956233,
        "sender_full_name": "mw",
        "timestamp": 1599134711
    },
    {
        "content": "<blockquote>\n<p>Did we consider buffering everything in memory and then writing at the end?</p>\n</blockquote>\n<p>The <code>MmapSerializationSink</code> does this. It seems to perform pretty well and in multithreaded mode it blows all the other implementations out of the water because it does not need a <code>Mutex</code>.</p>",
        "id": 208956408,
        "sender_full_name": "mw",
        "timestamp": 1599134826
    },
    {
        "content": "<p>but that only works because it works under the assumption that it has an unlimited amount of memory available.</p>",
        "id": 208956459,
        "sender_full_name": "mw",
        "timestamp": 1599134864
    },
    {
        "content": "<p>I'm not sure if that is a good trade off</p>",
        "id": 208956526,
        "sender_full_name": "mw",
        "timestamp": 1599134888
    },
    {
        "content": "<blockquote>\n<p>I guess maybe a scheme that buffers say 5 million events or so and then does a small burst on the separate thread to record them feels like it might be a bit better</p>\n</blockquote>\n<p>Yes, that is what the proof of concept impl does. Each page is X bytes (128KB seems to work fine) and once that's full, it gets sent to the background thread which immediately will write it to disk.</p>",
        "id": 208956793,
        "sender_full_name": "mw",
        "timestamp": 1599135042
    },
    {
        "content": "<p>it's not super complicated (all the threading and synchronization takes place within the one source file and is well-contained), but it's still quite a bit harder to reason about than the single-threaded version (especially when it comes to error modes)</p>",
        "id": 208956932,
        "sender_full_name": "mw",
        "timestamp": 1599135122
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> Regarding testing on perf.rlo: the postprocessing tools on the server obviously can't deal with the new file format. If I test a rustc with the new <code>measureme</code> version, will I still get the high-level timings or will it just crash?</p>",
        "id": 208957287,
        "sender_full_name": "mw",
        "timestamp": 1599135341
    },
    {
        "content": "<p>Hm I'm not sure</p>",
        "id": 208957717,
        "sender_full_name": "simulacrum",
        "timestamp": 1599135591
    },
    {
        "content": "<p>It'll probably crash, but I can make it not do that without too much trouble</p>",
        "id": 208957792,
        "sender_full_name": "simulacrum",
        "timestamp": 1599135614
    },
    {
        "content": "<p>We can also provide new post processing tools? Are they not yet ready?</p>",
        "id": 208957822,
        "sender_full_name": "simulacrum",
        "timestamp": 1599135634
    },
    {
        "content": "<p>At least for perf server we basically have unlimited memory (iirc, 32GB) for this use case</p>",
        "id": 208957897,
        "sender_full_name": "simulacrum",
        "timestamp": 1599135695
    },
    {
        "content": "<p>No rustc will use more than 10GB on our cases, and that's a high estimate I suspect</p>",
        "id": 208957922,
        "sender_full_name": "simulacrum",
        "timestamp": 1599135716
    },
    {
        "content": "<p>But other than bumping up to more than 128KB - that feels quite low, we probably want to pick as high a number as possible, right?</p>",
        "id": 208958102,
        "sender_full_name": "simulacrum",
        "timestamp": 1599135816
    },
    {
        "content": "<p>I suspect that the background thread will be fine</p>",
        "id": 208958118,
        "sender_full_name": "simulacrum",
        "timestamp": 1599135828
    },
    {
        "content": "<p>Wow <span class=\"user-mention\" data-user-id=\"124287\">@mw</span> nice work! I'm going to take some time later today to review the code and see if I can offer any feedback.</p>",
        "id": 208984260,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599146666
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/187831-t-compiler.2Fwg-self-profile/topic/paged.20single-file.20serialization/near/208958102\">said</a>:</p>\n<blockquote>\n<p>But other than bumping up to more than 128KB - that feels quite low, we probably want to pick as high a number as possible, right?</p>\n</blockquote>\n<p>I got this idea from how a lot of SQL databases store their clustered indexes. I think many of those try to align with the default OS page size which is usually 4kb I believe. I don't see any compelling reason to go that small for our use case though. Larger page sizes should mean we \"allocate\" less often which should have lower overhead on average.</p>",
        "id": 208985793,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147306
    },
    {
        "content": "<blockquote>\n<p>It'll probably crash, but I can make it not do that without too much trouble</p>\n</blockquote>\n<p>IIRC we had a hack at one point to run two different versions of the tooling to handle this case.</p>",
        "id": 208985879,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147354
    },
    {
        "content": "<p>Yeah, we still have that :)</p>",
        "id": 208985990,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147386
    },
    {
        "content": "<p>Is the \"for real\" fix to ship the tools in rustup? I think we talked about that at one point.</p>",
        "id": 208986102,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147448
    },
    {
        "content": "<p>in any case I'm not too worried about making the transition, we can manage it pretty well I suspect one way or another</p>",
        "id": 208986105,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147449
    },
    {
        "content": "<p>rustup would help for the summarize which runs synchronously -- but it doesn't help for the flamegraph and crox support recently added to perf, which is on-demand live</p>",
        "id": 208986230,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147490
    },
    {
        "content": "<p>But for that my suspicion is that we can just have perf.rlo frontend depend on multiple versions of the analyzeme libraries</p>",
        "id": 208986294,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147514
    },
    {
        "content": "<p>Ah ok</p>",
        "id": 208986310,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147524
    },
    {
        "content": "<p>We should have a meta tool that can read the various file headers and invoke the correct version of the tool for you</p>",
        "id": 208986377,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147546
    },
    {
        "content": "<p>Hm, bundling up all versions of the tooling into one? That would be great!</p>",
        "id": 208986423,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147565
    },
    {
        "content": "<p>I guess that may not work if you try to, for example, diff between two different versions</p>",
        "id": 208986455,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147576
    },
    {
        "content": "<p>well, I don't see why not</p>",
        "id": 208986465,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147581
    },
    {
        "content": "<p>if the two event streams are the same</p>",
        "id": 208986477,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147586
    },
    {
        "content": "<p>like, the event type is probably the same (or has the same API, anyway)</p>",
        "id": 208986521,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147602
    },
    {
        "content": "<p>Oh, we process to json right?</p>",
        "id": 208986579,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147610
    },
    {
        "content": "<p>well, for summarize, sure</p>",
        "id": 208986604,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147619
    },
    {
        "content": "<p>Then yeah that would mostly work then.</p>",
        "id": 208986643,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147635
    },
    {
        "content": "<p>but I mean that e.g. summarize wouldn't try to read the events file directly</p>",
        "id": 208986650,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147638
    },
    {
        "content": "<p>but it would instead call a \"analyzeme-combined\" crate that knows how to read any version of events files</p>",
        "id": 208986690,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147655
    },
    {
        "content": "<p>Oh I see</p>",
        "id": 208986702,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147662
    },
    {
        "content": "<p>so you can just directly call it with different versions and it basically just works</p>",
        "id": 208986736,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147679
    },
    {
        "content": "<p>I was thinking of something more along the lines of what rustup(?) does when you invoke <code>cargo</code> or <code>rustc</code></p>",
        "id": 208986747,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147685
    },
    {
        "content": "<p>That would certainly work though</p>",
        "id": 208986799,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599147708
    },
    {
        "content": "<p>yeah, that's a rustup feature</p>",
        "id": 208986867,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147724
    },
    {
        "content": "<p>I think it works poorly for our use case because we <em>do</em> want to compare across, at least somewhat</p>",
        "id": 208986911,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147742
    },
    {
        "content": "<p>and I <em>think</em> it might actually be simpler with my proposal</p>",
        "id": 208986961,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147768
    },
    {
        "content": "<p>since you only need to have the \"shared\" logic in one place</p>",
        "id": 208986978,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147777
    },
    {
        "content": "<p>(and you don't have multiple binaries etc etc)</p>",
        "id": 208987004,
        "sender_full_name": "simulacrum",
        "timestamp": 1599147789
    },
    {
        "content": "<p>At one point I wanted to just keep the old deserializers for each version around in the decoder. But having a \"dispatcher\" crate that just references old versions of <code>analyzeme</code> sounds pretty smart!</p>",
        "id": 208992215,
        "sender_full_name": "mw",
        "timestamp": 1599149802
    },
    {
        "content": "<p>Oh interesting... can we import old versions of the library from <a href=\"http://crates.io\">crates.io</a> and then rename them?</p>",
        "id": 208993220,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599150219
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/187831-t-compiler.2Fwg-self-profile/topic/paged.20single-file.20serialization/near/208958102\">said</a>:</p>\n<blockquote>\n<p>But other than bumping up to more than 128KB - that feels quite low, we probably want to pick as high a number as possible, right?</p>\n</blockquote>\n<p>There's a tradeoff here: The bigger the pages the longer it takes to write it to disk, but also the fewer times we have to go through the flushing overhead. With the background thread approach it doesn't matter too much (I observed no difference at all between 128KB and 512KB for example). With a synchronously approach you have to find some middle ground between stalling often for short periods and less often for longer periods. In an interactive scenario (e.g. for computer games in a garbage collected language) you clearly want the former. In the case of <code>rustc</code>, I'm not so sure what's better. Ultimately, and luckily, it doesn't matter too much, I think <code>:)</code></p>",
        "id": 208993251,
        "sender_full_name": "mw",
        "timestamp": 1599150226
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/187831-t-compiler.2Fwg-self-profile/topic/paged.20single-file.20serialization/near/208957822\">said</a>:</p>\n<blockquote>\n<p>We can also provide new post processing tools? Are they not yet ready?</p>\n</blockquote>\n<p>The tools work just fine, they don't need to change at all fortunately. But for doing a perf run of an unmerged PR they wouldn't be available on the server yet.</p>",
        "id": 208993605,
        "sender_full_name": "mw",
        "timestamp": 1599150323
    },
    {
        "content": "<blockquote>\n<p>With a synchronously approach you have to find some middle ground between stalling often for short periods and less often for longer periods.</p>\n</blockquote>\n<p>We might want to represent that time in the profile somehow. To make it clear the time spent isn't from a parent event but from mm itself.</p>",
        "id": 208994013,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599150462
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"124287\">mw</span> <a href=\"#narrow/stream/187831-t-compiler.2Fwg-self-profile/topic/paged.20single-file.20serialization/near/208993605\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/187831-t-compiler.2Fwg-self-profile/topic/paged.20single-file.20serialization/near/208957822\">said</a>:</p>\n<blockquote>\n<p>We can also provide new post processing tools? Are they not yet ready?</p>\n</blockquote>\n<p>The tools work just fine, they don't need to change at all fortunately. But for doing a perf run of an unmerged PR they wouldn't be available on the server yet.</p>\n</blockquote>\n<p>It should be easy enough to have summarize be in place and we can create dummy string-index and string-data files so the upload stuff doesn't break</p>",
        "id": 208995471,
        "sender_full_name": "simulacrum",
        "timestamp": 1599151036
    },
    {
        "content": "<p>I think async on a background thread that is mostly idle should be fine</p>",
        "id": 208995508,
        "sender_full_name": "simulacrum",
        "timestamp": 1599151057
    },
    {
        "content": "<p>my thought with larger chunks was that in the \"small crate\" case the larger the chunk is the more likely we flush at the end of compilation, not interfering with cache lines etc at all</p>",
        "id": 208995616,
        "sender_full_name": "simulacrum",
        "timestamp": 1599151089
    },
    {
        "content": "<p>I ran the micro benchmarks with a number of different page sizes. Note that there's quite a bit of variability in those numbers.</p>\n<table>\n<thead>\n<tr>\n<th>PAGE SIZE</th>\n<th>ASYNC</th>\n<th>SYNC</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>4KB</td>\n<td>5.953s</td>\n<td>5.744s</td>\n</tr>\n<tr>\n<td>64KB</td>\n<td>5.216s</td>\n<td>5.312s</td>\n</tr>\n<tr>\n<td>128KB</td>\n<td>5.076s</td>\n<td>5.145s</td>\n</tr>\n<tr>\n<td>512KB</td>\n<td>5.038s</td>\n<td>5.132s</td>\n</tr>\n<tr>\n<td>1MB</td>\n<td>5.157s</td>\n<td>5.329s</td>\n</tr>\n<tr>\n<td>4MB</td>\n<td>5.231s</td>\n<td>5.260s</td>\n</tr>\n<tr>\n<td>10MB</td>\n<td>4.944s</td>\n<td>5.172s</td>\n</tr>\n<tr>\n<td>16MB</td>\n<td>5.204s</td>\n<td>5.399s</td>\n</tr>\n<tr>\n<td>64MB</td>\n<td>5.432s</td>\n<td>5.455s</td>\n</tr>\n<tr>\n<td>512MB</td>\n<td>7.440s</td>\n<td>6.953s</td>\n</tr>\n</tbody>\n</table>",
        "id": 209013552,
        "sender_full_name": "mw",
        "timestamp": 1599159338
    },
    {
        "content": "<p>Interesting. I guess this number can be toggled up and down relatively easily anyway, so doesn't matter much</p>",
        "id": 209013953,
        "sender_full_name": "simulacrum",
        "timestamp": 1599159511
    },
    {
        "content": "<p>Yeah, it seems that the page size basically doesn't matter if it's somewhere between 100KB and 16MB</p>",
        "id": 209014054,
        "sender_full_name": "mw",
        "timestamp": 1599159567
    },
    {
        "content": "<p>I just ran the 10 MB test case again and it gives me 5.172s (ASYNC) and 5.391s (SYNC), so it's not really better than the other cases.</p>",
        "id": 209014249,
        "sender_full_name": "mw",
        "timestamp": 1599159645
    },
    {
        "content": "<p>Really interesting that the sync case is faster than the async case for 4kb pages. Which is probably what your OS pagesize is set to.</p>",
        "id": 209014457,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599159722
    },
    {
        "content": "<p>Unless that's just noise.</p>",
        "id": 209014511,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599159746
    },
    {
        "content": "<p>let me run that again. I'm pretty sure it's noise</p>",
        "id": 209014617,
        "sender_full_name": "mw",
        "timestamp": 1599159791
    },
    {
        "content": "<p>Here's the full output from <code>hyperfine</code>:</p>\n<div class=\"codehilite\"><pre><span></span><code>Benchmark #1: ./target/release/paged_sync_single_threaded\n  Time (mean ± σ):      5.750 s ±  0.108 s    [User: 3.719 s, System: 1.989 s]\n  Range (min … max):    5.626 s …  5.967 s    10 runs\n\nBenchmark #2: ./target/release/paged_async_single_threaded\n  Time (mean ± σ):      5.880 s ±  0.089 s    [User: 4.313 s, System: 2.556 s]\n  Range (min … max):    5.787 s …  6.067 s    10 runs\n</code></pre></div>\n\n\n<p>The SYNC case is still faster. But I would not read too much into these numbers <code>:)</code></p>",
        "id": 209015482,
        "sender_full_name": "mw",
        "timestamp": 1599160199
    },
    {
        "content": "<p>(<code>hyperfine</code> is awesome!)</p>",
        "id": 209015630,
        "sender_full_name": "mw",
        "timestamp": 1599160248
    },
    {
        "content": "<p>I could believe there's some special magic that happens when our page size happens to be the same as the OS's, at least on Linux.</p>",
        "id": 209015707,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599160289
    },
    {
        "content": "<p>But yeah, that doesn't seem compelling enough to use 4kb over 128kb or something.</p>",
        "id": 209015742,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599160303
    },
    {
        "content": "<p>Unless the async version is so complex that we don't want to ship it</p>",
        "id": 209015861,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599160340
    },
    {
        "content": "<p>(Haven't looked yet sorry!)</p>",
        "id": 209015873,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599160345
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span> At this point I would not recommend to review in detail. I think the main question to answer now is whether to go with the background thread approach or with the in-thread approach. But actually, it would be even better to do some performance testing on perf.rlo with both approach. If the single-threaded approach turns out to be fast enough, then it would just go with that.</p>",
        "id": 209099615,
        "sender_full_name": "mw",
        "timestamp": 1599229624
    },
    {
        "content": "<p>I plan to do the performance testing on perf.rlo, btw</p>",
        "id": 209099774,
        "sender_full_name": "mw",
        "timestamp": 1599229693
    },
    {
        "content": "<p>Well the async version wasn't as complex as I thought it would be :)</p>",
        "id": 209116402,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599238309
    },
    {
        "content": "<p>More complexity than the sync for sure but not an enormous amount.</p>",
        "id": 209116474,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599238336
    },
    {
        "content": "<p>I do like the sync version more as I think it's quite a bit easier to understand. I'm also really interested in trying a version of that where we use TLS variables instead of the mutex so each thread writes to their own pages.</p>",
        "id": 209116796,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599238542
    },
    {
        "content": "<p>I have some small nit-type suggestions for both of the approaches but nothing major <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 209116923,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599238590
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"125250\">Wesley Wiser</span> <a href=\"#narrow/stream/187831-t-compiler.2Fwg-self-profile/topic/paged.20single-file.20serialization/near/209116402\">said</a>:</p>\n<blockquote>\n<p>Well the async version wasn't as complex as I thought it would be :)</p>\n</blockquote>\n<p>Except that multi-threaded stuff is always 10x more complex than you think it is <code>:)</code></p>",
        "id": 209130496,
        "sender_full_name": "mw",
        "timestamp": 1599246494
    },
    {
        "content": "<p>With paging a TLS approach might actually be doable, yeah! This code spends a <em>lot</em> of time doing Mutex locking and unlocking. But with something like 128 KB pages I don't see a real resource problem with allocating a writer per page.</p>",
        "id": 209130828,
        "sender_full_name": "mw",
        "timestamp": 1599246695
    },
    {
        "content": "<p>I'm not sure if there are other problems. IIRC, values in TLS are properly dropped when a thread exits, so that should be fine.</p>",
        "id": 209131006,
        "sender_full_name": "mw",
        "timestamp": 1599246800
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span>, let me know about those nits, so I can work them in as I go.</p>",
        "id": 209131139,
        "sender_full_name": "mw",
        "timestamp": 1599246855
    },
    {
        "content": "<p>Oh sure! What's the best way for me to give them to you?</p>",
        "id": 209131164,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599246868
    },
    {
        "content": "<p>Just write them up here?</p>",
        "id": 209131174,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599246873
    },
    {
        "content": "<p>yes</p>",
        "id": 209131193,
        "sender_full_name": "mw",
        "timestamp": 1599246885
    },
    {
        "content": "<p>doesn't have to be now though :)</p>",
        "id": 209131234,
        "sender_full_name": "mw",
        "timestamp": 1599246908
    },
    {
        "content": "<p>I'm waiting on a build to finish so I might as well write down what's off the top of my head :)</p>",
        "id": 209131370,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599246982
    },
    {
        "content": "<p>Sync code:</p>\n<blockquote>\n<p>assert!(num_bytes + PAGE_HEADER_SIZE &lt;= self.shared_state.page_size);</p>\n</blockquote>\n<p>This constraint should be documented somehow. We should not be recording ~127.9kb strings though so it shouldn't be an issue in practice. </p>\n<p>Async code:</p>\n<blockquote>\n<div class=\"codehilite\"><pre><span></span><code>                    // A zero-length page is the signal for stopping the\n                   // background thread.\n</code></pre></div>\n\n\n</blockquote>\n<p>Unless there's a noticeable performance difference, I think we should use an enum here <code>enum Msg { Write(Vec&lt;u8&gt;), Close  }</code>.</p>\n<blockquote>\n<p>drop(sx.send(payload));</p>\n</blockquote>\n<p>I thought this was to get a <code>Drop::drop()</code> call to run but I think this is just to get rid of the \"unused result\" warning? <code>let _ = sx.send(payload);</code> might be clearer.</p>\n<p>Both:</p>\n<blockquote>\n<p>fn write_page_header</p>\n</blockquote>\n<p>I think it would be nice if this had an actual <code>struct</code> that got encoded as the header. I think we will want to have additional fields in the header eventually. (For instance, we may want to let the caller control the size of the page. We would then need to write that value into the header so we can set up the reader appropriately. Another example, we could encode the endianness in the header so that BE systems don't have to convert to LE during the recording.)</p>",
        "id": 209134218,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599248894
    },
    {
        "content": "<blockquote>\n<p>values in TLS are properly dropped when a thread exits</p>\n</blockquote>\n<p>I would hesitate to make this a unilateral claim, at least not on all platforms</p>",
        "id": 209136594,
        "sender_full_name": "simulacrum",
        "timestamp": 1599250580
    },
    {
        "content": "<p>e.g. I think today on macOS the main thread won't do this? Not sure though</p>",
        "id": 209136607,
        "sender_full_name": "simulacrum",
        "timestamp": 1599250595
    },
    {
        "content": "<p>Ah yes, the docs say that none of the Unix systems do this. Something to keep in mind.</p>",
        "id": 209166539,
        "sender_full_name": "mw",
        "timestamp": 1599291828
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"124287\">@mw</span> should I build and fallback to the self-profile tooling built from your branch?</p>",
        "id": 209312447,
        "sender_full_name": "simulacrum",
        "timestamp": 1599496567
    },
    {
        "content": "<p>(Is it already producing just one file?)</p>",
        "id": 209312455,
        "sender_full_name": "simulacrum",
        "timestamp": 1599496574
    },
    {
        "content": "<p>looks like yes</p>",
        "id": 209312531,
        "sender_full_name": "simulacrum",
        "timestamp": 1599496678
    },
    {
        "content": "<p>OK, we got some numbers from <a href=\"https://github.com/rust-lang/rust/pull/76436\">https://github.com/rust-lang/rust/pull/76436</a>. It looks like the background-thread version is not worth the hassle -- which is good news <code>:)</code></p>",
        "id": 209607006,
        "sender_full_name": "mw",
        "timestamp": 1599725205
    },
    {
        "content": "<p>I'd like to do a clean re-implementation of the whole thing now.</p>",
        "id": 209607046,
        "sender_full_name": "mw",
        "timestamp": 1599725253
    },
    {
        "content": "<p>I think it makes sense to get rid of <code>SerializationSink</code> pluggability altogether in the process. I think usage so far has shown we just use one or the other based on the platform, but that they perform pretty much the same. We can still have platform specific implementations if necessary but that would only be an internal implementation detail of  <code>measureme</code>, not something the is exposed to clients of the library.</p>",
        "id": 209607455,
        "sender_full_name": "mw",
        "timestamp": 1599725529
    },
    {
        "content": "<p>Although it's interesting that some platforms seem to use the <code>ByteVecSerializationSink</code>. I wonder what's that about</p>",
        "id": 209607540,
        "sender_full_name": "mw",
        "timestamp": 1599725598
    },
    {
        "content": "<blockquote>\n<p>Although it's interesting that some platforms seem to use the <code>ByteVecSerializationSink</code>. I wonder what's that about</p>\n</blockquote>\n<p>OK, that seems to be better solved by just disabling the profiler in cases where the platform doesn't allow writing to files at all.</p>",
        "id": 209607977,
        "sender_full_name": "mw",
        "timestamp": 1599725895
    },
    {
        "content": "<p>What do you folks, think: Shall we remove <code>SerializationSink</code> from the public interface of <code>measureme</code> altogether?</p>",
        "id": 209608048,
        "sender_full_name": "mw",
        "timestamp": 1599725963
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"124287\">mw</span> <a href=\"#narrow/stream/187831-t-compiler.2Fwg-self-profile/topic/paged.20single-file.20serialization/near/209608048\">said</a>:</p>\n<blockquote>\n<p>What do you folks, think: Shall we remove <code>SerializationSink</code> from the public interface of <code>measureme</code> altogether?</p>\n</blockquote>\n<p>This seems overall like a good idea to me. I've gotten feedback from 3rd parties trying to use <code>measureme</code> that there's a lot of \"internal\" details that bleed through the api and makes it a lot harder to get started with. I think simplifying the api where we can is good.</p>",
        "id": 209641601,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1599745329
    },
    {
        "content": "<p>Yes, I'd guess the whole string handling story is the hardest part.</p>",
        "id": 209752462,
        "sender_full_name": "mw",
        "timestamp": 1599812379
    },
    {
        "content": "<p>I'll go ahead and prepare some PRs then <code>:)</code></p>",
        "id": 209752596,
        "sender_full_name": "mw",
        "timestamp": 1599812457
    },
    {
        "content": "<p>I think the info in this article explains why the version with the background thread is only marginally faster than the one persisting things synchronously: <a href=\"https://lwn.net/Articles/457667/\">https://lwn.net/Articles/457667/</a></p>",
        "id": 211027746,
        "sender_full_name": "mw",
        "timestamp": 1600878394
    },
    {
        "content": "<p>That is, on Linux at least, writing data to a file means that it is actually only copied to some memory that is managed by the kernel which then does the writing asynchronously in the background.</p>",
        "id": 211027914,
        "sender_full_name": "mw",
        "timestamp": 1600878476
    },
    {
        "content": "<p>There's also all kinds other ways of writing data to disk. E.g. setting <code>O_DIRECT</code> when opening a file on Linux, one can skip those kernel buffers and write to disk directly. In that case the memory being written needs to be OS page aligned. That might be related to what you've seen in databases, <span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span>.</p>",
        "id": 211031951,
        "sender_full_name": "mw",
        "timestamp": 1600880448
    },
    {
        "content": "<p>But I think the bottleneck for <code>measureme</code> right now is the mutex when writing to the buffer, so I won't spend too much time on optimizing the other parts.</p>",
        "id": 211032229,
        "sender_full_name": "mw",
        "timestamp": 1600880560
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"124287\">mw</span> <a href=\"#narrow/stream/187831-t-compiler.2Fwg-self-profile/topic/paged.20single-file.20serialization/near/211032229\">said</a>:</p>\n<blockquote>\n<p>But I think the bottleneck for <code>measureme</code> right now is the mutex when writing to the buffer, so I won't spend too much time on optimizing the other parts.</p>\n</blockquote>\n<p>Sorry for the random idea, but I think file appends are atomic, at least under a certain limit. That might make it possible to omit acquiring a mutex. Here's an interesting SO comment about this: <a href=\"https://stackoverflow.com/a/35256561\">https://stackoverflow.com/a/35256561</a></p>",
        "id": 211039644,
        "sender_full_name": "Laurențiu",
        "timestamp": 1600883937
    },
    {
        "content": "<p>That's interesting!</p>",
        "id": 211242829,
        "sender_full_name": "mw",
        "timestamp": 1601031125
    },
    {
        "content": "<p>OK, <a href=\"https://github.com/rust-lang/measureme/pull/132\">https://github.com/rust-lang/measureme/pull/132</a> is ready for review.</p>",
        "id": 211246888,
        "sender_full_name": "mw",
        "timestamp": 1601034192
    },
    {
        "content": "<p>^ <span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span></p>",
        "id": 211246978,
        "sender_full_name": "mw",
        "timestamp": 1601034260
    },
    {
        "content": "<p>o/</p>",
        "id": 214267236,
        "sender_full_name": "eddyb",
        "timestamp": 1603418768
    },
    {
        "content": "<p>this is really cool! maybe <code>wg-self-profile</code> is where I should've kept all the <code>rdpmc</code> discussion too. one thing I didn't consider much is introducing breaking changes, had I realized there was this breaking change to batch it with</p>",
        "id": 214267260,
        "sender_full_name": "eddyb",
        "timestamp": 1603418852
    },
    {
        "content": "<p>anyway I came in here to check if there's <a href=\"https://github.com/rust-lang/rust/issues/77398\">#77398</a> discussion but I guess not</p>",
        "id": 214267269,
        "sender_full_name": "eddyb",
        "timestamp": 1603418870
    },
    {
        "content": "<p>I think it's not a problem to bump readily if we need to</p>",
        "id": 214268591,
        "sender_full_name": "simulacrum",
        "timestamp": 1603421165
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span> <span class=\"user-mention\" data-user-id=\"124287\">@mw</span> uhh just realized the docs still talk about 3 files, that's a bit unfortunate, since the README links to <a href=\"http://docs.rs/measureme\">docs.rs/measureme</a></p>",
        "id": 216674745,
        "sender_full_name": "eddyb",
        "timestamp": 1605302566
    }
]