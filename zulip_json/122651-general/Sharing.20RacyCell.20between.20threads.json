[
    {
        "content": "<p>I'm writing an emulator that stores the emulated program's memory inside some kind of <code>Box&lt;[RacyCell&lt;u8&gt;]</code> or <code>Vec&lt;RacyCell&lt;u8&gt;&gt;</code> and we had some discussion on discord whether sharing a <code>RacyCell</code> between threads is safe</p>\n<p>This emulator doesn't really care about what data it will get from the \"race\", it only gives the emulated process what it is asking for</p>\n<p>My thinking is, wrapping bytes in <code>UnsafeCell</code> would make compiler assume the data is shared and that it can be mutated any time making this abstraction \"sound enough\"</p>",
        "id": 233613379,
        "sender_full_name": "Soveu",
        "timestamp": 1617867205
    },
    {
        "content": "<p>This reminds me of <span class=\"user-mention\" data-user-id=\"209168\">@Thom Chiovoloni</span> 's <a href=\"https://shift.click/blog/tearcell/\"><code>TearCell</code></a></p>",
        "id": 233613902,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1617867486
    },
    {
        "content": "<p>The main argument is that \"races are UB no matter what\"<br>\nWell, they are UB, because language can't specify what will be the exact result of a race, but the emulator doesn't really care about it</p>",
        "id": 233613936,
        "sender_full_name": "Soveu",
        "timestamp": 1617867515
    },
    {
        "content": "<p>The UB is on the emulated program side</p>",
        "id": 233613986,
        "sender_full_name": "Soveu",
        "timestamp": 1617867554
    },
    {
        "content": "<p>I think it's okay to have a program which pushes some UB to runtime, in the sense that there are certain inputs that cause UB but the program itself comes with a safety condition on users to not exercise those code paths</p>",
        "id": 233614163,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1617867657
    },
    {
        "content": "<p>However, that means that you have to be okay with an emulator that makes no guarantees about what the emulator does in the face of a bad input program, which is an unusual choice for an interpreter</p>",
        "id": 233614308,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1617867743
    },
    {
        "content": "<p>Hello, I was the person on the other side of the discord discussion. I'm not too convinced it's ok to say \"this is UB iff the input program causes UB\". It's still UB. The compiler is free to optimize assuming it _never_ happens.</p>\n<p>\"They are UB, because language can't specify what will be the exact result of a race\" is not the whole story.</p>\n<p>The language is allowed to completely change the code around in ways that cause SIGSEGV on a data race. Or even cause the instruction pointer to jump inside the emulated program, allowing it to execute arbitrary code as if it were the emulator.</p>\n<p>Making a racy read and getting garbage is literally the best thing that can happen. It is by no means the worst.</p>",
        "id": 233614807,
        "sender_full_name": "FishTrashPlaneCurtain",
        "timestamp": 1617868036
    },
    {
        "content": "<blockquote>\n<p>The compiler is free to optimize assuming it _never_ happens.</p>\n</blockquote>\n<p>Well, yes. That's what I mean by pushing UB to run time: the author of the rust emulator is disclaiming all responsibility for what happens if code triggers the UB path, and uses this to be able to make a promise to the compiler that UB doesn't happen. The onus now falls on the author of the emulated code to ensure that they don't write a program containing emulated UB. As long as they do this, the emulator's promise is satisfied and so is the compiler's.</p>",
        "id": 233615230,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1617868263
    },
    {
        "content": "<blockquote>\n<p>The compiler is free to optimize assuming it _never_ happens</p>\n</blockquote>\n<p>That is exactly what I'm expecting - a casual read/write - nothing less, nothing more</p>",
        "id": 233615254,
        "sender_full_name": "Soveu",
        "timestamp": 1617868274
    },
    {
        "content": "<p>The <code>TearCell</code> approach is a bit different; it makes all races non-UB by resulting in data corruption instead, which is probably a fine alternative for emulated UB, if you don't want to actually produce a \"partial program\"</p>",
        "id": 233615436,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1617868357
    },
    {
        "content": "<blockquote>\n<p>That is exactly what I'm expecting - a casual read/write - nothing less, nothing more</p>\n</blockquote>\n<p>And I have to do it, because unaligned atomics are even more UB</p>",
        "id": 233615647,
        "sender_full_name": "Soveu",
        "timestamp": 1617868480
    },
    {
        "content": "<p>unaligned atomic operations are pretty easy to trap though</p>",
        "id": 233615685,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1617868513
    },
    {
        "content": "<p>but what is suggested is every casual operation gets promoted to atomic operation in emulation</p>",
        "id": 233615732,
        "sender_full_name": "Soveu",
        "timestamp": 1617868555
    },
    {
        "content": "<p>I guess if your semantics are \"emulated programs that cause a data race are UB\" it _may_ be fine?</p>\n<p>You have to be  _very careful_, tho. You could end up with a program that has a vulnerability. If you need to be secure _at all_ this won't do</p>",
        "id": 233615800,
        "sender_full_name": "FishTrashPlaneCurtain",
        "timestamp": 1617868592
    },
    {
        "content": "<p>and it is common to have unaligned reads/writes supported on cpus</p>",
        "id": 233615802,
        "sender_full_name": "Soveu",
        "timestamp": 1617868594
    },
    {
        "content": "<p>If you are talking about the TearCell implementation, I think it automatically splits reads into appropriately aligned (<code>Unordered</code>) atomic operations</p>",
        "id": 233615929,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1617868693
    },
    {
        "content": "<p>I'm pretty sure that aligned unordered atomic operations compile to plain reads and writes on basically every arch</p>",
        "id": 233615992,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1617868755
    },
    {
        "content": "<p>This isn't only about store/write, it is also about and/or/xor/add/sub/mul/div</p>",
        "id": 233616260,
        "sender_full_name": "Soveu",
        "timestamp": 1617868888
    },
    {
        "content": "<p>you don't even have atomic ops for some of them</p>",
        "id": 233616293,
        "sender_full_name": "Soveu",
        "timestamp": 1617868903
    },
    {
        "content": "<p>I would expect that if you just (atomic) read, perform your operation, and then (atomic) write, and set the atomics to the lowest setting, you will get the same codegen as normal</p>",
        "id": 233617340,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1617869560
    },
    {
        "content": "<p>The main thing you will be missing is the ability to perform read/write compiler optimizations, and that's not usually an issue for interpreters since there isn't much to optimize, but if it's a JIT compiler then that might be a performance issue</p>",
        "id": 233617545,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1617869720
    },
    {
        "content": "<p>Yeah, for now as it is an interpreter, it could use something like TearCell, since MMU stuff will consume most of cpu time</p>",
        "id": 233618381,
        "sender_full_name": "Soveu",
        "timestamp": 1617870266
    },
    {
        "content": "<p>but later i probably would want to have some sort of 1:1 mapping between emulated -&gt; emulator memory space</p>",
        "id": 233618693,
        "sender_full_name": "Soveu",
        "timestamp": 1617870480
    },
    {
        "content": "<p>and it can make a difference</p>",
        "id": 233618939,
        "sender_full_name": "Soveu",
        "timestamp": 1617870609
    },
    {
        "content": "<p>maybe similar to <code>MaybeUninit::assume_init</code> we should also have <code>RacyCell::assume_nonracy_read/write</code> <span aria-label=\"thinking\" class=\"emoji emoji-1f914\" role=\"img\" title=\"thinking\">:thinking:</span></p>",
        "id": 233619753,
        "sender_full_name": "Soveu",
        "timestamp": 1617871097
    },
    {
        "content": "<p><a href=\"https://github.com/rust-lang/rust/issues/19835\">#19835</a> looks interesting:</p>\n<blockquote>\n<p>Valid uses of this ordering are things like racy counters where you don't care about the operation actually being atomic, just want to avoid UB.</p>\n</blockquote>",
        "id": 233620596,
        "sender_full_name": "hyd-dev",
        "timestamp": 1617871599
    },
    {
        "content": "<p>Oh hey, someone mentioned my blog <span aria-label=\"heart\" class=\"emoji emoji-2764\" role=\"img\" title=\"heart\">:heart:</span>. The main use case for TearCell is as a building block for stuff like seqlocks (and other optimistic concurrency primitives), but yeah it's definitely usable for any case where what you want is a datarace-but-no-ub-please. (I meant to follow up with a post on these use cases, but, well, I have a lot of posts I'd like to write)</p>\n<p>Anyway, tearcell has a couple minor issues in the implementation that I havent gotten around to fix:</p>\n<ul>\n<li>if you have a large struct with a small alignment it won't use a very good algo for copying stuff (it will do it per-byte — wheras if it's higher alignment it does it with the largest atomic type available).</li>\n<li>Also it uses AtomicU64 for stuff with align_of &gt;= 8 on all arches, but it shouldn't on arm32, where AtomicU64 is supported but slower than AtomicUsize.</li>\n</ul>\n<p>(I also still wish that it were definable without requiring bytemuck::Pod bound, but I'm pretty sure that it turned out that my paranoia there was warranted and using it with types that e.g. have padding would be no bueno)</p>",
        "id": 233633829,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1617879326
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"374396\">hyd-dev</span> <a href=\"#narrow/stream/122651-general/topic/Sharing.20RacyCell.20between.20threads/near/233620596\">said</a>:</p>\n<blockquote>\n<p><a href=\"https://github.com/rust-lang/rust/issues/19835\">#19835</a> looks interesting:</p>\n<blockquote>\n<p>Valid uses of this ordering are things like racy counters where you don't care about the operation actually being atomic, just want to avoid UB.</p>\n</blockquote>\n</blockquote>\n<p>Unordered is basically Relaxed in this context</p>",
        "id": 233633963,
        "sender_full_name": "Soveu",
        "timestamp": 1617879424
    },
    {
        "content": "<p>it is still atomic, but can be reordered with other atomics, unlike Relaxed</p>",
        "id": 233634072,
        "sender_full_name": "Soveu",
        "timestamp": 1617879506
    },
    {
        "content": "<p>For a case like <code>Box&lt;[RacyCell&lt;u8&gt;]&gt;</code> I'd just do <code>Box&lt;[AtomicU8]&gt;</code> and use relaxed atomics. TearCell is not really useful for a type which has a direct atomic equivalent, since it's just doing relaxed ops.</p>\n<p>You could also use stuff like</p>\n<ul>\n<li><a href=\"https://doc.rust-lang.org/core/intrinsics/fn.atomic_load_unordered.html\">https://doc.rust-lang.org/core/intrinsics/fn.atomic_load_unordered.html</a></li>\n<li><a href=\"https://doc.rust-lang.org/core/intrinsics/fn.atomic_store_unordered.html\">https://doc.rust-lang.org/core/intrinsics/fn.atomic_store_unordered.html</a></li>\n</ul>\n<p>LLVM is willing to optimize these much more aggressively than relaxed, but you won't see it for all cases. Its really hard to find a way to use them in real concurrent code too — I've managed to write a couple algorithms that are coherent under the minimal guarantee they provide in multithreaded usage (which are something like \"a value loaded with an unordered load was definitely stored at one point\"), but most traditional ones require at least monotonic progress, as provided by relaxed</p>",
        "id": 233634163,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1617879590
    },
    {
        "content": "<p>i really think about manually loading the memory via</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"fm\">asm!</span><span class=\"p\">(</span><span class=\"s\">\"mov {}, [{}]\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">lateout</span><span class=\"p\">(</span><span class=\"n\">reg</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">value</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"p\">(</span><span class=\"n\">reg</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">addr</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">options</span><span class=\"p\">(</span><span class=\"n\">nostack</span><span class=\"p\">))</span><span class=\"w\"></span>\n</code></pre></div>",
        "id": 233634570,
        "sender_full_name": "Soveu",
        "timestamp": 1617879812
    },
    {
        "content": "<blockquote>\n<p>it is still atomic, but can be reordered with other atomics, unlike Relaxed</p>\n</blockquote>\n<p>The big differnce is that relaxed accesses to the same atomic can't be reordered across eachother, and that it's very hard for LLVM to do much clever like combine, elide, or otherwise optimize the accesses. In practice it has very different guarantees from it, but often behaves a bit like a volatile op — for the time being, it's almost certainly going to go to memory, and happen where you wrote it. (That said, there's the promise in the future that some of this may be improved... hopefully)</p>\n<p>That said it's not like unordered is much better. I've seen LLVM do wonders with it, and I've also seen LLVM completely fail to optimize it any any better than relaxed. It was really finnickey, and not always that much of a win.</p>\n<p>And really, relaxed is fine most of the time since what you're more concerned about is fencing.</p>",
        "id": 233634825,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1617879966
    },
    {
        "content": "<p>there wouldn't be much to optimize, load/store is basically one operation and the rest is MMU stuff</p>",
        "id": 233635403,
        "sender_full_name": "Soveu",
        "timestamp": 1617880332
    },
    {
        "content": "<p>My understanding about the asm case is that it's still UB to do that if you immediately extract the value.</p>\n<p>IIUC, it's only not UB to cheat the rules in an asm block if the cheating happens fully in the asm block — if you extract a value that would be <code>undef</code> or <code>poison</code>, it's still UB to return it to rust. If you do everything you need to do with it entirely within the ASM block and produce a fully well-defined result. </p>\n<p>For example, in a string match function, you might have an asm block that read past the end of an array (assuming you handle page boundaries properly etc), determines the result of the match within the asm, and returns the result of that to rust, but you can't have one that just reads past the end and passes it to rust.</p>\n<p>That is, i believe the asm block there is semantically equivalent to a volatile read, which isn't enough to escape UB. (In theory anway, in practice it likely is)</p>",
        "id": 233635500,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1617880399
    },
    {
        "content": "<p>to be precise, not only volatile, also unaligned</p>",
        "id": 233635850,
        "sender_full_name": "Soveu",
        "timestamp": 1617880631
    },
    {
        "content": "<p>Unordered like any other atomic can't have loads/stores merged together</p>",
        "id": 233636035,
        "sender_full_name": "Soveu",
        "timestamp": 1617880802
    },
    {
        "content": "<p>or are data races UB because some really old hardware used to catch fire?<br>\nif one core tried to zero (ground) a memory cell and the other one set it, the circuit would close</p>",
        "id": 233637191,
        "sender_full_name": "Soveu",
        "timestamp": 1617881559
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"328907\">Soveu</span> <a href=\"#narrow/stream/122651-general/topic/Sharing.20RacyCell.20between.20threads/near/233636035\">said</a>:</p>\n<blockquote>\n<p>Unordered like any other atomic can't have loads/stores merged together</p>\n</blockquote>\n<p>I think it can have them merged, no? obviously not split. Merging would introduce mixed size access, but given the semantics of unordered I don't see how that would be a problem on most hardware</p>",
        "id": 233637596,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1617881816
    },
    {
        "content": "<p><a href=\"https://releases.llvm.org/8.0.0/docs/Atomics.html#unordered\">https://releases.llvm.org/8.0.0/docs/Atomics.html#unordered</a></p>\n<div class=\"codehilite\"><pre><span></span><code>In terms of the optimizer, this prohibits any transformation that transforms a single load into\nmultiple loads,transforms a store into multiple stores, narrows a store, or stores a value which\nwould not be stored otherwise.\nSome examples of unsafe optimizations are narrowing an assignment into a bitfield,\nrematerializing a load, and turning loads and stores into a memcpy call. Reordering\nunordered operations is safe, though, and optimizers should take advantage of that\nbecause unordered operations are common in languages that need them.\n</code></pre></div>",
        "id": 233638011,
        "sender_full_name": "Soveu",
        "timestamp": 1617882026
    },
    {
        "content": "<p>that seems more restrictive than what java's default, non-atomic access promises</p>",
        "id": 233683252,
        "sender_full_name": "The 8472",
        "timestamp": 1617898870
    },
    {
        "content": "<p>Ah, it relies on the frontend to do the narrowing. Still, I'm not sure if the restriction on rematerialization is necessary.</p>",
        "id": 233684091,
        "sender_full_name": "The 8472",
        "timestamp": 1617899236
    },
    {
        "content": "<p>Hm? The LLVM docs don't say that it can't transform multiple loads into a single load, or multiple stores into single stores, which is what I meant by merging. It explicitly forbids the opposite of that, however.</p>",
        "id": 233907567,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1618012071
    },
    {
        "content": "<p>for now relaxed load of <code>[AtomicU8; 4]</code> are basically 4 different loads, idk how it will look with Unordered</p>",
        "id": 233940352,
        "sender_full_name": "Soveu",
        "timestamp": 1618045678
    },
    {
        "content": "<p>It's definitely allowed to merge those for unordered. I don't think so for Relaxed — mixed sized accesses are very weird and not particularly well specified.</p>\n<p>In practice with mixed size accesses, even if you used seqcst for all of them, i believe you don't actually get sequential consistency on a lot of hardware (exception being on x86) — there's just no way to have that fence. so, this is likely part of why compilers may avoid it if they do.</p>\n<p>Doesn't really apply to Unordered though.</p>",
        "id": 234019809,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1618121663
    },
    {
        "content": "<p>with Unordered it's so weird<br>\nload is split, store is also split<br>\n<a href=\"https://godbolt.org/z/TMKqrK8hj\">https://godbolt.org/z/TMKqrK8hj</a><br>\nfun fact: when store or load is inside a for loop, it calls <code>llvm_memcpy_unordered</code> or something like that</p>",
        "id": 234028485,
        "sender_full_name": "Soveu",
        "timestamp": 1618130980
    }
]