[
    {
        "content": "<p>Not sure if this is the best place to ask, it's unrelated to any Rust contributions I'm working on - I've got a project where the memory usage keeps growing when run with multiple threads. The same code without threads uses a constant amount of memory, running that code on multiple threads - even though they don't interact or share any data - causes the memory to keep increasing. </p>\n<p>I've made a small-ish example (<a href=\"https://gist.github.com/davidtwco/46b9a212654c9b8d8061df056467a20d\" target=\"_blank\" title=\"https://gist.github.com/davidtwco/46b9a212654c9b8d8061df056467a20d\">https://gist.github.com/davidtwco/46b9a212654c9b8d8061df056467a20d</a>) but it requires you have multiple Jenkins instances running with a bunch of jobs in each - so it's not really all that useful. I've checked it on three different machines with two different OS' and I've included a massif output file since it might be hard to run it. It seems to be all <code>serde_json</code> making the allocations, but I can't see why it wouldn't be getting dropped - it's entirely possible (and probably likely) that I've just made a mistake somewhere.</p>\n<p>Would appreciate any thoughts anyone has - been staring at this for a while with no luck.</p>",
        "id": 128592995,
        "sender_full_name": "davidtwco",
        "timestamp": 1529921865
    },
    {
        "content": "<p>Are more and more threads spinning up?</p>",
        "id": 128599163,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529932371
    },
    {
        "content": "<p>My first thought is memory fragmentation; memory usage isn't really going up, but thread 1 allocates memory, thread 2 allocates, thread 1 frees, but it cannot be coalesced because thread 2</p>",
        "id": 128599241,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529932443
    },
    {
        "content": "<p>so the total allocation from the OS gets bigger and bigger, but the actual used doesnt</p>",
        "id": 128599253,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529932467
    },
    {
        "content": "<p>heh, I was going to suggest trying a different allocator, do you see this with both jemalloc and system?</p>",
        "id": 128599264,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529932496
    },
    {
        "content": "<p>There's only five (one for each Jenkins instance) and then however many hyper/reqwest uses. That stays constant.</p>",
        "id": 128599274,
        "sender_full_name": "davidtwco",
        "timestamp": 1529932511
    },
    {
        "content": "<p>Happens with system and jemalloc.</p>",
        "id": 128599277,
        "sender_full_name": "davidtwco",
        "timestamp": 1529932514
    },
    {
        "content": "<p>I was just using system because I read jemalloc doesn't like valgrind.</p>",
        "id": 128599286,
        "sender_full_name": "davidtwco",
        "timestamp": 1529932526
    },
    {
        "content": "<p>it is true</p>",
        "id": 128599287,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529932532
    },
    {
        "content": "<p>If it were memory fragmentation, how would I go about resolving that?</p>",
        "id": 128599350,
        "sender_full_name": "davidtwco",
        "timestamp": 1529932593
    },
    {
        "content": "<p>Been banging my head against a wall for a good week looking through the stacktrace for the allocations looking for something dodgy.</p>",
        "id": 128599369,
        "sender_full_name": "davidtwco",
        "timestamp": 1529932640
    },
    {
        "content": "<p>I honestly don't know. One answer is \"use a compacting garbage collector\" <span class=\"emoji emoji-1f622\" title=\"cry\">:cry:</span></p>",
        "id": 128599572,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529932883
    },
    {
        "content": "<p>I don't suppose you can make it even smaller repro? Using just serde in threads?</p>",
        "id": 128599615,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529932923
    },
    {
        "content": "<p>I'll see what I can do about getting the repro smaller.</p>",
        "id": 128599644,
        "sender_full_name": "davidtwco",
        "timestamp": 1529932960
    },
    {
        "content": "<p>Thanks.</p>",
        "id": 128599646,
        "sender_full_name": "davidtwco",
        "timestamp": 1529932965
    },
    {
        "content": "<p>I don't even know how to validate that it is fragmentation</p>",
        "id": 128599655,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529932988
    },
    {
        "content": "<blockquote>\n<p>causes the memory to keep increasing.</p>\n</blockquote>\n<p>How are you seeing this manifest? Are you getting errors of some kind?</p>",
        "id": 128599738,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529933073
    },
    {
        "content": "<p>On machines with low memory, it would get killed. On my own working machine I can just watch htop or some other utility remark that the process' memory is increasing gradually. It can reach around 40% or so of the system memory (~7 GB) and it increases at around 20MB/s.</p>",
        "id": 128599779,
        "sender_full_name": "davidtwco",
        "timestamp": 1529933153
    },
    {
        "content": "<p>That's a pretty quick rate</p>",
        "id": 128599854,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529933224
    },
    {
        "content": "<p><a href=\"https://github.com/mockersf/jenkins-api.rs/blob/ba7e33540cd159c20d13bab4311406e4a2d31293/src/job/common.rs#L71-L83\" target=\"_blank\" title=\"https://github.com/mockersf/jenkins-api.rs/blob/ba7e33540cd159c20d13bab4311406e4a2d31293/src/job/common.rs#L71-L83\">https://github.com/mockersf/jenkins-api.rs/blob/ba7e33540cd159c20d13bab4311406e4a2d31293/src/job/common.rs#L71-L83</a></p>\n<p>I see two relatively interesting features there â€” the <code>flatten</code> and the <code>PhantomData</code></p>",
        "id": 128599874,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529933271
    },
    {
        "content": "<p>Yeah, luckily it often terminates before that but on hosts with less memory it won't. With a single thread it would use around 0.1-0.5% of system memory.</p>",
        "id": 128599877,
        "sender_full_name": "davidtwco",
        "timestamp": 1529933279
    },
    {
        "content": "<p><code>_IMPL_DESERIALIZE_FOR_CommonAction</code> , <code>_IMPL_DESERIALIZE_FOR_ShortView</code>, <code>_IMPL_DESERIALIZE_FOR_ShortJob</code>,  all make use of <code>flatten</code></p>",
        "id": 128600158,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529933635
    },
    {
        "content": "<p>I know that's a <em>newer</em> feature of Serde, but I cannot think how it would have threading differences at all</p>",
        "id": 128600206,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529933663
    },
    {
        "content": "<p>This may be inconsequential, but <a href=\"https://github.com/sile/libflate/blob/master/src/non_blocking/deflate/decode.rs#L204-L225\" target=\"_blank\" title=\"https://github.com/sile/libflate/blob/master/src/non_blocking/deflate/decode.rs#L204-L225\">this code</a> came up when I was running massif the first few times, but on recent runs it hasn't shown up, not sure why. It struck me as potentially the issue, though unlikely.</p>",
        "id": 128600236,
        "sender_full_name": "davidtwco",
        "timestamp": 1529933753
    },
    {
        "content": "<p>I've tried pasting (and anonymizing) a dump from the API and just doing a loop where I parse that a bunch into that <code>CommonJob</code> struct and that doesn't seem to reproduce it.</p>",
        "id": 128601032,
        "sender_full_name": "davidtwco",
        "timestamp": 1529934820
    },
    {
        "content": "<p>That makes me think it's something the API wrapper is doing.</p>",
        "id": 128601106,
        "sender_full_name": "davidtwco",
        "timestamp": 1529934920
    },
    {
        "content": "<p>You can try cloning the jenkins crate locally and adding drop impls which print 'dropping' to the struct you are deserializing as a spot-check, but that does seem unlikely to indicate problems...</p>",
        "id": 128601565,
        "sender_full_name": "simulacrum",
        "timestamp": 1529935502
    },
    {
        "content": "<p>I've thrown in <code>mem::drop(..)</code> manually and I can't see anywhere that they override or mess with the default <code>Drop</code>.</p>",
        "id": 128601584,
        "sender_full_name": "davidtwco",
        "timestamp": 1529935535
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span>n1: 2799128 0x15E065: serde_json::de::from_reader (de.rs:2178)\n n1: 2799128 0x1A3CC8: reqwest::response::Response::json (response.rs:167)\n  n1: 2799128 0x1908E8: jenkins_api::home::&lt;impl jenkins_api::client::Jenkins&gt;::get_home (home.rs:51)\n   n1: 2799128 0x132A4A: poller_repro::buggy_code (main.rs:72)\n    n1: 2799128 0x133977: poller_repro::run_threads::{{closure}} (dfa.rs:650)\n</pre></div>\n\n\n<p>There's not a lot of \"API\" around the serde code though</p>",
        "id": 128601607,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529935562
    },
    {
        "content": "<p>Instead of printing, you may want like an atomic variable that counts creations and one that counts drops, then print that when it should be equal</p>",
        "id": 128601659,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529935626
    },
    {
        "content": "<p>That specifically isn't the big allocation, the call to <code>get_home</code> is fine. The massif data lists allocation backtraces by time, so if you go to the bottom of that dump, then you'll see the big allocation - at the start that allocation may be biggest but it doesn't grow and quickly gets overtaken.</p>",
        "id": 128601667,
        "sender_full_name": "davidtwco",
        "timestamp": 1529935644
    },
    {
        "content": "<p>valgrind's memcheck might tell you where you're missing memory, too</p>",
        "id": 128601674,
        "sender_full_name": "simulacrum",
        "timestamp": 1529935656
    },
    {
        "content": "<p>It prints out \"leaks\"</p>",
        "id": 128601682,
        "sender_full_name": "simulacrum",
        "timestamp": 1529935665
    },
    {
        "content": "<p>It's the <code>get_full_job</code> call that seems to allocate a lot. I've ran heaptrack on it and observed that its lots of small allocations rather than one large one.</p>",
        "id": 128601732,
        "sender_full_name": "davidtwco",
        "timestamp": 1529935687
    },
    {
        "content": "<p>I've not yet run normal valgrind on the repro code, I'll do that.</p>",
        "id": 128601741,
        "sender_full_name": "davidtwco",
        "timestamp": 1529935710
    },
    {
        "content": "<p>The last thing in the file also doesn't have much:</p>\n<div class=\"codehilite\"><pre><span></span>n1: 11111824 0x13B4B5: serde_json::de::from_reader (de.rs:2178)\n n1: 11111824 0x128528: reqwest::response::Response::json (response.rs:167)\n  n1: 11111824 0x151D72: &lt;jenkins_api::job::common::ShortJob&lt;T&gt;&gt;::get_full_job (common.rs:93)\n   n1: 11111824 0x132F9E: poller_repro::buggy_code2 (main.rs:81)\n    n1: 11111824 0x132E38: poller_repro::buggy_code (dfa.rs:1417)\n</pre></div>",
        "id": 128601744,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529935718
    },
    {
        "content": "<p>I reinstalled my OS recently, so now I'm reinstalling Xcode to get those profiling tools</p>",
        "id": 128601779,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529935801
    },
    {
        "content": "<p>just to see if I can do anything</p>",
        "id": 128601827,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529935808
    },
    {
        "content": "<p>I recommend using massif_visualizer or ms_print to read it rather than just looking at the raw dump.</p>",
        "id": 128601833,
        "sender_full_name": "davidtwco",
        "timestamp": 1529935834
    },
    {
        "content": "<p>I've updated the gist with some public Jenkins servers (this doesn't poll too hard, so it's probably alright to run it for a minute or two to see the memory tick up, sorry Ubuntu people!) that way you'll be able to see it happening.</p>",
        "id": 128601869,
        "sender_full_name": "davidtwco",
        "timestamp": 1529935920
    },
    {
        "content": "<p>A very small scale DDOS, nice</p>",
        "id": 128601936,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529935982
    },
    {
        "content": "<p>I've added Valgrind data, I had to CTRL+C because it would have taken a few hours on my local test Jenkii with the Valgrind performance hit, but you can probably see the largest allocation at the bottom as the one that is problematic.</p>",
        "id": 128602972,
        "sender_full_name": "davidtwco",
        "timestamp": 1529937339
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116107\">@David Wood</span> this does exit normally after a while , yah?</p>",
        "id": 128604267,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529938985
    },
    {
        "content": "<p>Once it has looped over every job, there's a <em>lot</em> on the Ubuntu jenkins so it will take a while. Normally a minute or so is enough to demonstrate the effect in htop/etc.</p>",
        "id": 128604297,
        "sender_full_name": "davidtwco",
        "timestamp": 1529939034
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/JWZH9GTV3IzNGRs-x66CBr8e/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/JWZH9GTV3IzNGRs-x66CBr8e/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/JWZH9GTV3IzNGRs-x66CBr8e/pasted_image.png\"></a></div>",
        "id": 128604349,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529939065
    },
    {
        "content": "<p>It will go back down once some threads finish.</p>",
        "id": 128604357,
        "sender_full_name": "davidtwco",
        "timestamp": 1529939081
    },
    {
        "content": "<p>no leaks <em>reported</em></p>",
        "id": 128604360,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529939087
    },
    {
        "content": "<p>That peak is ~750MB</p>",
        "id": 128604394,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529939138
    },
    {
        "content": "<p>Do you see the memory just keep increasing when it is running? I don't know if it leaks memory necessarily. But if you run it with the <code>run_no_threads</code>, it hovers around 0.1-0.5% system usage on my machine, whereas with <code>run_threads</code> it will go to 20+, 40+% eventually (though sometimes less too).</p>",
        "id": 128604445,
        "sender_full_name": "davidtwco",
        "timestamp": 1529939166
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/l0HAI8bpn7yBLc0L5x-fduN4/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/l0HAI8bpn7yBLc0L5x-fduN4/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/l0HAI8bpn7yBLc0L5x-fduN4/pasted_image.png\"></a></div>",
        "id": 128604497,
        "sender_full_name": "davidtwco",
        "timestamp": 1529939250
    },
    {
        "content": "<p>That was the whole run for ubuntu repos it went up was stable, went down</p>",
        "id": 128604601,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529939384
    },
    {
        "content": "<p>I mean, I guess it could be related to the specific Jenkii I'm testing with. But it seems odd that the memory usage just keeps going up for me when running with multiple threads - I'd imagine there'd be more memory used, sure, but it would be a constant factor on the single-threaded version.</p>",
        "id": 128604695,
        "sender_full_name": "davidtwco",
        "timestamp": 1529939494
    },
    {
        "content": "<p>did you graph your ubuntu version?</p>",
        "id": 128604751,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529939547
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116107\">@David Wood</span> It's worth pointing out that it looks like the memory is in the Error since at least for me I don't see the println in the Ok at all</p>",
        "id": 128604865,
        "sender_full_name": "simulacrum",
        "timestamp": 1529939694
    },
    {
        "content": "<p>I haven't with the ubuntu repos since I've got local ones to test with. I'll kick that off now. Unless my Jenkii are returning some strange malformed stuff that is causing the memory to just keep increasing..? Seems strange.</p>",
        "id": 128604867,
        "sender_full_name": "davidtwco",
        "timestamp": 1529939695
    },
    {
        "content": "<p>Specifically I don't see <a href=\"https://gist.github.com/davidtwco/46b9a212654c9b8d8061df056467a20d#file-main-rs-L77\" target=\"_blank\" title=\"https://gist.github.com/davidtwco/46b9a212654c9b8d8061df056467a20d#file-main-rs-L77\">https://gist.github.com/davidtwco/46b9a212654c9b8d8061df056467a20d#file-main-rs-L77</a> at all with the gist as is</p>",
        "id": 128604891,
        "sender_full_name": "simulacrum",
        "timestamp": 1529939732
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span>thread &#39;ubuntu3&#39; panicked at &#39;called `Result::unwrap()` on an `Err` value: Error { kind: Json(Error(&quot;Unexpected eof during chunk size line&quot;, line: 1, column: 19333120)), url: None }&#39;, libcore/result.rs:945:5\nnote: Run with `RUST_BACKTRACE=1` for a backtrace.\nthread &#39;ubuntu2&#39; panicked at &#39;called `Result::unwrap()` on an `Err` value: Error { kind: Json(Error(&quot;Unexpected eof during chunk size line&quot;, line: 1, column: 23142400)), url: None }&#39;, libcore/result.rs:945:5\nthread &#39;ubuntu1&#39; panicked at &#39;called `Result::unwrap()` on an `Err` value: Error { kind: Json(Error(&quot;Unexpected eof during chunk size line&quot;, line: 1, column: 26378240)), url: None }&#39;, libcore/result.rs:945:5\n</pre></div>",
        "id": 128604909,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529939762
    },
    {
        "content": "<p>I didn't make it particularly robust, but I see job names being printed regularly.</p>",
        "id": 128604953,
        "sender_full_name": "davidtwco",
        "timestamp": 1529939796
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116107\">@David Wood</span> It looks to me like the main allocations that keep growing come from the jenkins.get_home() or something -- not from get_full_job</p>",
        "id": 128605053,
        "sender_full_name": "simulacrum",
        "timestamp": 1529939896
    },
    {
        "content": "<p>It's possible, I only say <code>get_full_job</code> because of all the various massif and heaptrack runs I've done, I always see <code>get_full_job</code> as the largest contributor.</p>",
        "id": 128605081,
        "sender_full_name": "davidtwco",
        "timestamp": 1529939936
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116107\">@David Wood</span>  Are you running in dev or release?</p>",
        "id": 128605085,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529939943
    },
    {
        "content": "<p>(I'm using a debug build)</p>",
        "id": 128605101,
        "sender_full_name": "simulacrum",
        "timestamp": 1529939952
    },
    {
        "content": "<p>Currently dev, but I've seen it in release too.</p>",
        "id": 128605103,
        "sender_full_name": "davidtwco",
        "timestamp": 1529939956
    },
    {
        "content": "<p>I originally didn't notice it in the normal application because I ran it in short bursts to test - it was only when I deployed it with a release build did I see it get killed and started investigating.</p>",
        "id": 128605130,
        "sender_full_name": "davidtwco",
        "timestamp": 1529939996
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> On the Ubuntu Jenkins, all I get is <code>Err(e)</code> too. But on my own Jenkins instances, I see <code>Ok(..)</code>.</p>",
        "id": 128605328,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940198
    },
    {
        "content": "<p>Hadn't noticed that before.</p>",
        "id": 128605329,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940203
    },
    {
        "content": "<p>I still see the steady increase though.</p>",
        "id": 128605334,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940214
    },
    {
        "content": "<p>It might be that the increase is not as dramatic if the data returned from the API calls is smaller - on my own Jenkins instances, they return quite a lot of data, probably more than the individual ubuntu instance API calls are returning - probably why I see more memory used.</p>",
        "id": 128605410,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940268
    },
    {
        "content": "<p>Though the same increasing effect is observed.</p>",
        "id": 128605418,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940284
    },
    {
        "content": "<p>Have you seen that the no threads function does not have a steady increase but stays pretty much the same? That's mostly where I'm confused - if that increased gradually, then at least it wouldn't be the threads. But it's that when introducing threads - that don't interact - suddenly there's an increase rather than a constant factor more memory usage.</p>",
        "id": 128605498,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940370
    },
    {
        "content": "<p>Let me look</p>",
        "id": 128605623,
        "sender_full_name": "simulacrum",
        "timestamp": 1529940547
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/Hdi3nTfpV73-cps5ElmpfZJV/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a>  <br>\nmy graph for the ubuntu ones - there's still a increase before it crashes, though not as sharp as I think it is returning less information in the API calls.</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/Hdi3nTfpV73-cps5ElmpfZJV/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/Hdi3nTfpV73-cps5ElmpfZJV/pasted_image.png\"></a></div>",
        "id": 128605693,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940603
    },
    {
        "content": "<p>How big is your <code>instances</code>? e.g. how many threads?</p>",
        "id": 128605726,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529940658
    },
    {
        "content": "<p>Yeah - I don't think there's a memory leak, it is freed when the thread terminates at the end (if allowed to finish naturally), but I'd expect it to be freed before that (and it seemingly is when threads are not involved).</p>",
        "id": 128605732,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940663
    },
    {
        "content": "<p>Same as the example I gave for ubuntu, five.</p>",
        "id": 128605738,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940673
    },
    {
        "content": "<p>Though I have reproduced it with less (I forget exactly how few I've tried - I was checking whether any one of my Jenkins instances was causing the issue but alas, no)</p>",
        "id": 128605744,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940681
    },
    {
        "content": "<p>Is it possible that while the majority of the <em>allocations</em> come from the get_full_job call the majority of the memory use is the jobs list (which lives until thread end)</p>",
        "id": 128605818,
        "sender_full_name": "simulacrum",
        "timestamp": 1529940735
    },
    {
        "content": "<p>I had considered that, but looking through the source of <code>jenkins-api</code>, <code>get_full_job</code> just takes the url of the job (a <code>String</code>)  from the <code>ShortJob</code> in <code>home.jobs</code> and does a separate query on that (or slightly modified) URL - returning a new struct that we own. I'd understand it if it was a reference that had the same lifetime as the Jenkins instance, or as the <code>ShortJob</code>, but it isn't?</p>",
        "id": 128605860,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940835
    },
    {
        "content": "<p>Yeah, I think you can see it with just one thread (that isn't main thread)</p>",
        "id": 128605920,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529940875
    },
    {
        "content": "<p>And even if that were the case, why wouldn't we see an, albeit smaller, but steady increase with the one thread?</p>",
        "id": 128605934,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940887
    },
    {
        "content": "<p>Yes but <code>ShortJob</code> seems to be a fairly large struct in theory -- it has that #[serde(flatten)] other field...</p>\n<p>In other words, <code>ShortJob</code> lives until thread end</p>",
        "id": 128605935,
        "sender_full_name": "simulacrum",
        "timestamp": 1529940890
    },
    {
        "content": "<p>Ah, yeah, good point.</p>",
        "id": 128605953,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940904
    },
    {
        "content": "<p>So maybe we're basically seeing the same thing it's just less noticeable in the non-threaded case...</p>",
        "id": 128605974,
        "sender_full_name": "simulacrum",
        "timestamp": 1529940935
    },
    {
        "content": "<blockquote>\n<p>Yeah, I think you can see it with just one thread (that isn't main thread)</p>\n</blockquote>\n<p>Is this referring to the how many instances query?</p>",
        "id": 128606030,
        "sender_full_name": "davidtwco",
        "timestamp": 1529940967
    },
    {
        "content": "<p>It's also possibly worth noting that it looks like these responses are fairly huge -- &gt;45 MB for the one that crashes at least... and serde is likely storing effectively the whole response in memory via flatten etc</p>",
        "id": 128606083,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941045
    },
    {
        "content": "<blockquote>\n<p>So maybe we're basically seeing the same thing it's just less noticeable in the non-threaded case...</p>\n</blockquote>\n<p>Perhaps. Looking at a API response from Jenkins (just prefix a URL of a job with <code>/api/json</code>), the shortjob normally is only like three fields.</p>",
        "id": 128606095,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941053
    },
    {
        "content": "<p>hm, okay</p>",
        "id": 128606109,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941073
    },
    {
        "content": "<p>Here's <a href=\"https://jenkins.qa.ubuntu.com/view/All/job/account-plugins-utopic-amd64-ci/api/json\" target=\"_blank\" title=\"https://jenkins.qa.ubuntu.com/view/All/job/account-plugins-utopic-amd64-ci/api/json\">an example</a>. It has two builds whereas in my internal Jenkins instances, I have potentially hundreds, so the response is huge.</p>",
        "id": 128606175,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941128
    },
    {
        "content": "<p>Threaded case for me:</p>\n<div class=\"codehilite\"><pre><span></span>GB\n1.760^                                                          #\n     |                                                       @@@#         :@:\n     |                                                    @@@@@@#      ::::@:\n     |                                                 @@@@@@@@@#:  ::@::::@:\n     |                                             ::::@@@@@@@@@#:::::@::::@:\n     |                                            :::: @@@@@@@@@#:::::@::::@:\n     |                                        :@:::::: @@@@@@@@@#:::::@::::@:\n     |                                     @:@:@: :::: @@@@@@@@@#:::::@::::@:\n     |                                  ::@@:@:@: :::: @@@@@@@@@#:::::@::::@:\n     |                               ::@::@@:@:@: :::: @@@@@@@@@#:::::@::::@:\n     |                             @@: @::@@:@:@: :::: @@@@@@@@@#:::::@::::@::\n     |                         :@@@@@: @::@@:@:@: :::: @@@@@@@@@#:::::@::::@::\n     |                       @@:@@ @@: @::@@:@:@: :::: @@@@@@@@@#:::::@::::@::\n     |                    @@@@@:@@ @@: @::@@:@:@: :::: @@@@@@@@@#:::::@::::@::\n     |                 @@@@ @@@:@@ @@: @::@@:@:@: :::: @@@@@@@@@#:::::@::::@::\n     |              @@@@@@@ @@@:@@ @@: @::@@:@:@: :::: @@@@@@@@@#:::::@::::@::\n     |           @@:@@ @@@@ @@@:@@ @@: @::@@:@:@: :::: @@@@@@@@@#:::::@::::@::\n     |        :@@@ :@@ @@@@ @@@:@@ @@: @::@@:@:@: :::: @@@@@@@@@#:::::@::::@::\n     |     :@@@@@@ :@@ @@@@ @@@:@@ @@: @::@@:@:@: :::: @@@@@@@@@#:::::@::::@::\n     |  @@@:@ @@@@ :@@ @@@@ @@@:@@ @@: @::@@:@:@: :::: @@@@@@@@@#:::::@::::@::\n   0 +-----------------------------------------------------------------------&gt;Gi\n     0                                                                   68.91\n</pre></div>\n\n\n<p>Single:</p>\n<div class=\"codehilite\"><pre><span></span>    MB\n458.3^              #\n     |              #                                          :             :\n     |              #                           :             ::             @\n     |            @@#            :@            ::             ::            :@\n     |            @ #           ::@           :::            :::           ::@\n     |           :@ #          :::@           :::           ::::          :::@\n     |           :@ #          :::@          ::::           ::::          :::@\n     |         :::@ #         ::::@         :::::         ::::::         ::::@\n     |         : :@ #        :::::@         :::::         : ::::        :::::@\n     |        @: :@ #        :::::@       :::::::        :: ::::       ::::::@\n     |        @: :@ #      :::::::@       : :::::       ::: ::::      :::::::@\n     |      @@@: :@ #      : :::::@      :: :::::      :::: ::::      :::::::@\n     |      @ @: :@ #     @: :::::@     @:: :::::      :::: ::::      :::::::@\n     |     @@ @: :@ #     @: :::::@     @:: :::::     ::::: ::::     @:::::::@\n     |    @@@ @: :@ #   ::@: :::::@    :@:: :::::     ::::: ::::    :@:::::::@\n     |    @@@ @: :@ #   : @: :::::@    :@:: :::::   ::::::: ::::   ::@:::::::@\n     |  @@@@@ @: :@ #  :: @: :::::@   ::@:: :::::   : ::::: ::::   ::@:::::::@\n     |  @ @@@ @: :@ #  :: @: :::::@   ::@:: :::::   : ::::: ::::  :::@:::::::@\n     | @@ @@@ @: :@ # ::: @: :::::@ ::::@:: :::::  :: ::::: :::: ::::@:::::::@\n     | @@ @@@ @: :@ #:::: @: :::::@:: ::@:: :::::  :: ::::: :::: ::::@:::::::@\n   0 +-----------------------------------------------------------------------&gt;Gi\n     0                                                                   71.92\n</pre></div>",
        "id": 128606208,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941166
    },
    {
        "content": "<p>Each entry in the <code>build</code> list from the response being a <code>ShortJob</code>. It takes the url from it, appends <code>/api/json</code> and parses that into a <code>CommonJob</code> in <code>get_full_job</code>.</p>",
        "id": 128606218,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941189
    },
    {
        "content": "<p>I think that is what I would expect, in the single threaded case, it frees each <code>CommonJob</code> and then fetches another. In the multi-threaded case, it just keeps growing (as if those <code>CommonJob</code>s, allocated by <code>get_full_job</code> are not being freed).</p>",
        "id": 128606296,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941234
    },
    {
        "content": "<p>We're freeing common jobs -- just not short jobs</p>",
        "id": 128606308,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941258
    },
    {
        "content": "<p>or at least, we should be</p>",
        "id": 128606319,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941269
    },
    {
        "content": "<p>Yeah, that's what I'd expect. But we only fetch the <code>ShortJob</code>s once - in the <code>get_home</code> call - so it shouldn't be increasing over time. (I realise that the API example I gave previously was slightly incorrect, as we get the short job from the home query and then the <code>CommonJob</code> from the job page query - the builds key we were looking at is another thing entirely).</p>",
        "id": 128606483,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941399
    },
    {
        "content": "<p>Only the <code>CommonJob</code>s are fetched over time, and therefore could contribute to the increasing memory (at least as far as I can work out).</p>",
        "id": 128606512,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941434
    },
    {
        "content": "<p>I agree that what I would expect is that as we fetch <code>ShortJob</code>s we should increase (faster in the threaded case) and then the commonjobs should be a up/down (effectively average flat line)</p>",
        "id": 128606561,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941449
    },
    {
        "content": "<p>Yeah, and the <code>ShortJob</code>s should live the duration of the execution.</p>",
        "id": 128606583,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941471
    },
    {
        "content": "<p>(This is where we get the <code>ShortJob</code> from in the API - <a href=\"https://jenkins.qa.ubuntu.com//api/json\" target=\"_blank\" title=\"https://jenkins.qa.ubuntu.com//api/json\">link</a> (ala <code>get_home</code>) - and then something like this - <a href=\"https://jenkins.qa.ubuntu.com/view/All/job/account-plugins-utopic-amd64-ci/api/json\" target=\"_blank\" title=\"https://jenkins.qa.ubuntu.com/view/All/job/account-plugins-utopic-amd64-ci/api/json\">link</a> (ala <code>get_full_job</code>) - is a <code>CommonJob</code> - I got this mixed up earlier).</p>",
        "id": 128606624,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941514
    },
    {
        "content": "<p>CommonJobs do look fairly small</p>",
        "id": 128606701,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941582
    },
    {
        "content": "<p>I'd expect a increase in memory usage at the start to a sort-of base-level as all the <code>ShortJob</code> are loaded, then up and down spikes as <code>CommonJob</code> is loaded and unloaded as the iteration proceeds.</p>",
        "id": 128606707,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941601
    },
    {
        "content": "<p>That reponse is slightly less as it doesn't use <code>?depth=1</code> like the example code (just returns more info - don't think it makes a difference in terms of this behaviour we're seeing) and the job I linked has very few builds.</p>",
        "id": 128606745,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941664
    },
    {
        "content": "<p>I apologize if I'm doing a awful job at explaining and clarifying what I mean.</p>",
        "id": 128606802,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941701
    },
    {
        "content": "<p>massif is reporting that ~60% of the allocated bytes come from the home call</p>",
        "id": 128606830,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941760
    },
    {
        "content": "<p>Or at least that's how I understand the output</p>",
        "id": 128606856,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941796
    },
    {
        "content": "<p>So maybe in both cases we're seeing the same behavior effectively -- steady growth, and then the get_full_job calls are fast and as such almost unnoticeable for ms_print... let me try switching it to time-based graph</p>",
        "id": 128606916,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941829
    },
    {
        "content": "<p>Are you using massif visualizer to view the data in a nice way?</p>",
        "id": 128606951,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941896
    },
    {
        "content": "<p>ms_print</p>",
        "id": 128606953,
        "sender_full_name": "simulacrum",
        "timestamp": 1529941905
    },
    {
        "content": "<p>Here's a link to some other massif data I've collected: <a href=\"https://www.dropbox.com/sh/5i0vxq19v5tr4wy/AAAuZHS2atedrv6eDgCxx_8oa?dl=0\" target=\"_blank\" title=\"https://www.dropbox.com/sh/5i0vxq19v5tr4wy/AAAuZHS2atedrv6eDgCxx_8oa?dl=0\">link</a> with my own Jenkins instances.</p>\n<div class=\"message_inline_ref\"><a href=\"https://www.dropbox.com/sh/5i0vxq19v5tr4wy/AAAuZHS2atedrv6eDgCxx_8oa?dl=0\" target=\"_blank\" title=\"Temporary Jenkins Memory Profiling\"><img src=\"https://www.dropbox.com/static/images/spectrum-icons/generated/content/content-folder_dropbox-large.png\"></a><div><div class=\"message_inline_image_title\">Temporary Jenkins Memory Profiling</div><desc class=\"message_inline_image_desc\"></desc></div></div>",
        "id": 128607007,
        "sender_full_name": "davidtwco",
        "timestamp": 1529941950
    },
    {
        "content": "<blockquote>\n<p>line: 1, column: 35758080</p>\n</blockquote>\n<p>35,758,080 that's 35MB</p>",
        "id": 128607533,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529942435
    },
    {
        "content": "<p>What's that from?</p>",
        "id": 128607544,
        "sender_full_name": "davidtwco",
        "timestamp": 1529942452
    },
    {
        "content": "<p>that seems odd, are any of these responses supposed to be that big</p>",
        "id": 128607546,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529942455
    },
    {
        "content": "<blockquote>\n<p>thread 'ubuntu2' panicked at 'called <code>Result::unwrap()</code> on an <code>Err</code> value: Error { kind: Json(Error(\"Unexpected eof during chunk size line\", line: 1, column: 35758080)), url: None }', libcore/result.rs:945:5</p>\n</blockquote>",
        "id": 128607558,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529942465
    },
    {
        "content": "<p>Yeah, not sure. There doesn't seem to be a difference between the two profiles really, as far as I can tell</p>",
        "id": 128607561,
        "sender_full_name": "simulacrum",
        "timestamp": 1529942468
    },
    {
        "content": "<p>Yeah, they certainly could be. Jenkins can return <em>lots</em> of data.</p>",
        "id": 128607566,
        "sender_full_name": "davidtwco",
        "timestamp": 1529942482
    },
    {
        "content": "<p>jeez jenkins</p>",
        "id": 128607570,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529942489
    },
    {
        "content": "<p>I have jobs that just time out because we can't get the data fast enough.</p>",
        "id": 128607580,
        "sender_full_name": "davidtwco",
        "timestamp": 1529942517
    },
    {
        "content": "<p>Do either of you agree that there is some strange behaviour here or am I just going mad?</p>",
        "id": 128607625,
        "sender_full_name": "davidtwco",
        "timestamp": 1529942530
    },
    {
        "content": "<p>RIIR</p>",
        "id": 128607639,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529942552
    },
    {
        "content": "<p>If the common jobs are actually large, then yes, but if they're very small, then the results I get are expected</p>",
        "id": 128607676,
        "sender_full_name": "simulacrum",
        "timestamp": 1529942604
    },
    {
        "content": "<p>How is it explained if they are small?</p>",
        "id": 128607740,
        "sender_full_name": "davidtwco",
        "timestamp": 1529942643
    },
    {
        "content": "<p>Well, mostly explained -- we see growth only as the get_home call runs and then growth \"stops\" in theory if I'm right about where this comes from</p>",
        "id": 128607766,
        "sender_full_name": "simulacrum",
        "timestamp": 1529942687
    },
    {
        "content": "<p>I see the get_home call complete in ~2-4 seconds and then the iteration through it take &lt;1 second</p>",
        "id": 128607884,
        "sender_full_name": "simulacrum",
        "timestamp": 1529942852
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/Ncf4JPiyzPr9Ps4csnlWR6Hj/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a>  Here's an example of a large <code>CommonJob</code> response I picked at random from my own instances.</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/Ncf4JPiyzPr9Ps4csnlWR6Hj/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/Ncf4JPiyzPr9Ps4csnlWR6Hj/pasted_image.png\"></a></div>",
        "id": 128607887,
        "sender_full_name": "davidtwco",
        "timestamp": 1529942854
    },
    {
        "content": "<p>i.e., sure, we have large CommonJobs but they're nearly immediately deallocated</p>",
        "id": 128607984,
        "sender_full_name": "simulacrum",
        "timestamp": 1529942922
    },
    {
        "content": "<p>If the <code>CommonJob</code> allocations are small (or being correctly deallocated), then do you propose the <code>get_home</code> call is allocating the growing memory over time or something else?</p>",
        "id": 128607996,
        "sender_full_name": "davidtwco",
        "timestamp": 1529942941
    },
    {
        "content": "<p>right, yeah, that's where I think the memory growth comes from</p>",
        "id": 128608012,
        "sender_full_name": "simulacrum",
        "timestamp": 1529942957
    },
    {
        "content": "<p>Wouldn't that only allocate as the <code>get_home</code> call is happening - and afterwards during iteration, be constant?</p>",
        "id": 128608041,
        "sender_full_name": "davidtwco",
        "timestamp": 1529943002
    },
    {
        "content": "<p>right but I see iteration happen instantaneously in a release build at least</p>",
        "id": 128608093,
        "sender_full_name": "simulacrum",
        "timestamp": 1529943017
    },
    {
        "content": "<p>I'm open to the idea, whatever it is that causes this, I'd just want to figure out why it's happening. I've not looked into <code>get_home</code> much because my massif profiles have always pointed at <code>get_full_job</code>.</p>",
        "id": 128608113,
        "sender_full_name": "davidtwco",
        "timestamp": 1529943051
    },
    {
        "content": "<p>I don't see get_full_job in profiles at all...</p>",
        "id": 128608133,
        "sender_full_name": "simulacrum",
        "timestamp": 1529943097
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/ZC0sGMh9dAaBe7xCv9UIcKEc/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a> <br>\nThis is from the massif visualizer for data I've shared (particularly 19141) - if I go to the later snapshots, and dig down into the largest allocation location, I see <code>get_full_job</code>.</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/ZC0sGMh9dAaBe7xCv9UIcKEc/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/ZC0sGMh9dAaBe7xCv9UIcKEc/pasted_image.png\"></a></div>",
        "id": 128608231,
        "sender_full_name": "davidtwco",
        "timestamp": 1529943202
    },
    {
        "content": "<p>Let me try loading that file</p>",
        "id": 128608250,
        "sender_full_name": "simulacrum",
        "timestamp": 1529943231
    },
    {
        "content": "<p>(the image isn't great because I run massif visualizer in a VM - not supported on Windows - and the VM can't be larger than 1920x1080 and I need to resize the window partially offscreen to fit it all in because there's no horizontal scrollbar)</p>",
        "id": 128608298,
        "sender_full_name": "davidtwco",
        "timestamp": 1529943246
    },
    {
        "content": "<p>I see the same result in all my profiles.</p>",
        "id": 128608305,
        "sender_full_name": "davidtwco",
        "timestamp": 1529943257
    },
    {
        "content": "<p>The massif data lists the backtraces of memory allocations, ordered by % of total allocated memory which came from that location, for each snapshot in time, the most recent snapshot at the bottom.</p>",
        "id": 128608360,
        "sender_full_name": "davidtwco",
        "timestamp": 1529943296
    },
    {
        "content": "<p>So it ends up being a little bit up from the bottom of the file that you see the big \"89% of allocated memory from X\" in \"snapshot 47\" (for example) one that we want.</p>",
        "id": 128608440,
        "sender_full_name": "davidtwco",
        "timestamp": 1529943336
    },
    {
        "content": "<p>hm, yeah, that might be debug vs. release build difference</p>",
        "id": 128608504,
        "sender_full_name": "simulacrum",
        "timestamp": 1529943395
    },
    {
        "content": "<p>let me try a debug build...</p>",
        "id": 128608516,
        "sender_full_name": "simulacrum",
        "timestamp": 1529943398
    },
    {
        "content": "<p>It's interesting that there is a difference - I've not profiled the release builds, only observed the issue on them.</p>",
        "id": 128608601,
        "sender_full_name": "davidtwco",
        "timestamp": 1529943508
    },
    {
        "content": "<p>well, in theory we're not reading anything but display_name from full job which is tiny so LLVM could skip a bunch of allocation in theory</p>",
        "id": 128608709,
        "sender_full_name": "simulacrum",
        "timestamp": 1529943614
    },
    {
        "content": "<p>Ah. In the main application that I encountered this on, I take much more from the struct, so in release builds of that (which is what I've checked), it would still be pretty bad.</p>",
        "id": 128608735,
        "sender_full_name": "davidtwco",
        "timestamp": 1529943668
    },
    {
        "content": "<p>Printing the name was all that was included here as it was sufficient to demonstrate it.</p>",
        "id": 128608744,
        "sender_full_name": "davidtwco",
        "timestamp": 1529943684
    },
    {
        "content": "<p>that could explain it</p>",
        "id": 128608821,
        "sender_full_name": "simulacrum",
        "timestamp": 1529943763
    },
    {
        "content": "<p>Would it be possible to send your massif output <span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> so I can see what it looks like? Curious about the increased allocations from <code>get_home</code>.</p>",
        "id": 128609149,
        "sender_full_name": "davidtwco",
        "timestamp": 1529944203
    },
    {
        "content": "<p>Sure, yeah</p>",
        "id": 128609271,
        "sender_full_name": "simulacrum",
        "timestamp": 1529944286
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/raBzSVaFWm-cgdIdE7jbbs_w/massif.out.single.bytes\" target=\"_blank\" title=\"massif.out.single.bytes\">massif.out.single.bytes</a> <a href=\"/user_uploads/4715/X4aVLafF02wPx52SeYGP1Rhh/massif.out.single\" target=\"_blank\" title=\"massif.out.single\">massif.out.single</a> <a href=\"/user_uploads/4715/U9rx_QTpQwbYAklBsFVvXOgU/massif.out.threaded.bytes\" target=\"_blank\" title=\"massif.out.threaded.bytes\">massif.out.threaded.bytes</a></p>",
        "id": 128609326,
        "sender_full_name": "simulacrum",
        "timestamp": 1529944332
    },
    {
        "content": "<p>What's different about the <code>*.bytes</code> profiles?</p>",
        "id": 128609342,
        "sender_full_name": "davidtwco",
        "timestamp": 1529944375
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/9za0eoHccR1uVQPcTk2MkvNz/massif.out.threaded\" target=\"_blank\" title=\"massif.out.threaded\">massif.out.threaded</a> <a href=\"/user_uploads/4715/-KyHWIVvMl1rUjspw6vI52xW/massif.debug.multi\" target=\"_blank\" title=\"massif.debug.multi\">massif.debug.multi</a></p>",
        "id": 128609343,
        "sender_full_name": "simulacrum",
        "timestamp": 1529944376
    },
    {
        "content": "<p>They aren't all that interesting -- <code>--time-unit=B</code> vs. <code>--time-unit=I</code></p>",
        "id": 128609351,
        "sender_full_name": "simulacrum",
        "timestamp": 1529944402
    },
    {
        "content": "<p>Ah, cool.</p>",
        "id": 128609356,
        "sender_full_name": "davidtwco",
        "timestamp": 1529944411
    },
    {
        "content": "<p>That is interesting. They are very different from my own debug profiles.</p>",
        "id": 128609524,
        "sender_full_name": "davidtwco",
        "timestamp": 1529944569
    },
    {
        "content": "<p>all but .debug.multi I think are release profiles</p>",
        "id": 128609533,
        "sender_full_name": "simulacrum",
        "timestamp": 1529944584
    },
    {
        "content": "<p>but you should be able to see that in the profile (based on the command run)</p>",
        "id": 128609536,
        "sender_full_name": "simulacrum",
        "timestamp": 1529944594
    },
    {
        "content": "<p>Yeah. I think in .debug.multi you need <code>--depth=100</code> (or similar) in order to see what call it originates from as the backtrace is much larger on debug builds.</p>",
        "id": 128609553,
        "sender_full_name": "davidtwco",
        "timestamp": 1529944632
    },
    {
        "content": "<p>yeah, possible</p>",
        "id": 128609557,
        "sender_full_name": "simulacrum",
        "timestamp": 1529944653
    },
    {
        "content": "<p>Well, I can say that I'm no closer to understanding this problem. If anything, I've gotten more confused.</p>",
        "id": 128609619,
        "sender_full_name": "davidtwco",
        "timestamp": 1529944711
    },
    {
        "content": "<p>Mostly agreed -- it looks like there's somewhat of a \"leak\" in that memory use constantly increases but I can't see how that could be thread-related</p>",
        "id": 128609635,
        "sender_full_name": "simulacrum",
        "timestamp": 1529944750
    },
    {
        "content": "<p>Yeah, it's the threading that has been confusing me. I'd expect it to be a problem regardless of threads, or not at all.</p>",
        "id": 128609830,
        "sender_full_name": "davidtwco",
        "timestamp": 1529944998
    },
    {
        "content": "<p>However, I'm not completely certain that I see different behavior</p>",
        "id": 128610053,
        "sender_full_name": "simulacrum",
        "timestamp": 1529945264
    },
    {
        "content": "<p>I think it's the same thing, just manifesting itself differently in the profile - the end result of a gradually increasing memory footprint, only when threaded, is the same.</p>",
        "id": 128610117,
        "sender_full_name": "davidtwco",
        "timestamp": 1529945322
    },
    {
        "content": "<p>My theory, as much as it makes no sense, is that the <code>CommonJob</code>s from <code>get_full_job</code> is where the allocations are accumulating and for some reason, when multi-threaded, are dropped at the end of the thread, and when single-threaded, are dropped at the end of the function. It makes _zero_ sense, but that lines up with what I think I'm observing.</p>",
        "id": 128610142,
        "sender_full_name": "davidtwco",
        "timestamp": 1529945401
    },
    {
        "content": "<p>And with the graphs you posted earlier, if we assume that the <code>get_home</code> allocation is just a constant size that both include.</p>",
        "id": 128610194,
        "sender_full_name": "davidtwco",
        "timestamp": 1529945438
    },
    {
        "content": "<p>I don't think I see a difference threaded vs. not threaded -- both profiles have steady increase for thread lifetime</p>",
        "id": 128610197,
        "sender_full_name": "simulacrum",
        "timestamp": 1529945440
    },
    {
        "content": "<p>i.e. there is no period in them when the bytes allocated is constant</p>",
        "id": 128610209,
        "sender_full_name": "simulacrum",
        "timestamp": 1529945463
    },
    {
        "content": "<p>I would expect that in both, we would see spikes of allocations and de-allocations, but the spikes when multi-threaded would be larger.</p>",
        "id": 128610223,
        "sender_full_name": "davidtwco",
        "timestamp": 1529945494
    },
    {
        "content": "<p>(in the ideal case)</p>",
        "id": 128610228,
        "sender_full_name": "davidtwco",
        "timestamp": 1529945516
    },
    {
        "content": "<p>But then again, I have no idea.</p>",
        "id": 128610277,
        "sender_full_name": "davidtwco",
        "timestamp": 1529945550
    },
    {
        "content": "<p>I'd expect that the thread upon start would show steady growth as get_home runs and then a spikey flat line for the iteration, allocating/deallocating commonjobs</p>",
        "id": 128610304,
        "sender_full_name": "simulacrum",
        "timestamp": 1529945616
    },
    {
        "content": "<p>Yeah, that's what I mean. But instead we see a steady increase instead of the spikes.</p>",
        "id": 128610355,
        "sender_full_name": "davidtwco",
        "timestamp": 1529945648
    },
    {
        "content": "<p>well, instead of the spikey flat line, right?</p>",
        "id": 128610362,
        "sender_full_name": "simulacrum",
        "timestamp": 1529945674
    },
    {
        "content": "<p>Yeah.</p>",
        "id": 128610367,
        "sender_full_name": "davidtwco",
        "timestamp": 1529945682
    },
    {
        "content": "<p>Yeah, I think so -- and we don't know why</p>",
        "id": 128610380,
        "sender_full_name": "simulacrum",
        "timestamp": 1529945708
    },
    {
        "content": "<p>I think we've now agreed on what the problem is.</p>",
        "id": 128610390,
        "sender_full_name": "davidtwco",
        "timestamp": 1529945741
    },
    {
        "content": "<p>Yes, though we're no closer to finding out why :)</p>",
        "id": 128610457,
        "sender_full_name": "simulacrum",
        "timestamp": 1529945807
    },
    {
        "content": "<p>Yes. In fact, I've regressed in my understanding of this issue since I discovered it last week.</p>",
        "id": 128610482,
        "sender_full_name": "davidtwco",
        "timestamp": 1529945840
    },
    {
        "content": "<p>That's usually a good thing, while not feeling like it ;-)</p>",
        "id": 128610923,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529946233
    },
    {
        "content": "<p>You know, I wonder if it's a bug with Hyper / Tokio</p>\n<p><a href=\"/user_uploads/4715/hzCG8q0GRVu3Kt5RWFEIo_Dm/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/hzCG8q0GRVu3Kt5RWFEIo_Dm/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/hzCG8q0GRVu3Kt5RWFEIo_Dm/pasted_image.png\"></a></div><p>I added some prints:</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"w\">    </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">job</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"n\">home</span><span class=\"p\">.</span><span class=\"n\">jobs</span><span class=\"p\">.</span><span class=\"n\">into_iter</span><span class=\"p\">().</span><span class=\"n\">enumerate</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">        </span><span class=\"n\">eprintln</span><span class=\"o\">!</span><span class=\"p\">(</span><span class=\"s\">&quot;Mark generation {} now&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">        </span><span class=\"k\">use</span><span class=\"w\"> </span><span class=\"n\">std</span>::<span class=\"n\">io</span>::<span class=\"n\">Write</span><span class=\"p\">;</span><span class=\"w\"></span>\n<span class=\"w\">        </span><span class=\"n\">std</span>::<span class=\"n\">io</span>::<span class=\"n\">stderr</span><span class=\"p\">().</span><span class=\"n\">flush</span><span class=\"p\">().</span><span class=\"n\">unwrap</span><span class=\"p\">();</span><span class=\"w\"></span>\n<span class=\"w\">        </span><span class=\"n\">thread</span>::<span class=\"n\">sleep_ms</span><span class=\"p\">(</span><span class=\"mi\">5_000</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">        </span><span class=\"n\">buggy_code2</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"n\">jenkins</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">job</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"p\">}</span><span class=\"w\"></span>\n</pre></div>\n\n\n<p>but those never showed up. It used 800+ MB without ever running that <code>eprintln</code>...</p>",
        "id": 128611357,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529946788
    },
    {
        "content": "<p>If that's from the initial population of home then that would be reasonable...</p>",
        "id": 128611758,
        "sender_full_name": "simulacrum",
        "timestamp": 1529947223
    },
    {
        "content": "<p>What do you mean by initial population? Does home not only get populated once (per thread)?</p>",
        "id": 128611774,
        "sender_full_name": "davidtwco",
        "timestamp": 1529947262
    },
    {
        "content": "<p>(may be confused with terminology)</p>",
        "id": 128611780,
        "sender_full_name": "davidtwco",
        "timestamp": 1529947277
    },
    {
        "content": "<blockquote>\n<p>This may be inconsequential, but <a href=\"https://github.com/sile/libflate/blob/master/src/non_blocking/deflate/decode.rs#L204-L225\" target=\"_blank\" title=\"https://github.com/sile/libflate/blob/master/src/non_blocking/deflate/decode.rs#L204-L225\">this code</a> came up when I was running massif the first few times, but on recent runs it hasn't shown up, not sure why. It struck me as potentially the issue, though unlikely.</p>\n</blockquote>\n<p>I sent this earlier, some other information - when I was originally debugging this particular part of code (when it was still coming up in backtraces, I've not looked since or investigated why it no longer is), I placed a conditional breakpoint in that code that never ever got triggered but the problem went away when I did so IIRC (this was a week ago).</p>",
        "id": 128611863,
        "sender_full_name": "davidtwco",
        "timestamp": 1529947401
    },
    {
        "content": "<blockquote>\n<p><code>impl&lt;'a&gt; ToString for Path&lt;'a&gt; {</code></p>\n</blockquote>\n<p>That's a PR waiting to happen</p>",
        "id": 128611953,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529947509
    },
    {
        "content": "<p>Yes, that initial and only population</p>",
        "id": 128611954,
        "sender_full_name": "simulacrum",
        "timestamp": 1529947511
    },
    {
        "content": "<p>Ah, I assumed when you said initial that you were implying there were further populations. My bad.</p>",
        "id": 128611978,
        "sender_full_name": "davidtwco",
        "timestamp": 1529947558
    },
    {
        "content": "<p>That code surface-level looks wrong to me (not panic safe)... but I don't obviously see how it could be related</p>",
        "id": 128612049,
        "sender_full_name": "simulacrum",
        "timestamp": 1529947633
    },
    {
        "content": "<p>I don't think it is related, but on an initial glance I thought it might make the buffer too large if there was a data race or something like that due to the unsafe block.</p>",
        "id": 128612120,
        "sender_full_name": "davidtwco",
        "timestamp": 1529947709
    },
    {
        "content": "<p>hm, I don't think so</p>",
        "id": 128612147,
        "sender_full_name": "simulacrum",
        "timestamp": 1529947766
    },
    {
        "content": "<p>Now I'm installing Wireshark to see what the network data is</p>",
        "id": 128612371,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529948030
    },
    {
        "content": "<p>So, that code is still being called. The <code>old_len</code> size does keep increasing (I kept setting increasingly larger and larger conditional breakpoints on line 213) and with a conditional breakpoint that will not be hit (checking for <code>old_len &lt; 0</code>), the memory usage does not increase (it is a lot slower, so I might just not have observed it yet).</p>",
        "id": 128612675,
        "sender_full_name": "davidtwco",
        "timestamp": 1529948326
    },
    {
        "content": "<p>Isn't that expected behavior though? i.e., that we want to grow the buffer?</p>",
        "id": 128612699,
        "sender_full_name": "simulacrum",
        "timestamp": 1529948383
    },
    {
        "content": "<p>At least I understood that code as intentionally growing the buffer</p>",
        "id": 128612707,
        "sender_full_name": "simulacrum",
        "timestamp": 1529948392
    },
    {
        "content": "<p>It is, yeah.</p>",
        "id": 128612753,
        "sender_full_name": "davidtwco",
        "timestamp": 1529948411
    },
    {
        "content": "<p>I think I may be jumping the gun, I think it's just increasing slowly.</p>",
        "id": 128612777,
        "sender_full_name": "davidtwco",
        "timestamp": 1529948467
    },
    {
        "content": "<p>And I know that the buffer isn't shared between threads after having dug into that library a bit.</p>",
        "id": 128612788,
        "sender_full_name": "davidtwco",
        "timestamp": 1529948491
    },
    {
        "content": "<p>(which makes sense, there's no reason for it to be shared)</p>",
        "id": 128612792,
        "sender_full_name": "davidtwco",
        "timestamp": 1529948508
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116107\">@David Wood</span> are you using https locally too? can you use not?</p>",
        "id": 128613158,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529948941
    },
    {
        "content": "<p>I'm only using http locally. It's not TLS by the looks of it.</p>",
        "id": 128613180,
        "sender_full_name": "davidtwco",
        "timestamp": 1529948971
    },
    {
        "content": "<p>ah. I think my errors were coming from https (at least switching to http gets further)</p>",
        "id": 128613357,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529949115
    },
    {
        "content": "<p>Yeah, normally mine will execute until the end (for my local Jenkins instances, it's around 27 mins).</p>",
        "id": 128613418,
        "sender_full_name": "davidtwco",
        "timestamp": 1529949161
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/KRLECS3vJCiqwZmCKEyk9Fto/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/KRLECS3vJCiqwZmCKEyk9Fto/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/KRLECS3vJCiqwZmCKEyk9Fto/pasted_image.png\"></a></div><p>Seems reasonably stable</p>",
        "id": 128613483,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529949259
    },
    {
        "content": "<p>clearly you need to switch to macOS</p>",
        "id": 128613500,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529949278
    },
    {
        "content": "<p>That's interesting - I wonder if is proportional to how many threads there are? I've had it happen on Linux and Windows (well, technically, inside WSL, but close enough).</p>",
        "id": 128613597,
        "sender_full_name": "davidtwco",
        "timestamp": 1529949367
    },
    {
        "content": "<p>In fact, that might make sense - when I ran the original program to completion with heaptrack (it took 2 days and 18 hours..), it started to plateau at around 7GB.</p>",
        "id": 128613636,
        "sender_full_name": "davidtwco",
        "timestamp": 1529949442
    },
    {
        "content": "<p>If a relationship between number of threads and maximum memory (and size of responses from Jenkins) holds, then I guess the question is, why does increasing the number of threads have a large (non-linear?) impact on memory usage of the threads?</p>",
        "id": 128613687,
        "sender_full_name": "davidtwco",
        "timestamp": 1529949488
    },
    {
        "content": "<p>Running a single-instance, single-threaded process for each Jenkins instance would result in a great deal less memory being used (which is how I'm working around this for now, I'm really just curious why it is happening)</p>",
        "id": 128613813,
        "sender_full_name": "davidtwco",
        "timestamp": 1529949652
    },
    {
        "content": "<p>I mean, I still think there's something truly wrong cause there was only like a few MB of data transferred. Where are 100MB coming form</p>",
        "id": 128614357,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529950328
    },
    {
        "content": "<p>Right, it's not clear where the memory use is coming from</p>",
        "id": 128614825,
        "sender_full_name": "simulacrum",
        "timestamp": 1529950885
    },
    {
        "content": "<p>When I run heaptrack, it shows the size of allocations as a bar chart. The vast majority of allocations are very small. For that reason I've always considered it a \"why isn't it being dropped\" rather than \"why is it being allocated\".</p>",
        "id": 128615031,
        "sender_full_name": "davidtwco",
        "timestamp": 1529951122
    },
    {
        "content": "<p>Since I expected the allocations to be small, like the responses.</p>",
        "id": 128615048,
        "sender_full_name": "davidtwco",
        "timestamp": 1529951154
    },
    {
        "content": "<p>Though, it occurs to me it could just be a few large allocations alongside a lot of small ones.</p>",
        "id": 128615099,
        "sender_full_name": "davidtwco",
        "timestamp": 1529951184
    },
    {
        "content": "<p>BTreeMap is inherently lots of small allocations, IIRC</p>",
        "id": 128615116,
        "sender_full_name": "simulacrum",
        "timestamp": 1529951207
    },
    {
        "content": "<p>And BTreeMap is where the majority of memory is spent according to massif</p>",
        "id": 128615138,
        "sender_full_name": "simulacrum",
        "timestamp": 1529951238
    },
    {
        "content": "<p>I see each thread allocating ~400MB at \"boot\": </p>\n<p><a href=\"/user_uploads/4715/Xy8QIN2b8uYJIkTJuPzPHFfh/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/Xy8QIN2b8uYJIkTJuPzPHFfh/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/Xy8QIN2b8uYJIkTJuPzPHFfh/pasted_image.png\"></a></div><p>It's all coming from <code>get_home</code>:</p>\n<p><a href=\"/user_uploads/4715/TaZthCy8Ld6d6H1K_h_lB74C/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/TaZthCy8Ld6d6H1K_h_lB74C/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/TaZthCy8Ld6d6H1K_h_lB74C/pasted_image.png\"></a></div><p>There are 20997 entries in the jobs vector...</p>",
        "id": 128618799,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529955676
    },
    {
        "content": "<p>Is that the same amount allocated at boot with no threads?</p>",
        "id": 128618812,
        "sender_full_name": "davidtwco",
        "timestamp": 1529955703
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/sfSpFD5sA-PSm6PYAX8Wfg5s/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/sfSpFD5sA-PSm6PYAX8Wfg5s/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/sfSpFD5sA-PSm6PYAX8Wfg5s/pasted_image.png\"></a></div><p>Looks like ~400 MB twice</p>",
        "id": 128618970,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529955914
    },
    {
        "content": "<p>Huh, I wonder why it is twice.</p>",
        "id": 128618984,
        "sender_full_name": "davidtwco",
        "timestamp": 1529955948
    },
    {
        "content": "<p>I guess that contributes to a bunch of the memory usage. Does it explain the memory increasing with threads?</p>",
        "id": 128619041,
        "sender_full_name": "davidtwco",
        "timestamp": 1529955973
    },
    {
        "content": "<p>each instance is sequential w/o threads, yeah?</p>",
        "id": 128619042,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529955974
    },
    {
        "content": "<p>Yeah.</p>",
        "id": 128619046,
        "sender_full_name": "davidtwco",
        "timestamp": 1529955981
    },
    {
        "content": "<p>I guess it would be that.</p>",
        "id": 128619047,
        "sender_full_name": "davidtwco",
        "timestamp": 1529955984
    },
    {
        "content": "<p>You could start scattering <a href=\"https://crates.io/crates/heapsize\" target=\"_blank\" title=\"https://crates.io/crates/heapsize\">https://crates.io/crates/heapsize</a> annotations everywhere...</p>",
        "id": 128619279,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529956275
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116107\">@David Wood</span> can you use nightly? I wonder if <a href=\"https://github.com/rust-lang/rust/pull/50352\" target=\"_blank\" title=\"https://github.com/rust-lang/rust/pull/50352\">https://github.com/rust-lang/rust/pull/50352</a> would help you out a lot.</p>",
        "id": 128619809,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529956939
    },
    {
        "content": "<p>If that was merged on 12th May, wouldn't it be in nightly and helping right now? Since we need to use nightly to use the system allocator in our profiles?</p>",
        "id": 128619852,
        "sender_full_name": "davidtwco",
        "timestamp": 1529957037
    },
    {
        "content": "<p>Uhhhhh<br>\n<em>shhh</em>.</p>",
        "id": 128619902,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529957073
    },
    {
        "content": "<p>This explains why <code>cargo +nightly build</code> had nothing to do...</p>",
        "id": 128619921,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529957116
    },
    {
        "content": "<p>I wonder if there's a way to improve the 56MB response from <code>get_home</code> turning into ~400MB of data structure.</p>",
        "id": 128620001,
        "sender_full_name": "davidtwco",
        "timestamp": 1529957198
    },
    {
        "content": "<p><code>serde_json</code> taking up more space than the responses certainly explains the magnitude of the memory usage, just not the increasing behaviour?</p>",
        "id": 128620038,
        "sender_full_name": "davidtwco",
        "timestamp": 1529957255
    },
    {
        "content": "<p>right, but I can't yet see the increasing behavior. It is interesting you saw it on multiple OS though. </p>\n<p>You said WSL, right? I wonder what \"system allocator\" means there</p>",
        "id": 128620126,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529957318
    },
    {
        "content": "<p>Yeah, all the data I've shown has been from WSL - that's where I primarily work. The linux machine it's ran on was the one where I discovered the issue when the program got killed.</p>",
        "id": 128620176,
        "sender_full_name": "davidtwco",
        "timestamp": 1529957383
    },
    {
        "content": "<p>Would it be non-trivial to try Windows Real?</p>",
        "id": 128620270,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529957497
    },
    {
        "content": "<p>but I suppose you did jemalloc and \"system\"...</p>",
        "id": 128620280,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529957520
    },
    {
        "content": "<p><a href=\"https://github.com/mockersf/jenkins-api.rs/issues/19\" target=\"_blank\" title=\"https://github.com/mockersf/jenkins-api.rs/issues/19\">https://github.com/mockersf/jenkins-api.rs/issues/19</a> could certainly help reduce the size of the requests - but as <code>?depth=1</code> is required to get the data I need, and that's incompatible with <code>tree</code> (oh, Jenkins...). Though, we don't need <code>?depth=1</code> for the <code>get_home</code> call - that makes it _way_ bigger and the library parses it as <code>ShortJob</code> anyway (throwing away a bunch of the data). So perhaps that is a improvement for reducing the usage. Again, doesn't explain the increasing behaviour.</p>",
        "id": 128620345,
        "sender_full_name": "davidtwco",
        "timestamp": 1529957576
    },
    {
        "content": "<p>I can give it a go.</p>",
        "id": 128620346,
        "sender_full_name": "davidtwco",
        "timestamp": 1529957582
    },
    {
        "content": "<blockquote>\n<p>(throwing away a bunch of the data)</p>\n</blockquote>\n<p>But it doesn't, right? </p>\n<div class=\"codehilite\"><pre><span></span><span class=\"w\">    </span><span class=\"cp\">#[serde(flatten)]</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">pub</span><span class=\"p\">(</span><span class=\"k\">crate</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">other_fields</span>: <span class=\"nb\">Option</span><span class=\"o\">&lt;</span><span class=\"n\">serde_json</span>::<span class=\"n\">Value</span><span class=\"o\">&gt;</span><span class=\"p\">,</span><span class=\"w\"></span>\n</pre></div>",
        "id": 128620878,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529958217
    },
    {
        "content": "<p>Technically no, but there's no API (unlike <code>CommonJob</code>) to convert it to a type that will take advantage of all the fields.</p>",
        "id": 128620941,
        "sender_full_name": "davidtwco",
        "timestamp": 1529958260
    },
    {
        "content": "<p>Then it seems like a terrible idea to have the <code>other_fields</code> field...</p>",
        "id": 128621159,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529958535
    },
    {
        "content": "<p>There's no reason the author couldn't add a function that does it - but it would only have sufficient information to do it if the <code>Jenkins</code> instance was made with <code>?depth=1</code>.</p>",
        "id": 128621194,
        "sender_full_name": "davidtwco",
        "timestamp": 1529958592
    },
    {
        "content": "<p>It seems like this still happens on Windows.</p>",
        "id": 128621249,
        "sender_full_name": "davidtwco",
        "timestamp": 1529958618
    },
    {
        "content": "<p>huh. I can't fathom why I wouldn't see it on macos</p>",
        "id": 128621270,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529958659
    },
    {
        "content": "<p>VMMap is actually pretty interesting - it shows each individual allocation and its size. I should be able to get traces too. But saving the data to a file makes it stop responding, so, there's that.</p>",
        "id": 128621278,
        "sender_full_name": "davidtwco",
        "timestamp": 1529958673
    },
    {
        "content": "<p>I think it does happen on MacOS based on what you sent previously - it's just that when running with two instances, the maximum that it increases to is lower?</p>",
        "id": 128621309,
        "sender_full_name": "davidtwco",
        "timestamp": 1529958717
    },
    {
        "content": "<p>I removed</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"w\">    </span><span class=\"cp\">#[serde(flatten)]</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">pub</span><span class=\"p\">(</span><span class=\"k\">crate</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">other_fields</span>: <span class=\"nb\">Option</span><span class=\"o\">&lt;</span><span class=\"n\">serde_json</span>::<span class=\"n\">Value</span><span class=\"o\">&gt;</span><span class=\"p\">,</span><span class=\"w\"></span>\n</pre></div>\n\n\n<p>And it still compiles. So that be a thing to try.</p>",
        "id": 128621356,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529958728
    },
    {
        "content": "<p>Yeah, I've got a few different ways to lower the overall memory usage now, I think.</p>",
        "id": 128621458,
        "sender_full_name": "davidtwco",
        "timestamp": 1529958854
    },
    {
        "content": "<p>I do think there's a bug in Hyper / Tokio. It pretty reliably dies with </p>\n<div class=\"codehilite\"><pre><span></span>thread &#39;ubuntu1&#39; panicked at &#39;called `Result::unwrap()` on an `Err` value: Error { kind: Json(Error(&quot;Unexpected eof during chunk size line&quot;, line: 1, column: 58261504)), url: None }&#39;, libcore/result.rs:945:5\n</pre></div>",
        "id": 128621572,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529959003
    },
    {
        "content": "<p>Isn't that only because I call <code>unwrap()</code> everywhere in the repro?</p>",
        "id": 128621598,
        "sender_full_name": "davidtwco",
        "timestamp": 1529959050
    },
    {
        "content": "<p>I mean, how is the data being corrupted?</p>",
        "id": 128621670,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529959145
    },
    {
        "content": "<p>It wouldn't surprise me if the Jenkins instance was sending messed up data when asked for jobs that have _lots_ of data rather than it being a bug in Tokio/Hyper.</p>",
        "id": 128621733,
        "sender_full_name": "davidtwco",
        "timestamp": 1529959205
    },
    {
        "content": "<p>heh, if thread2 gets the panic, thread1 can keep on going</p>",
        "id": 128621739,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529959209
    },
    {
        "content": "<p>I spose I'd have to figure out how to use wireshark with HTTPS to find out for sure</p>",
        "id": 128621748,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529959236
    },
    {
        "content": "<p>We're still not sure why it increases gradually though. There's an expected increase as all the <code>get_home</code> calls return, then it should be a straight line with spikes for the <code>CommonJob</code> allocations and frees - but there isn't, it just keeps going. I don't know how much memory gets allocated for the <code>get_home</code> call on my Jenkins instances, but over time it increases up to 7GB of memory, so it can't be from <code>get_home</code>, you can observe all the threads having passed that point.</p>",
        "id": 128621869,
        "sender_full_name": "davidtwco",
        "timestamp": 1529959426
    },
    {
        "content": "<p>I'm trying with 2x threads. One thread died right away, but the other kept going. Over 50 jobs (~5 min), it allocated a net of +17.42 MB. Is that the rate you were seeing?</p>",
        "id": 128622055,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529959631
    },
    {
        "content": "<p>I believe heaptrack said 20.4MB/s - but yeah, thereabouts.</p>",
        "id": 128622075,
        "sender_full_name": "davidtwco",
        "timestamp": 1529959659
    },
    {
        "content": "<p>Unless you meant it allocated a total of 17.42MB. That's about what I'd see each second.</p>",
        "id": 128622184,
        "sender_full_name": "davidtwco",
        "timestamp": 1529959780
    },
    {
        "content": "<p>a total, yes</p>",
        "id": 128622323,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529959941
    },
    {
        "content": "<p>Which I'd actually expect that to be because my range doesn't include the next deallocation</p>",
        "id": 128622364,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529959988
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/coz4GyybN91eon5522kRpEgY/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/coz4GyybN91eon5522kRpEgY/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/coz4GyybN91eon5522kRpEgY/pasted_image.png\"></a></div><p>generations are roughly each iteration of the jobs <code>for</code> loop</p>",
        "id": 128622610,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529960283
    },
    {
        "content": "<p>You're not seeing any allocations from the later loop iterations?</p>",
        "id": 128622659,
        "sender_full_name": "davidtwco",
        "timestamp": 1529960352
    },
    {
        "content": "<p>Nope<br>\n<a href=\"/user_uploads/4715/6hzH4wKl26HNaGo5B6202ahZ/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/6hzH4wKl26HNaGo5B6202ahZ/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/6hzH4wKl26HNaGo5B6202ahZ/pasted_image.png\"></a></div>",
        "id": 128622892,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529960610
    },
    {
        "content": "<p>Maybe I do need to switch to macOS.</p>",
        "id": 128622905,
        "sender_full_name": "davidtwco",
        "timestamp": 1529960630
    },
    {
        "content": "<p>well, I can't get both threads to run at the same time, so maybe it's not all milk and honey</p>",
        "id": 128622989,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529960740
    },
    {
        "content": "<p>Now I have a <code>Mutex</code> <em>and</em> a <code>Barrier</code>. Exciting</p>",
        "id": 128623449,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529961336
    },
    {
        "content": "<p>haha. you'll love this â€” the memory usage is now going down over the iterations</p>",
        "id": 128623590,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529961521
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/9ppZg4nhBy6zlR8-k6lzB4iA/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/9ppZg4nhBy6zlR8-k6lzB4iA/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/9ppZg4nhBy6zlR8-k6lzB4iA/pasted_image.png\"></a></div><p>2 threads, 44 iterations. no memory increase</p>",
        "id": 128623885,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529961858
    },
    {
        "content": "<p>It seems a lot like my local Jenkins data is causing a quite different effect.</p>",
        "id": 128623922,
        "sender_full_name": "davidtwco",
        "timestamp": 1529961917
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/4hg4ryvHojGtCpd2oVGk3cD-/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a>  This is my most recent profiling run with the ubuntu Jenkins.</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/4715/4hg4ryvHojGtCpd2oVGk3cD-/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/4715/4hg4ryvHojGtCpd2oVGk3cD-/pasted_image.png\"></a></div>",
        "id": 128623986,
        "sender_full_name": "davidtwco",
        "timestamp": 1529961971
    },
    {
        "content": "<p>The first flag is 861MB , the second is 856MB</p>",
        "id": 128623992,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529961983
    },
    {
        "content": "<p>That's with five copies of the instance.</p>",
        "id": 128624015,
        "sender_full_name": "davidtwco",
        "timestamp": 1529962009
    },
    {
        "content": "<p>It definitely terminates quicker and doesn't go up as much as with my local Jenkins.</p>",
        "id": 128624032,
        "sender_full_name": "davidtwco",
        "timestamp": 1529962036
    },
    {
        "content": "<p>In particular, the profiling data said <code>get_home</code> for that run.</p>",
        "id": 128624038,
        "sender_full_name": "davidtwco",
        "timestamp": 1529962060
    },
    {
        "content": "<p>But my local Jenkins massif data is all <code>get_full_job</code>.</p>",
        "id": 128624046,
        "sender_full_name": "davidtwco",
        "timestamp": 1529962071
    },
    {
        "content": "<p>huh. 5threads ~ 800 MB. Mine is 2threads ~800 MB.</p>",
        "id": 128624099,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1529962099
    },
    {
        "content": "<p>Well, I have no clue what's going on. Pretty sure there's a bug here, but I could tell you nothing about it.</p>",
        "id": 128624152,
        "sender_full_name": "davidtwco",
        "timestamp": 1529962178
    },
    {
        "content": "<p>Currently running memory profile for my Jenkins instances in single-threaded and multi-threaded. For multi-threaded, I see the memory usage creep up over time, currently at 14.6% of my total memory. For the single-threaded, it doesn't grow, it got to a constant 0.5-0.7% and it stays around there. Just in writing this, multi-threaded has increased to 15.7% now. I'll post the massif data when it finishes. But this is enough to convince me that there is a problem and I'm not imagining things.</p>",
        "id": 128626779,
        "sender_full_name": "davidtwco",
        "timestamp": 1529965709
    },
    {
        "content": "<p><a href=\"/user_uploads/4715/NmsD6T9t0anKfTc23WFMoLgG/massif-singlethreaded.out.18768\" target=\"_blank\" title=\"massif-singlethreaded.out.18768\">massif-singlethreaded.out.18768</a> <a href=\"/user_uploads/4715/UNdpfWSxx4MdI_yQkvxLy6Kv/massif-multithreaded.out.18042\" target=\"_blank\" title=\"massif-multithreaded.out.18042\">massif-multithreaded.out.18042</a></p>",
        "id": 128627723,
        "sender_full_name": "davidtwco",
        "timestamp": 1529967394
    },
    {
        "content": "<p>So, I just got a chance today to look at the data from those profiles yesterday. Sometime after my comment here, the single-threaded memory usage spiked.</p>",
        "id": 128658225,
        "sender_full_name": "davidtwco",
        "timestamp": 1530022184
    },
    {
        "content": "<p>I'm thinking that this isn't a threading issue at all. It's just that the threads highlight the problem quicker/more obviously.</p>",
        "id": 128658258,
        "sender_full_name": "davidtwco",
        "timestamp": 1530022202
    },
    {
        "content": "<p>Or there might not be a problem at all, and this application just uses a bunch of memory in a strange-looking way. In which case, I'll have wasted your time, if that's the case - sorry about that.</p>",
        "id": 128658334,
        "sender_full_name": "davidtwco",
        "timestamp": 1530022289
    },
    {
        "content": "<p>I'm now pretty confident having done a bunch more profiling that threads aren't an issue at all - I've got a Jenkins response that is giant (hundreds of megabytes) and that makes the memory usage spike something awful - I've seen it range from 1.9GB to 7GB total, and I'm not sure why there's a range, but I've confirmed that everything looks about normal without that Jenkins instance.</p>",
        "id": 128664745,
        "sender_full_name": "davidtwco",
        "timestamp": 1530029623
    },
    {
        "content": "<p>FWIW, I did the digging I did because I wanted to learn a bit more about using Instruments for memory-related stuff, so no harm no foul</p>",
        "id": 128664807,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1530029673
    },
    {
        "content": "<p>(just as an FYI, it occurs to me the 7GB usage is explained by that job being huge too - in the actual application it goes further and uses fetches the builds of that job, they're also going to be giant, so the usage increases then too - to 7GB. In the smaller repro, it just uses 1.9GB because it is only the job.)</p>",
        "id": 128665708,
        "sender_full_name": "davidtwco",
        "timestamp": 1530030714
    },
    {
        "content": "<p>I don't think any API should be sending back 10MB of JSON, let alone 100 or 1000MB</p>",
        "id": 128666268,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1530031370
    },
    {
        "content": "<p>I think it should be solvable with some PRs (though the author has started work on this functionality) to the <code>jenkins-api</code> crate so that it can use the <code>tree</code> query parameter instead of only <code>depth</code>.</p>",
        "id": 128666467,
        "sender_full_name": "davidtwco",
        "timestamp": 1530031583
    },
    {
        "content": "<p>it's also possible that this could be a case where you may want to scatter some <code>drop</code> calls about, if you have \"longer\" methods</p>",
        "id": 128666579,
        "sender_full_name": "Jake Goulding",
        "timestamp": 1530031741
    },
    {
        "content": "<p>Perhaps, yeah. Regardless, thanks to both you (<span class=\"user-mention\" data-user-id=\"116155\">@Jake Goulding</span> ) and <span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span>  for spending some time looking into this.</p>",
        "id": 128666684,
        "sender_full_name": "davidtwco",
        "timestamp": 1530031845
    }
]