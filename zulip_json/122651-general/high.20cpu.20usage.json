[
    {
        "content": "<p>I have a multithreaded tcp server than receives binary data, parse it and creates some prometheus stats. One of the threads is clogging the cpu (stuck at 100% CPU usage). I did a perf recording and it looks like most of the CPU is consumed by prometheus lib (see bellow). The strange part is that there are no segments sent by the clients but the CPU usage is still 100%. Also I don't get anything if I strace the thread. So it looks stuck.</p>\n<p>Any clue what could be happening? </p>\n<div class=\"codehilite\"><pre><span></span><code>    41.55%    41.10%           783  ebmd     ebmd               [.] prometheus::vec::MetricVec&lt;T&gt;::with_label_values\n            |\n            |--19.16%--0x1\n            |          prometheus::vec::MetricVec&lt;T&gt;::with_label_values\n            |\n            |--16.93%--0\n            |          |\n            |          |--15.83%--prometheus::vec::MetricVec&lt;T&gt;::with_label_values\n            |          |\n            |           --0.70%--0\n            |                     prometheus::vec::MetricVec&lt;T&gt;::with_label_values\n            |\n            |--4.71%--prometheus::vec::MetricVec&lt;T&gt;::with_label_values\n            |\n             --0.74%--ebmd::server::handle_ebm_stream_connection\n\n    35.46%     0.00%             0  ebmd     [unknown]          [.] 0000000000000000\n            |\n            ---0\n               |\n               |--16.28%--prometheus::vec::MetricVec&lt;T&gt;::with_label_values\n               |\n               |--12.35%--hashbrown::map::make_hash\n               |\n               |--6.13%--&lt;std::collections::hash::map::DefaultHasher as core::hash::Hasher&gt;::write\n               |\n                --0.70%--0\n                          prometheus::vec::MetricVec&lt;T&gt;::with_label_values\n\n    20.24%    19.20%           344  ebmd     ebmd               [.] ebmd::server::handle_ebm_stream_connection\n            |\n             --19.80%--ebmd::server::handle_ebm_stream_connection\n\n    19.16%     0.00%             0  ebmd     [unknown]          [.] 0x0000000000000001\n            |\n            ---0x1\n               prometheus::vec::MetricVec&lt;T&gt;::with_label_values\n\n    13.54%    12.75%           233  ebmd     ebmd               [.] hashbrown::map::make_hash\n            |\n            |--12.75%--0\n            |          |\n            |          |--11.56%--hashbrown::map::make_hash\n            |          |\n            |           --0.79%--&lt;std::collections::hash::map::DefaultHasher as core::hash::Hasher&gt;::write\n            |\n             --0.79%--hashbrown::map::make_hash\n\n    10.79%    10.35%           194  ebmd     ebmd               [.] prometheus::atomic64::AtomicU64::inc_by_with_ordering\n            |\n             --10.64%--prometheus::atomic64::AtomicU64::inc_by_with_ordering\n\n    10.67%    10.67%           216  ebmd     ebmd               [.] ebmd::parser::Parser::parse\n            |\n            ---ebmd::parser::Parser::parse\n\n     6.47%     5.68%           115  ebmd     ebmd               [.] &lt;std::collections::hash::map::DefaultHasher as core::hash::Hasher&gt;::write\n            |\n            |--5.68%--0\n            |          |\n            |           --5.33%--&lt;std::collections::hash::map::DefaultHasher as core::hash::Hasher&gt;::write\n            |\n             --0.79%--&lt;std::collections::hash::map::DefaultHasher as core::hash::Hasher&gt;::write\n</code></pre></div>",
        "id": 225879472,
        "sender_full_name": "Maximilian Hristache",
        "timestamp": 1612979520
    },
    {
        "content": "<p>maybe your metric cardinality is huge and you're filling a MetricVec with new metrics as your code otherwise executes?</p>",
        "id": 225880403,
        "sender_full_name": "nagisa",
        "timestamp": 1612979851
    },
    {
        "content": "<p>Sounds like a question for <a href=\"https://github.com/tikv/rust-prometheus/issues\">https://github.com/tikv/rust-prometheus/issues</a>. Looking at <a href=\"https://docs.rs/prometheus/0.11.0/src/prometheus/vec.rs.html#257-259\">https://docs.rs/prometheus/0.11.0/src/prometheus/vec.rs.html#257-259</a>, I don't see anything obvious -- it might use CPU if <code>desc</code> is very large, but it should still finish.</p>",
        "id": 225880635,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612979925
    },
    {
        "content": "<p>it is in the order of 20k. is that too much?</p>",
        "id": 225880684,
        "sender_full_name": "Maximilian Hristache",
        "timestamp": 1612979942
    },
    {
        "content": "<p>I'd try to make a test case as a separate app</p>",
        "id": 225880772,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612979989
    },
    {
        "content": "<p>20k doesn't sound much to me, but I'm not really familiar with metrics APIs</p>",
        "id": 225880900,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612980027
    },
    {
        "content": "<p>It appears to be O(n) in the number of descriptors</p>",
        "id": 225881017,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612980076
    },
    {
        "content": "<p>Are you building it in release mode?</p>",
        "id": 225881087,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612980111
    },
    {
        "content": "<p>I think so. Was actually thinking about that as it's build via Jenkins using docker. <br>\nIt looks like it's built in release mode by checking docker files. <br>\nIs there a way to check the binary that it's built in release mode, to be sure?</p>",
        "id": 225881387,
        "sender_full_name": "Maximilian Hristache",
        "timestamp": 1612980225
    },
    {
        "content": "<p>I don't know an easy way. You seem to have good symbol names, I'm guessing you also set <code>[profile.release] debug = true</code>?</p>",
        "id": 225881573,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612980298
    },
    {
        "content": "<p>(That's not a bad thing to do, just checking.)</p>",
        "id": 225881635,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612980329
    },
    {
        "content": "<p>I actually don't set that</p>",
        "id": 225881782,
        "sender_full_name": "Maximilian Hristache",
        "timestamp": 1612980384
    },
    {
        "content": "<p>Hmm</p>",
        "id": 225881828,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612980404
    },
    {
        "content": "<p>when I build it locally on my PC the size in debug mode is much bigger than what I have in the docker container so it looks like it's using release mode</p>",
        "id": 225881929,
        "sender_full_name": "Maximilian Hristache",
        "timestamp": 1612980456
    },
    {
        "content": "<p>That also depends on whether you're stripping the binary or not, and on the platform</p>",
        "id": 225881965,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612980479
    },
    {
        "content": "<p>E.g. <code>rust-analyzer</code> Windows binaries are 50% larger than Linux ones, all other things being equal</p>",
        "id": 225882078,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612980519
    },
    {
        "content": "<p>You could enable <code>debug</code>, that way <code>perf</code> might give you more info if the issue happens again</p>",
        "id": 225882126,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612980544
    },
    {
        "content": "<p>But I'd still try to reproduce it in a test program</p>",
        "id": 225882170,
        "sender_full_name": "Laurențiu",
        "timestamp": 1612980568
    },
    {
        "content": "<p>thanks for the suggestions</p>",
        "id": 225884826,
        "sender_full_name": "Maximilian Hristache",
        "timestamp": 1612981746
    },
    {
        "content": "<p>checking the instructions, the CPU is used in instructions prefixed with LOCK</p>\n<div class=\"codehilite\"><pre><span></span><code>Percent│125:   mov       %rdi,0x10(%rsp)\n       │       mov       (%r12),%rax\n       │       test      $0x8,%al\n       │     ↓ jne       258\n  1.34 │       mov       %rax,%rcx\n       │       add       $0x10,%rcx\n       │     ↓ jb        258\n 22.85 │       lock      cmpxchg   %rcx,(%r12)\n       │     ↓ jne       258\n  2.16 │14f:   mov       %r12,(%rsp)\n       │       lea       0x18(%r13),%rdi\n       │       lea       0x10(%rsp),%rsi\n       │       mov       %rdi,0x8(%rsp)\n  1.34 │     → callq     hashbrown::map::make_hash\n  0.78 │       mov       0x28(%r13),%rdx\n  0.22 │       mov       0x30(%r13),%r11\n  0.11 │       mov       %rdx,%rsi\n  0.45 │       and       %rax,%rsi\n  0.45 │       shr       $0x39,%rax\n       │       lea       0x10(%rsi),%rdi\n  0.34 │       and       %rdx,%rdi\n  0.78 │       movdqu    (%r11,%rsi,1),%xmm1\n  1.01 │       movd      %eax,%xmm0\n       │       punpcklbw %xmm0,%xmm0\n  0.11 │       pshuflw   $0xe0,%xmm0,%xmm0\n  0.89 │       pshufd    $0x0,%xmm0,%xmm0\n  0.56 │       movdqa    %xmm0,%xmm2\n  0.68 │       pcmpeqb   %xmm1,%xmm2\n       │       pmovmskb  %xmm2,%eax\n  0.11 │       mov       $0x10,%r9d\n  0.34 │       test      %ax,%ax\n       │     ↓ je        216\n  0.57 │1ae:   bsf       %ax,%bp\n  0.11 │       movzwl    %bp,%ebx\n  0.56 │       add       %rsi,%rbx\n  0.22 │       mov       0x10(%rsp),%r10\n  0.45 │       and       %rdx,%rbx\n       │       mov       %rbx,%rbp\n  0.45 │       shl       $0x4,%rbp\n       │       mov       %r11,%rcx\n  0.38 │       sub       %rbp,%rcx\n  2.04 │       cmp       -0x10(%rcx),%r10\n       │     ↓ jne       281\n       │       neg       %rbx\n       │1da:   shl       $0x4,%rbx\n       │       mov       -0x8(%r11,%rbx,1),%rbp\n 17.91 │       lock      addq      $0x1,0x0(%rbp)\n       │     ↓ jle       5ee\n       │       test      %rbp,%rbp\n       │     ↓ je        2f9\n  0.78 │       mov       $0xfffffffffffffff0,%rax\n 23.41 │       lock      xadd      %rax,(%r12)\n  0.11 │       and       $0xfffffffffffffff2,%rax\n</code></pre></div>",
        "id": 225889969,
        "sender_full_name": "Maximilian Hristache",
        "timestamp": 1612983658
    },
    {
        "content": "<p>That might be the <code>prometheus::atomic64::AtomicU64::inc_by_with_ordering</code> in the tree you posted earlier.</p>",
        "id": 225893518,
        "sender_full_name": "The 8472",
        "timestamp": 1612985077
    },
    {
        "content": "<p>You can get <code>perf</code> to show the corresponding source code too if you compile locally with debug info.</p>",
        "id": 225893946,
        "sender_full_name": "The 8472",
        "timestamp": 1612985258
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"363601\">Maximilian Hristache</span> <a href=\"#narrow/stream/122651-general/topic/high.20cpu.20usage/near/225879472\">said</a>:</p>\n<blockquote>\n<p>Also I don't get anything if I strace the thread. So it looks stuck.</p>\n</blockquote>\n<p>It could be a logic bug where it loops forever. The innermost loop can be a red herring if it's an outer one that doesn't make progress.</p>",
        "id": 225894417,
        "sender_full_name": "The 8472",
        "timestamp": 1612985470
    }
]