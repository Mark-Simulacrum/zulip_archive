[
    {
        "content": "<p>I'm in an async context using smol, and I'm trying to figure out the best way to run a bunch of things in parallel for performance. I have an <code>Mmap</code> of a file, and I'd like to process sections of it in parallel, but I also need to collect the output of that parallel processing in order. Also, each of the individual steps produces a <code>Result</code> that might be an <code>Err</code>.</p>",
        "id": 211397510,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601180179
    },
    {
        "content": "<p>Any suggestions for a good starting point?</p>",
        "id": 211397515,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601180196
    },
    {
        "content": "<p>cc <span class=\"user-mention\" data-user-id=\"128448\">@stjepang</span></p>",
        "id": 211397516,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601180200
    },
    {
        "content": "<p>I feel like there's something I'm completely missing. I've used rayon before, which was extremely straightforward.</p>",
        "id": 211398427,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601181987
    },
    {
        "content": "<p>I think the main question is weather or not the parallel processing is blocking or non-blocking and in turn weather or not you want to<br>\n\"fork out\" the work into a different thread pool then the one done for the async I/O.</p>\n<p>This might also change weather or not you have need to poll the futures for the resulting chunks in parallel or not.</p>\n<p><code>join_all</code> lets you poll futures in parallel and wait for all to be completed.</p>\n<p><code>FuturesOrdered</code>/<code>StreamExt::buffered(number_of_chunks)</code> gives you a stream which polls the futures in parallel and returns the resulting items in order (in your case this would be a stream of <code>Results</code>.</p>\n<p>I don't know about Mmap and if liftimes get in your way. </p>\n<p>Generally async if focused on non-blocking I/O and rayon on parallel computation, I don't know if there is a \"rayon for async\" (<strong>anyone knows of such a crate?</strong>).</p>",
        "id": 211420616,
        "sender_full_name": "Philipp Korber",
        "timestamp": 1601219644
    },
    {
        "content": "<p>Also note that while smol by default used <code>futures_lite</code> all the <code>std::future</code> based futures libraries can be combined freely without problems. The buffered method comes from the <code>futures</code> crate.</p>",
        "id": 211420651,
        "sender_full_name": "Philipp Korber",
        "timestamp": 1601219724
    },
    {
        "content": "<p>Parallel or concurrent? The latter can happen with <code>FuturesOrdered</code> as mentioned above. For the former you need to either tie your code to the runtime and spawn tasks (and then later gather them, in order, which you can do with <code>FuturesOrdered</code> as well) or use an external thread pool (to much the same effect).</p>",
        "id": 211421100,
        "sender_full_name": "nagisa",
        "timestamp": 1601220401
    },
    {
        "content": "<p>the caveat with using tasks for parallelisation of blocking tasks is that you can accidentally exhaust the executor's thread pool and end up blocking the non-blocking code from progressing any further until the blocking code is done.</p>",
        "id": 211421218,
        "sender_full_name": "nagisa",
        "timestamp": 1601220598
    },
    {
        "content": "<p>I still want to get some kind of <code>spawn_future</code> into rayon:<br>\n<a href=\"https://github.com/rayon-rs/rayon/pull/679\">https://github.com/rayon-rs/rayon/pull/679</a></p>",
        "id": 211422490,
        "sender_full_name": "cuviper",
        "timestamp": 1601222467
    },
    {
        "content": "<p>The debate was whether my simple map from <code>FnOnce</code> to <code>Future</code> is enough, or if we should really map <code>Future</code> to <code>Future</code></p>",
        "id": 211422565,
        "sender_full_name": "cuviper",
        "timestamp": 1601222545
    },
    {
        "content": "<p>I don't really want to teach rayon to be an executor though, and I think users can just as well <code>.await</code> the input in their own context before calling rayon</p>",
        "id": 211422649,
        "sender_full_name": "cuviper",
        "timestamp": 1601222648
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"239881\">@Josh Triplett</span> maybe you can just use a one-shot futures channel? Spawn a rayon job with the sender, then <code>.await</code> the receiver in your async context</p>",
        "id": 211422975,
        "sender_full_name": "cuviper",
        "timestamp": 1601223176
    },
    {
        "content": "<p>The problem is that I need to do an <code>.await</code> <em>inside</em> each parallel job.</p>",
        "id": 211423230,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601223554
    },
    {
        "content": "<p>(Specifically, a rusoto call.)</p>",
        "id": 211423240,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601223581
    },
    {
        "content": "<p>Ah, I have no rayon solution for that</p>",
        "id": 211423278,
        "sender_full_name": "cuviper",
        "timestamp": 1601223605
    },
    {
        "content": "<p>(unless we go full executor)</p>",
        "id": 211423299,
        "sender_full_name": "cuviper",
        "timestamp": 1601223621
    },
    {
        "content": "<p>To answer an earlier question, I would be happy to share the executor's normal thread pool; that's what I would hope for.</p>",
        "id": 211423382,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601223786
    },
    {
        "content": "<p>Do you need non 'static lifetimes/borrowed from the chunk processing to the outside?</p>",
        "id": 211423604,
        "sender_full_name": "Philipp Korber",
        "timestamp": 1601224107
    },
    {
        "content": "<p>(if so this would be a bit tricky)</p>",
        "id": 211423690,
        "sender_full_name": "Philipp Korber",
        "timestamp": 1601224298
    },
    {
        "content": "<p>Not this time, no. But I do ideally need non-<code>'static</code> lifetimes borrowed from the code just above the parallel processing <em>into</em> the chunks.</p>",
        "id": 211423911,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601224663
    },
    {
        "content": "<p>I need to do parallel processing on chunks of a buffer.</p>",
        "id": 211424022,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601224806
    },
    {
        "content": "<p>Or more generally, chunks of a file.</p>",
        "id": 211424031,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601224824
    },
    {
        "content": "<p>For each parallel chunk, I need to do a small amount of CPU-based computation (a hash), then make a request that will require a <code>.await</code>, then yield a value. And I need to process the yielded values in order, either concurrently or afterwards.</p>",
        "id": 211424109,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601224960
    },
    {
        "content": "<p>(technically, the yielded value does not depend on the request; it's just the value of that hash)</p>",
        "id": 211424126,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601225002
    },
    {
        "content": "<p>You should be able to do one of the two  approached shown in this example:</p>\n<p><a href=\"https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=915937a1c50d6b2e8eed428aa51e8262\">https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=915937a1c50d6b2e8eed428aa51e8262</a></p>\n<p>WARNING: This won't run on <a href=\"http://play.rust-lang.org\">play.rust-lang.org</a> as you can't use smol there! (Also that's just fastely written down, so style isn't perfect and I forgot to run <code>cargo fmt</code> <span aria-label=\"stuck out tongue wink\" class=\"emoji emoji-1f61c\" role=\"img\" title=\"stuck out tongue wink\">:stuck_out_tongue_wink:</span> )</p>",
        "id": 211438989,
        "sender_full_name": "Philipp Korber",
        "timestamp": 1601247931
    },
    {
        "content": "<p>That looks really promising! Will investigate tomorrow.</p>",
        "id": 211457645,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601276432
    },
    {
        "content": "<p>Instead of <code>FuturesOrdered</code> you can also use <code>smol::LocalExecutor</code> but it's a bit more complex/roundabout.</p>",
        "id": 211472343,
        "sender_full_name": "Philipp Korber",
        "timestamp": 1601287132
    },
    {
        "content": "<p>(small nit: you can push directly into <code>FuturesOrdered</code> without the intermediate vector)</p>",
        "id": 211501012,
        "sender_full_name": "nagisa",
        "timestamp": 1601303949
    }
]