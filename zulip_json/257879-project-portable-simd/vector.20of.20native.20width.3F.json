[
    {
        "content": "<p>Bad question: would it be beneficial to have some compile-time access to the native vector register width somehow? People writing some AoSoA data structure may benefit from that sort of stuff, but everyone writing their own bunch of <code>#[cfg]</code> to guess the size is probably not a good idea.</p>\n<p>(And I assume that GPUs really need a <code>clinfo</code> call instead for that?)</p>",
        "id": 239548009,
        "sender_full_name": "Mingye Wang",
        "timestamp": 1621496410
    },
    {
        "content": "<p>it may vary at runtime depending on the cpu and os, or even on how many registers the compiler decides to use (like in RISC-V V and SimpleV), so there may not be a good answer available at compile-time</p>",
        "id": 239549157,
        "sender_full_name": "Jacob Lifshay",
        "timestamp": 1621496955
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"413916\">Mingye Wang</span> <a href=\"#narrow/stream/257879-project-portable-simd/topic/vector.20of.20native.20width.3F/near/239548009\">said</a>:</p>\n<blockquote>\n<p>Bad question: would it be beneficial to have some compile-time access to the native vector register width somehow? </p>\n</blockquote>\n<p>As Jacob mentioned: There is not necessarily a coherent or good answer here.</p>",
        "id": 239550703,
        "sender_full_name": "Jubilee",
        "timestamp": 1621497766
    },
    {
        "content": "<p>Trying to optimize around the \"native register size\" just on the assumption such will be more performant can wind up pessimizing performance. This applies for aiming for a 32-bit or 64-bit GPR, and it also applies for trying to mix things up based on 128-bit or 256-bit SIMD registers.</p>\n<p>It is totally valid for an x86 microarchitecture to implement 64 physical 128-bit XMM registers and use those to implement the 16 xmm, ymm, and zmm architectural registers, using register renaming to deconflict, making it enormously faster at doing operations on individual xmm-wide registers since it doesn't have to use 4 at a time, even though it has access to what appears to be 512-bit registers.</p>\n<p>This is simplified but not really hypothetical: Intel and AMD have both done something similar to this, repeatedly, over the years. And because using the full AVX512 width is more situational, with its license-based downclocking, LLVM likes to, even when AVX512F is enabled, use the narrower ymm and xmm registers, unless it knows it is targeting Ice Lake or KNL.</p>",
        "id": 239551315,
        "sender_full_name": "Jubilee",
        "timestamp": 1621498112
    },
    {
        "content": "<p>I think the way to do this (and the way I have done this in the past) is to make the vector width generic in your algorithm, and dispatch at runtime based on the instruction set, and change the generic parameter based on the instruction set.</p>",
        "id": 239567056,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1621506486
    },
    {
        "content": "<p>I would hope that the compiler just handles this at compile time, tbh, after you use whatever size of SimdArray you actually <strong>need</strong>.</p>",
        "id": 239636288,
        "sender_full_name": "Jubilee",
        "timestamp": 1621534518
    },
    {
        "content": "<p>I think that's optimistic because auto vectorization doesn't really work, but I mostly agree. Depends on algorithm</p>",
        "id": 239646333,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1621538968
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"312331\">@Caleb Zulawski</span> I mean with our API that actually uses LLVM SIMD instructions.</p>",
        "id": 239656834,
        "sender_full_name": "Jubilee",
        "timestamp": 1621543619
    },
    {
        "content": "<p>I guess I meant that I think it's unavoidable to care about vector width to some extent, if the compiler is bad at auto vectorization I'm not sure it'll be smart enough to do something clever with your non-native vector sizes in all situations</p>",
        "id": 239657854,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1621544148
    },
    {
        "content": "<p>Though I don't think that means we should provide a native width or anything (since there are so many factors that go into it), I just think developers will need to be aware of the width in some situations</p>",
        "id": 239657970,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1621544205
    },
    {
        "content": "<p>That's fair I guess. I think that with LLVM SIMD instructions we will be probably providing a strong enough \"hint\" that it <em>should</em> get it right, but I could still see problems emerging.</p>",
        "id": 239663367,
        "sender_full_name": "Jubilee",
        "timestamp": 1621546682
    },
    {
        "content": "<p>There are many cases where LLVM can get an optimization correct if it just does another additional pass over the code, but it doesn't, because finite time, so it misses the optimization. Here, we're skipping past all the optimization passes just to recognize a valid vectorization site.</p>",
        "id": 239663812,
        "sender_full_name": "Jubilee",
        "timestamp": 1621546895
    },
    {
        "content": "<p>I think that Arm's SVE should have a set of intrinsics (that end up just reading a register) for accessing VL (the vector length).<br>\nIIRC, these should come in several flavours</p>\n<ul>\n<li>Return VL/n: the number of elements of width &lt;n&gt; that fit in a vector (for n in 8, 16, 32, 64)</li>\n<li>Return log2(VL/n): log of the number of elements of width &lt;n&gt; that fit in a vector. (I remember these being useful in DSP algorithms like FFT)</li>\n<li>Return a mask which sets the first VL/n n-bit elements to active. (May be SVE specific since it would probably just be 'set all bits' in fixed-length architectures.)</li>\n</ul>\n<p>These sorts of intrinsics (or just constants on fixed-width architectures) are pretty useful for writing code that adapts to different vector lengths.<br>\nThough, for this to work, you obviously don't want to use typenames that hardcode the vector length. So 'u8x16' is an inflexible name because it obviously only works for 128-bit vectors but vec_u8 (vector of u8) is meaningful across a range of vector widths.</p>",
        "id": 239764930,
        "sender_full_name": "Alastair Reid",
        "timestamp": 1621609357
    }
]