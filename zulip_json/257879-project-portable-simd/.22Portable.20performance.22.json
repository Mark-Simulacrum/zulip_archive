[
    {
        "content": "<p>Popped into my head, just lobbing it over the wall:<br>\n\"The goal of <code>std::simd</code> is obviously 'performance optimization' to at least some level, but more specifically it is to be the opposite of being penny-wise and pound-foolish in code to performance: the library is instead penny<em>-foolish</em> and pound<em>-wise</em> in code to performance, while leaving enough of a trail in the ledger to allow miserly scraping back the pennies here and there. There is no particular goal to emit every architecture's assembly instructions exactly, though allowing that to happen where it is feasible is desired. Instead, we want to offer the chance to optimize the vast majority of programs that are currently either written in scalar form yet don't autovectorize even though they 'obviously should', or the handful of programs that are currently optimized for one architecture but, insofar as they do not rely on any architecture-specific concepts, could be ported, without much effort, to <code>std::simd</code>'s more general form. No one micro-benchmark is our specific target: it's making the entire program overall faster on all targets, and making it possible to still read it without having to be intimately aware of a given architecture's assembly instruction format.</p>\n<p>Obviously, this is done by making it use SIMD types that embed the concept of data parallelism directly into the program when they are used.  Because this data parallel design yields a generally-closer parallel to 'what the hardware does', it is easier to optimize from the compiler's view. But this <strong>also</strong> means it is easier for the programmer to restructure it and optimize it the rest of the way using e.g. the intrinsics or handrolled assembly, because you have <strong>already</strong> made it follow almost the format it would need to be thus optimized, while having minimal overhead.\"</p>",
        "id": 270444373,
        "sender_full_name": "Jubilee",
        "timestamp": 1643830426
    }
]