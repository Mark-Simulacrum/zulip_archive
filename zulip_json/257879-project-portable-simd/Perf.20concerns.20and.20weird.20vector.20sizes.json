[
    {
        "content": "<p>A common issue that people seem to express is \"won't code for Vec3 be slow?\", which relies on assuming various things about how the vectors are packed and such, and since our API is const generic, we're going to have this discussion a lot if we loosen our lane limits. Something to keep paying attention to, I suppose.</p>",
        "id": 237223001,
        "sender_full_name": "Jubilee",
        "timestamp": 1620073449
    },
    {
        "content": "<p>On SimpleV, vec3 is likely faster than vec4 and uses less space.</p>",
        "id": 237225593,
        "sender_full_name": "Jacob Lifshay",
        "timestamp": 1620074500
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"229517\">@Jacob Lifshay</span> are SimpleV vectors O(n)?</p>",
        "id": 237230936,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1620076276
    },
    {
        "content": "<p>yes...they are processed by sending individual vector element operations to the cpu backend, which then packs them up in groups and executes them. so you can still get the same performance as traditional simd, but with waay more flexibility. if you have a bunch of vec3 the backend can repack those operations into groups of 4, 8, 16, etc. -- whatever width the simd alu(s) have.</p>",
        "id": 237234421,
        "sender_full_name": "Jacob Lifshay",
        "timestamp": 1620077192
    },
    {
        "content": "<p>Cool, sounds like a good design</p>",
        "id": 237234710,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1620077283
    },
    {
        "content": "<p>the vector instructions are unpacked into the individual element operations when the instructions are decoded.</p>",
        "id": 237234747,
        "sender_full_name": "Jacob Lifshay",
        "timestamp": 1620077286
    }
]