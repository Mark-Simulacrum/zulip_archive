[
    {
        "content": "<p>So, \"Vec3\" here is a placeholder for \"vectors of an irregular lane count that is not a power of 2\".<br>\nIn <a href=\"https://github.com/rust-lang/rust/pull/80652\">https://github.com/rust-lang/rust/pull/80652</a> we introduced a monomorphization error for those to <code>repr_simd</code>.<br>\n<a href=\"https://github.com/WebAssembly/flexible-vectors/\">https://github.com/WebAssembly/flexible-vectors/</a> is probably going to be a thing someday so I would EXPECT these vectors to become a thing in Cranelift, but at the moment it isn't very far along and it might be pretty limited.</p>\n<p>Some discussion of this is here <a href=\"https://github.com/bjorn3/rustc_codegen_cranelift/issues/1136\">https://github.com/bjorn3/rustc_codegen_cranelift/issues/1136</a><br>\nWorth thinking about what our options are!</p>",
        "id": 225626407,
        "sender_full_name": "Jubilee",
        "timestamp": 1612828010
    },
    {
        "content": "<p>It seems like most of the implementations of actual vectors with non-power-of-2 sizes are a bit different than the \"next power of 2 up\" model that seems to be assumed by e.g. the vek code and that LLVM uses for these. Am I incorrect there?</p>",
        "id": 225629952,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612830611
    },
    {
        "content": "<p>How important is having <code>Vec3</code> be part of this simd-portable crate?</p>",
        "id": 225630309,
        "sender_full_name": "Edmund Cape",
        "timestamp": 1612830947
    },
    {
        "content": "<p>*crate</p>",
        "id": 225630417,
        "sender_full_name": "Jubilee",
        "timestamp": 1612831002
    },
    {
        "content": "<p>I think it would be completely appropriate to use next power of 2 up on platforms that only have powers of two, and do it \"the right way\" on platforms that support it</p>",
        "id": 225630572,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612831097
    },
    {
        "content": "<p>I personally wouldn't consider it critically important because this is \"portable SIMD\" and at the moment we can't actually target any architectures that support non-power-of-two sizes. But even then I think it's a useful abstraction</p>",
        "id": 225630673,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612831195
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"312331\">@Caleb Zulawski</span> I'm inclined to agree. It's already a major win to get SIMD processing from a single thread.  I would have to consider more, but I suspect that much of the win also depends on the ability to avoid incremental branching.  With a datatype of say 24-bit none of the operations work \"off the rack\".  Everything becomes custom. With that customization comes more complexity in the logic... more chances of branching and other sub-optimal data \"work-flows\".  All this to say, trade-offs the user of the crate will be best equipped to manage?... at least until more is learned about the performance and final design of the crate?</p>",
        "id": 225631158,
        "sender_full_name": "Edmund Cape",
        "timestamp": 1612831563
    },
    {
        "content": "<p>As far as \"how important\", it's not devastatingly important but establishing priorities is part of the game, and if we don't deal with !(2^N) vectors now, we're kind of kicking the can down the road more than anything. And this is already considered a desirable feature by SIMD authors, the classical example being a 3D dot product.</p>",
        "id": 225631407,
        "sender_full_name": "Jubilee",
        "timestamp": 1612831796
    },
    {
        "content": "<blockquote>\n<p>I think it would be completely appropriate to use next power of 2 up on platforms that only have powers of two, and do it \"the right way\" on platforms that support it</p>\n</blockquote>\n<p>This feels rather nonportable. Given the discussions around NaN architecture dependence being unacceptable, this feels like a way more significant departure between architectures that's much more liable to cause people to write nonportable code.</p>",
        "id": 225631489,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612831835
    },
    {
        "content": "<p>I'm curious, what would be non portable about it?</p>",
        "id": 225631555,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612831893
    },
    {
        "content": "<p>If the behavior is drastically different depending on architecture that feels like a very non-\"portable simd\" thing, no?</p>",
        "id": 225631642,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612831936
    },
    {
        "content": "<p>It's exactly what LLVM does, but we may need to handle it at the rust level or maybe even MIR lowering instead of in cranelift</p>",
        "id": 225631654,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612831947
    },
    {
        "content": "<p>the question is if it's different vs. observably different</p>",
        "id": 225631669,
        "sender_full_name": "Jubilee",
        "timestamp": 1612831961
    },
    {
        "content": "<p>Great question.  It would just require more vectors from one platform to the next. No?</p>",
        "id": 225631703,
        "sender_full_name": "Edmund Cape",
        "timestamp": 1612831995
    },
    {
        "content": "<p>in a strict sense, our API will be behaviorally different based on architecture already but in nonobservable ways.</p>",
        "id": 225631738,
        "sender_full_name": "Jubilee",
        "timestamp": 1612832031
    },
    {
        "content": "<p>What is considered observable? Speed?</p>",
        "id": 225631807,
        "sender_full_name": "Edmund Cape",
        "timestamp": 1612832072
    },
    {
        "content": "<p>Observable would be resulting in different values</p>",
        "id": 225631833,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612832106
    },
    {
        "content": "<p>yeah i definitely wouldn't consider speed observable here.</p>",
        "id": 225631857,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612832128
    },
    {
        "content": "<p>Cool.  Just level-setting my understanding of what is being described.</p>",
        "id": 225631936,
        "sender_full_name": "Edmund Cape",
        "timestamp": 1612832170
    },
    {
        "content": "<p>I think it behaving like <code>[T; N]</code> vs <code>[T; N.next_power_of_two()]</code> (where the T's over N are uninit) is observable. I also expect if we support this where all of these behave that way, people will definitely write code under that assumption</p>",
        "id": 225632092,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612832280
    },
    {
        "content": "<p>So the concern is that placing a 24-bit value in a 32-bit lane could generate different values compared to a system that places 24-bit value in a 24-bit lane. Yes?</p>",
        "id": 225632133,
        "sender_full_name": "Edmund Cape",
        "timestamp": 1612832284
    },
    {
        "content": "<p>we're taling about stuff like <code>Simd&lt;f32, 3&gt;</code> rather than non power of two bit widths, i think</p>",
        "id": 225632180,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612832329
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"209168\">@Thom Chiovoloni</span> what is observable about that, other than the size of the type?</p>",
        "id": 225632309,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612832412
    },
    {
        "content": "<p>Like, it's one thing if it behaves a certain way on some architectures and another on others out of the gate, but if it were to stabilize and behave consistently on all supported architectures and we later plan on changing it in major ways that will cause code to either fail to compile or fail to run...</p>\n<p>Especially because this is just not actually a supported thing on the architectures we'd be adding it for, and we know we later want to support arches that do this differently.</p>",
        "id": 225632388,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612832478
    },
    {
        "content": "<p>It would mean that you can't transmute between arrays and vectors, but that doesn't mean it's not portable</p>",
        "id": 225632393,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612832483
    },
    {
        "content": "<p>Sorry, maybe something is unclear, but if it rounds up to the next power of two, the public API would still operate entirely on the reduced lane set</p>",
        "id": 225632501,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612832543
    },
    {
        "content": "<p>Which is precisely what LLVM does to handle non-native vector sizes</p>",
        "id": 225632546,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612832584
    },
    {
        "content": "<p>I know, but the layout would be entirely different. While it's true that there are existing types with different layout guarantees on different targets, they all pretty much have to do with #[cfg(target_pointer_width)], and where possible for has made this identical, no?</p>\n<p>I think it's realistic to assume people will write code assuming the layout is \"round up to PoT\" if we stabilize these working this way on targets that don't support them, don't have any that don't work that way, and call it portable simd</p>",
        "id": 225632800,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612832735
    },
    {
        "content": "<p>It seems bizarre to stabilize something that will have such drastically different layout considerations on future targets when it doesn't exist on current targets, when we could just... not support it until it comes up on the targets that actually behave that way.</p>",
        "id": 225632900,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612832794
    },
    {
        "content": "<p>Like, we're deliberately going out of our way to introduce a thing with non-portable behaviors that doesn't otherwise exist (in this api, yet)</p>",
        "id": 225632944,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612832828
    },
    {
        "content": "<p>eh, I think that's making assumptions re: how data gets loaded into and stored from vector registers that might not hold or might just already be a concern anyways.</p>",
        "id": 225632968,
        "sender_full_name": "Jubilee",
        "timestamp": 1612832855
    },
    {
        "content": "<p>I still fail to see why that's non-portable, I think.  Vectors will already have different alignments on different architectures</p>",
        "id": 225632992,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612832869
    },
    {
        "content": "<p>That.</p>",
        "id": 225633001,
        "sender_full_name": "Jubilee",
        "timestamp": 1612832877
    },
    {
        "content": "<p>So we probably need to spend some time looking more closely at vector alignment and layout between arches and what that actually means for us.</p>",
        "id": 225633078,
        "sender_full_name": "Jubilee",
        "timestamp": 1612832922
    },
    {
        "content": "<p>Perhaps there should be functions for loading vectors from aligned and unaligned element types, which can take the layout into consideration</p>",
        "id": 225633126,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612832957
    },
    {
        "content": "<p>if the answer to the question is \"how do non-power of two vectors behave\" is \"round up to next power of two and discard the excess\" for all initially supported targets, I think that's going to be hard to change if later we want to.</p>",
        "id": 225633128,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612832959
    },
    {
        "content": "<p>I think the answer is \"use std::mem::size_of\" because it could be radically different between any given architectures</p>",
        "id": 225633208,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612833008
    },
    {
        "content": "<p>I don't even think it's a breaking change to change the layout of a vector in the future?</p>",
        "id": 225633301,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612833088
    },
    {
        "content": "<p>for FFI it is, definitely, and until safe_transmute exists if we expect these types to be transumted (I would, since we know that people will want to use them with core::arch intrinsics) that is too. In particular I see a lot of ways that people could easily end accidentally up writing nonportable code using this because they send things to the GPU and such.</p>\n<p>I know you can just say \"should have read the docs better since you're writing unsafe code\" but it just makes very little sense to be to introduce something like this when nothing currently supported actually has it</p>",
        "id": 225633549,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612833264
    },
    {
        "content": "<p>Well I wouldn't say nothing has it, vek already showed that it's useful regardless of architecture, but I agree that if we decide we don't want to support it we can and it's probably ok</p>",
        "id": 225633700,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612833374
    },
    {
        "content": "<p>Ah, I meant no architecture we currently target supports it directly.</p>",
        "id": 225633717,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1612833392
    },
    {
        "content": "<p>Rust is already very conservative about layout guarantees, and already has types (<code>usize</code>) that change their layout across architectures. I don't see how this is any different. Just make it clear what is and is not a stable guarantee</p>",
        "id": 225634012,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1612833614
    },
    {
        "content": "<p>almost a joke but not really: hey Rust has a GPGPU target doesn't it? GPUs support vec3s :^)</p>",
        "id": 225634033,
        "sender_full_name": "Jubilee",
        "timestamp": 1612833637
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"209168\">@Thom Chiovoloni</span>   - I agree they could generate different values.  If we went \"next ^2\" hoping to change that in the future, that could be difficult to replicate.  The way I'm looking at it there are two sources of the gap: the values hosted on the bits, and the operations that need to be adapted.  Emulating 24-bit data values on a 32-bit lane I expect is doable and pretty straightforward.  It's running the operations on them that will create a bunch of work (over-flow, sign-bit-dependent ops, and wrap-around).  If I were a user (even 3D for now) and was determined to get as much as I could into a single vector (increase the lane count per vec), I would save that for very specialized/hot locations in my data-transformation workflow... because without built-in support from the operations, how exactly I chose to make all these decisions is not something I could expect to be a norm.  </p>\n<p>If we implement too early, we risk generating values that turn out to be just plain wrong.  Let that be up to the user for now.  </p>\n<p>For me I, I value transparency, reliability and solid performance in that order.  Without the first two, I can't do much to make my app more performant.</p>",
        "id": 225634648,
        "sender_full_name": "Edmund Cape",
        "timestamp": 1612834241
    },
    {
        "content": "<p>we don't handle overflow.</p>",
        "id": 225634771,
        "sender_full_name": "Jubilee",
        "timestamp": 1612834347
    },
    {
        "content": "<p>Rust integers by default wrap on overflow, same as the CPU does. If this happens in scalar code in debug mode, they panic, but we have decided to not make that a feature of our API, and in any case it differs in release mode.</p>",
        "id": 225634902,
        "sender_full_name": "Jubilee",
        "timestamp": 1612834441
    },
    {
        "content": "<p>So, my problem (insofar as I have a \"problem\") is that so far I have not been convinced that we do not have any unique problems with f32x3 that are not already raised by u8x8 SIMD operations on Intel CPUs.</p>",
        "id": 225635036,
        "sender_full_name": "Jubilee",
        "timestamp": 1612834540
    },
    {
        "content": "<blockquote>\n<p>we don't handle overflow.</p>\n</blockquote>\n<p>Not sure if you are referring to what I posted.  But your point I believe describes where the two approaches can diverge: In the 32-bit lane hosting 24-bit data, we would have to \"emulate\" overflow to keep the two approaches from generating different values.</p>",
        "id": 225635043,
        "sender_full_name": "Edmund Cape",
        "timestamp": 1612834545
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"281757\">@Jubilee</span> or even smaller vectors. We support u8x2</p>",
        "id": 225635142,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612834619
    },
    {
        "content": "<p>We are not discussing vector lane bitwidths differing from used vector lane bitwidths, we are discussing vector register lane counts differing from used vector lanes.</p>",
        "id": 225635149,
        "sender_full_name": "Jubilee",
        "timestamp": 1612834623
    },
    {
        "content": "<p>We have no compiler support for u24 at the moment, a fact that has caused us some grief but we are somewhat resigned to the reality of. ^^;</p>",
        "id": 225635181,
        "sender_full_name": "Jubilee",
        "timestamp": 1612834668
    },
    {
        "content": "<p>Oh! Got it. My bad, apologies for mixing up T for N in how I was thinking about <code>[T; N]</code> vs <code>[T; N.next_power_of_two()]</code>.</p>",
        "id": 225635702,
        "sender_full_name": "Edmund Cape",
        "timestamp": 1612835142
    },
    {
        "content": "<p>Why is the size of u8x3 4 bytes on x86-64?</p>",
        "id": 225635828,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612835262
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"388128\">@Edmund Cape</span>  It is completely understandable.</p>",
        "id": 225635948,
        "sender_full_name": "Jubilee",
        "timestamp": 1612835367
    },
    {
        "content": "<p>That's... interesting. <span class=\"user-mention\" data-user-id=\"312331\">@Caleb Zulawski</span></p>",
        "id": 225636080,
        "sender_full_name": "Jubilee",
        "timestamp": 1612835486
    },
    {
        "content": "<p>I have a bad feeling that rustc just rounds all repr(simd) to the next power of two to be safe</p>",
        "id": 225636149,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612835534
    },
    {
        "content": "<p>lol.</p>",
        "id": 225636188,
        "sender_full_name": "Jubilee",
        "timestamp": 1612835570
    },
    {
        "content": "<p>y'all are gonna find lots of bugs with me around. :^)</p>",
        "id": 225636211,
        "sender_full_name": "Jubilee",
        "timestamp": 1612835594
    },
    {
        "content": "<p>So I think I'm confident that I would like to support non-power-of-two but I'm also confident it may not be practical any time soon</p>",
        "id": 225636282,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612835649
    },
    {
        "content": "<p>When lane count available from lane count used differ, when can the value of the vectorized computation differ?  I don't see vertical computations changing.  <span class=\"user-mention\" data-user-id=\"209168\">@Thom Chiovoloni</span> do you?  Loading and unloading data where every 4th element needs to be skipped seems easy enough.  The issue would be with any horizontal computations. Would those computations be easy to unify?</p>",
        "id": 225636928,
        "sender_full_name": "Edmund Cape",
        "timestamp": 1612836251
    },
    {
        "content": "<p>I think horizontal ops are accomplished by appropriately filling the unused lanes</p>",
        "id": 225637120,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612836458
    },
    {
        "content": "<p>One thing I'm now confused about is why does the size of a non-native vector sometimes, but not always, round up?  Why is the size of u8x2 2 (not a vector size), u8x3 is 4 (also not a vector size) but u32x3 is 16 (an SSE vector)</p>",
        "id": 225637267,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612836587
    },
    {
        "content": "<p>have we considered the possibility that the Code is Wrong</p>",
        "id": 225637342,
        "sender_full_name": "Jubilee",
        "timestamp": 1612836652
    },
    {
        "content": "<p>I'm fairly sure that's what it is <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 225637541,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612836773
    },
    {
        "content": "<p>So the question is, should it always be a valid vector size? Is there some other reason it's rounded up? Is there no good reason?</p>",
        "id": 225637655,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612836853
    },
    {
        "content": "<p>Without going to deeply into the details of various SIMD implementations, I would expect the size of the rust type to be the size of the actual used numbers of lanes, and that size to be rounded up to the next supported vector size by the backend.</p>",
        "id": 225637812,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612837001
    },
    {
        "content": "<p>The thing that's particularly concerning is that the layout is determined before even getting to codegen, does it even depend on architecture?</p>",
        "id": 225637862,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612837046
    },
    {
        "content": "<p>all wonderful questions!</p>",
        "id": 225637929,
        "sender_full_name": "Jubilee",
        "timestamp": 1612837104
    },
    {
        "content": "<p>Why does repr(simd) pass vectors by reference?</p>",
        "id": 225637954,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612837137
    },
    {
        "content": "<p>well...!</p>",
        "id": 225638183,
        "sender_full_name": "Jubilee",
        "timestamp": 1612837381
    },
    {
        "content": "<p>...a good question!</p>",
        "id": 225638190,
        "sender_full_name": "Jubilee",
        "timestamp": 1612837385
    },
    {
        "content": "<p>Oh, I figured out the answer to the last question</p>",
        "id": 225638210,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612837419
    },
    {
        "content": "<p>!</p>",
        "id": 225638215,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612837422
    },
    {
        "content": "<p>If you compile for AVX, vectors (in general, not in rust) are allowed to be passed via ymm register.  This would make target_feature unusable</p>",
        "id": 225638285,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612837474
    },
    {
        "content": "<p>In rust they are always passed by reference to allow crossing feature support boundaries</p>",
        "id": 225638304,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612837501
    },
    {
        "content": "<p>So I think this is unrelated to the size of the vector rounding up.</p>",
        "id": 225638334,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612837535
    },
    {
        "content": "<p>hmm wait.</p>",
        "id": 225638347,
        "sender_full_name": "Jubilee",
        "timestamp": 1612837554
    },
    {
        "content": "<p>right, I see now.</p>",
        "id": 225638858,
        "sender_full_name": "Jubilee",
        "timestamp": 1612838035
    },
    {
        "content": "<p>Maybe time to find the layout code that rounds up the size, and blame it? Maybe it is just wrong...</p>",
        "id": 225639054,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612838198
    },
    {
        "content": "<p>I searched issues and prs but didn't see anything right away</p>",
        "id": 225639068,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612838214
    },
    {
        "content": "<p>So, this appears to be where the size is actually determined:<br>\n<a href=\"https://github.com/rust-lang/rust/blob/bb587b1a1737738658d2eaecd4c8c1cab555257a/compiler/rustc_middle/src/ty/layout.rs#L762\">https://github.com/rust-lang/rust/blob/bb587b1a1737738658d2eaecd4c8c1cab555257a/compiler/rustc_middle/src/ty/layout.rs#L762</a></p>\n<p>The size is chosen based on the alignment</p>",
        "id": 225644721,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612844631
    },
    {
        "content": "<p>This appears to be the original commit specifying the layout of vectors: <a href=\"https://github.com/rust-lang/rust/commit/fe48a4af8403289ebc811884964fc4ef91f6bc09\">https://github.com/rust-lang/rust/commit/fe48a4af8403289ebc811884964fc4ef91f6bc09</a></p>",
        "id": 225645506,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612845584
    },
    {
        "content": "<p>Also, playing around with clang types such as <code>typedef char vec __attribute__((ext_vector_type(3)));</code> I seem to get the same layout as rustc.  u8x3 is 4 bytes, etc</p>",
        "id": 225645727,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612845866
    },
    {
        "content": "<p>hum.</p>",
        "id": 225646053,
        "sender_full_name": "Jubilee",
        "timestamp": 1612846331
    },
    {
        "content": "<p>I filed this bug. <a href=\"https://github.com/rust-lang/rust/issues/81931\">https://github.com/rust-lang/rust/issues/81931</a></p>",
        "id": 225718425,
        "sender_full_name": "Jubilee",
        "timestamp": 1612888988
    },
    {
        "content": "<p>Yeah I was focusing more on the size, but the alignment thing is weird too</p>",
        "id": 225718945,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612889167
    },
    {
        "content": "<p>It's a little odd because I'd expect llvm to at least emit movups instead</p>",
        "id": 225719032,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612889200
    },
    {
        "content": "<p>imo if the size is wrong but the alignment is correct then we can make that work, but if the alignment is wrong then we have serious problems even if the size is correct.</p>",
        "id": 225719817,
        "sender_full_name": "Jubilee",
        "timestamp": 1612889507
    },
    {
        "content": "<p>Agreed. Though perhaps I'll file a complementary issue for the size later today</p>",
        "id": 225720012,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612889584
    },
    {
        "content": "<p>Mm, I would add the data into that issue and your reasoning for why size is also relevant, and if it actually is another issue we can split it out, but I suspect if both need to be addressed then both can/will be addressed at the same time.</p>",
        "id": 225720529,
        "sender_full_name": "Jubilee",
        "timestamp": 1612889769
    },
    {
        "content": "<p>They're definitely related, but it <em>might</em> be a separate issue?  Perhaps not</p>",
        "id": 225724031,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612891006
    },
    {
        "content": "<p>Given that size is dependent on alignment, based on what you said, I think kinda not?</p>",
        "id": 225728794,
        "sender_full_name": "Jubilee",
        "timestamp": 1612892911
    },
    {
        "content": "<p>Good point.</p>",
        "id": 225729849,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1612893299
    },
    {
        "content": "<blockquote>\n<p>Like the Intel386 architecture, the AMD64 architecture in general does not re-quire all data accesses to be properly aligned. Misaligned data accesses are slower than aligned accesses but otherwise behave identically.  The only exceptions are that <code>__m128</code>, <code>__m256</code> and <code>__m512</code> must always be aligned properly.</p>\n</blockquote>\n<p>Well.</p>",
        "id": 226759142,
        "sender_full_name": "Jubilee",
        "timestamp": 1613617405
    },
    {
        "content": "<p>Yeah x86-64 doesn't really care about alignment at all (though if you use explicitly aligned loads they will fail/do bad things if your data is unaligned)</p>",
        "id": 226759212,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1613617475
    },
    {
        "content": "<p>Aarch64 does care about alignment afaik</p>",
        "id": 226759220,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1613617490
    },
    {
        "content": "<p>yeah, unaligned loads are still pretty fast on aarch64 (even if they're actually unaligned — they're very cheap, almost free, if they're actually aligned), but you need to explicitly use unaligned loads</p>",
        "id": 226759890,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1613618233
    },
    {
        "content": "<p>that's true for at least armv7 as well actually</p>",
        "id": 226759908,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1613618258
    },
    {
        "content": "<p>Won't armv7 error on unaligned loads?</p>",
        "id": 226759965,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1613618291
    },
    {
        "content": "<p>depends on the size of the value and some other stuff</p>",
        "id": 226760205,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1613618571
    },
    {
        "content": "<blockquote>\n<p>In Armv7-A and Armv7-R ... If alignment checking is enabled, all unaligned word and halfword transfers cause an alignment exception. If disabled, unaligned accesses are permitted for the LDR, LDRH, STR, STRH, LDRSH, LDRT, STRT, LDRSHT, LDRHT, STRHT, and TBH instructions. Other data-accessing instructions always cause an alignment exception for unaligned data. For STRD and LDRD, the specified address must be word-aligned.</p>\n</blockquote>\n<p>But note that if even using the unaligned operations isn't really that slow on arm. also, rust will use these instructions fro read_unaligned, so presumably we don't support the case where you enable alignemnet checking (requires flipping a bit in the system control register)</p>",
        "id": 226760515,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1613618918
    },
    {
        "content": "<p>rust makes working with unaligned data painful though, and apis like align_to encourage writig bulk processing algorithms by handling the head and tail with per-byte operations (for aligning &amp;[u8] to some T, e.g. usize or a vector), which is generally going to be worse for performance than e.g. doing an unaligned reads for the head and tail</p>",
        "id": 226760757,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1613619200
    },
    {
        "content": "<p>(this is a rant for another day though)</p>",
        "id": 226760838,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1613619283
    },
    {
        "content": "<p>No I was noticing the claim that the vector types <strong>must</strong> be aligned and thought that was interesting.</p>",
        "id": 226774494,
        "sender_full_name": "Jubilee",
        "timestamp": 1613634124
    }
]