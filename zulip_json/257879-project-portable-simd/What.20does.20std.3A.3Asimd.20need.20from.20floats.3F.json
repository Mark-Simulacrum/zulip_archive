[
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"281757\">Jubilee</span> <a href=\"#narrow/stream/257879-project-portable-simd/topic/meeting.202020-10-26/near/214629070\">said</a>:</p>\n<blockquote>\n<p>I. am working on analyzing our float situation and am interested in hearing others on desirables.</p>\n</blockquote>\n<p>From the const eval and unsafe code perspective, what is desired is super-reliable, predictable behavior, where runtime performance doesn't matter as long as we have strong guarantees, so then we can do tricks that let us have <em>less</em> of a runtime.</p>\n<p>From the SIMD perspective, I'm guessing what we would _like_ is to be able to tightly control performance characteristics? But I'm wondering if that's just an assumption I'm making and if others feel the same about it.</p>",
        "id": 214766461,
        "sender_full_name": "Jubilee",
        "timestamp": 1603834077
    },
    {
        "content": "<p>I think we're close to potentially being able to expose safe (definitely non-deterministic) \"fast\" math through LLVM freeze, if that would be helpful for what you're up to.  <a href=\"#narrow/stream/136281-t-lang.2Fwg-unsafe-code-guidelines/topic/Taking.20advantage.20of.20.60freeze.60.3F/near/212979135\">https://rust-lang.zulipchat.com/#narrow/stream/136281-t-lang.2Fwg-unsafe-code-guidelines/topic/Taking.20advantage.20of.20.60freeze.60.3F/near/212979135</a></p>",
        "id": 214767529,
        "sender_full_name": "scottmcm",
        "timestamp": 1603834676
    },
    {
        "content": "<p>i think you want more than just a toggle for fast/not fast tbh... in general IMO the reordering, precision relaxation, and etc parts of ffast-math are more valuable (and less footgun-ey) than the \"pretend infinity and nan dont exist\" (the infinity one is <em>especially</em> footguney)</p>",
        "id": 214767818,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1603834800
    },
    {
        "content": "<p>Yeah, I have been eyeing that difference too. Rust really would rather control various things that are already considered acceptable variation under IEEE754 than the optimizations that <em>deviate</em> from floating point.<br>\nonly half-joking: if we had dependent types we could make this all easy! :^)</p>",
        "id": 214768531,
        "sender_full_name": "Jubilee",
        "timestamp": 1603835176
    },
    {
        "content": "<p><code>RelaxedFloat&lt;f64, AllowReciprocal&gt;</code>? Could lower that to the corresponding llvm fast-math flags...</p>",
        "id": 214770503,
        "sender_full_name": "scottmcm",
        "timestamp": 1603836360
    },
    {
        "content": "<p>iirc, SIMD stuff on intel at least uses the FP state, but LLVM doesn't let you change the FP state. So until LLVM lets you change FP state, what we really get is \"default FP state\" floats only.</p>",
        "id": 214770976,
        "sender_full_name": "Lokathor",
        "timestamp": 1603836631
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"224471\">@Lokathor</span> that's not part of the FP state, it's the set of optimizations that LLVM is allowed to do.</p>\n<p><span class=\"user-mention\" data-user-id=\"125270\">@scottmcm</span> I don't know about the LLVM internals, it's almost possible to implement AllowReciprocal entirely in software if it's its own type by e.g. <code>impl Div</code> as <code>* (1.0 / self)</code>... I think also for simd we'd prefer it not to be purely type-based to avoid more combinatoric type explosion...</p>\n<p>Perhaps it wouldn't matter though.</p>",
        "id": 214773820,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1603838476
    },
    {
        "content": "<p>Agreed on making it type wrappers as much as possible</p>",
        "id": 214774868,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1603839243
    },
    {
        "content": "<p>Worst case it could use an unstable trait to implement the wrapper</p>",
        "id": 214775029,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1603839376
    },
    {
        "content": "<p>So there's a nuance here where what functions we link against and what code we emit to LLVM in the first place can significantly change the results and speed of floating point operations, even after LLVM optimizations, even without changing the surface semantics, and remaining within acceptable variation as perceived by IEEE754.</p>",
        "id": 214775098,
        "sender_full_name": "Jubilee",
        "timestamp": 1603839449
    },
    {
        "content": "<p>Do you have an example?  Not sure I'm following</p>",
        "id": 214775457,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1603839672
    },
    {
        "content": "<p>one example might be which way LLVM translates a fadd to the target ISA, if it uses <code>a + b</code> that gives different results to <code>b + a</code> since, if <code>a</code> and <code>b</code> are different quiet NaNs, one way it will return <code>a</code> and the other way will return <code>b</code>. This is true for almost all ISAs: RISC-V doesn't have that issue since it returns the canonical NaN instead of <code>a</code> or <code>b</code>; x87 doesn't have that issue since it has special hardware to compare <code>a</code> and <code>b</code>'s payloads and pick the max/min (ICR which), though x87 has its own mountain of oddities.</p>",
        "id": 214782741,
        "sender_full_name": "Jacob Lifshay",
        "timestamp": 1603845304
    },
    {
        "content": "<p>Another example is...</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"c1\">// Rust</span>\n<span class=\"w\">    </span><span class=\"n\">bridge</span><span class=\"p\">.</span><span class=\"n\">d</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">bridge</span><span class=\"p\">.</span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"kt\">f64</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">bridge</span><span class=\"p\">.</span><span class=\"n\">b</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"kt\">f64</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">bridge</span><span class=\"p\">.</span><span class=\"n\">c</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"kt\">f64</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"kt\">f32</span><span class=\"p\">;</span><span class=\"w\"></span>\n</code></pre></div>\n\n<div class=\"codehilite\" data-code-language=\"C++\"><pre><span></span><code><span class=\"c1\">// C++</span>\n    <span class=\"n\">bridge</span><span class=\"p\">.</span><span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">bridge</span><span class=\"p\">.</span><span class=\"n\">a</span> <span class=\"o\">*</span> <span class=\"n\">bridge</span><span class=\"p\">.</span><span class=\"n\">b</span> <span class=\"o\">+</span> <span class=\"n\">bridge</span><span class=\"p\">.</span><span class=\"n\">c</span><span class=\"p\">;</span>\n</code></pre></div>\n\n<p>emit the same IR.<br>\n( or did at one point, I'd have to double check since this example is old. )</p>",
        "id": 214783274,
        "sender_full_name": "Jubilee",
        "timestamp": 1603845732
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"281757\">@Jubilee</span> you mean as opposed to using f32 multiplication?</p>",
        "id": 214783567,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1603845965
    },
    {
        "content": "<p>yeah C++ lets you do some pretty cursed implicit coercions. :^)</p>",
        "id": 214784005,
        "sender_full_name": "Jubilee",
        "timestamp": 1603846306
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"281757\">@Jubilee</span> That seems like optimizing it needs the \"assume no infinities\" flag that Thom mentioned as being particularly errorprone</p>\n<p>(because a and b are positive and c is negative, the calculation could be infinite in f32 but finite in f64)</p>",
        "id": 214785976,
        "sender_full_name": "scottmcm",
        "timestamp": 1603848011
    },
    {
        "content": "<p>Indeed. And yeah, I'm reading Kahan go on a rant about Java's (early) choices in terms of FP (he is... acidic) and a thing he lingers on for a while is how infinities are actually more of a problem than NaNs with single-precision and computer architectures often run doubles at equivalent speed, so you should evaluate intermediates as doubles but store in whatever.</p>",
        "id": 214786335,
        "sender_full_name": "Jubilee",
        "timestamp": 1603848439
    },
    {
        "content": "<p>I'm not particularly advocating for implicit promotion to doubles, I'm just pointing out that some surprising stuff can happen with floating point evaluation.</p>",
        "id": 214786538,
        "sender_full_name": "Jubilee",
        "timestamp": 1603848623
    },
    {
        "content": "<p><a href=\"https://www2.math.uconn.edu/~olshevsky/classes/2017_Fall/math5510/JAVAhurt.pdf\">https://www2.math.uconn.edu/~olshevsky/classes/2017_Fall/math5510/JAVAhurt.pdf</a>?</p>",
        "id": 214786836,
        "sender_full_name": "oliver",
        "timestamp": 1603848949
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"281757\">Jubilee</span> <a href=\"#narrow/stream/257879-project-portable-simd/topic/What.20does.20std.3A.3Asimd.20need.20from.20floats.3F/near/214786538\">said</a>:</p>\n<blockquote>\n<p>I'm not particularly advocating for implicit promotion to doubles, I'm just pointing out that some surprising stuff can happen with floating point evaluation.</p>\n</blockquote>\n<p>I'd argue for avoiding <code>f64</code> unless you actually need it -- with automatic vectorization <code>f32</code> is often twice as fast since the compiler can cram twice as many values in a SIMD register.</p>",
        "id": 214789100,
        "sender_full_name": "Jacob Lifshay",
        "timestamp": 1603851061
    },
    {
        "content": "<p>I'd have to double check, but I think <code>(a as f64 * b as f64 + c as f64) as f32</code> is exactly the same as <code>f32::mul_add(a, b, c)</code> if you treat all NaNs the same. It does properly account for when <code>f32::mul(a, b)</code> is an infinity due to overflow.</p>",
        "id": 214789432,
        "sender_full_name": "Jacob Lifshay",
        "timestamp": 1603851386
    },
    {
        "content": "<p>turns out I was wrong: I forgot to account for double rounding. it would work with intermediate values with more than about 72 bits of mantissa, so <code>f128</code> would work, but not <code>f64</code>. An interesting paper related to that <a href=\"https://hal.archives-ouvertes.fr/hal-01091186\">https://hal.archives-ouvertes.fr/hal-01091186</a></p>",
        "id": 214790365,
        "sender_full_name": "Jacob Lifshay",
        "timestamp": 1603852528
    },
    {
        "content": "<p>I can comment briefly as a maintainer of PETSc (a widely-used distributed-memory algebraic solvers library) and broader computational science practitioner. I think <code>fp-contract=fast</code> should be default (or easy to select at crate granularity). Many applications are also fine with associative math project-wide, so long as they can guarantee it in the select places where it counts. Note that supporting associative math is essential for reductions (like norms and inner products) to run at load-store throughput, and it's implied by the likes of <code>#pragma omp simd reduction(+:sum)</code>. If crates need to use special types to get FMA contractions and associative math, a lot of potential adopters of Rust will conclude that the language is slow or too onerous for numerical code and thus stick with C/C++/Fortran.</p>\n<p>The rest of <code>-ffast-math</code> is more delicate. There are certainly apps that are happy to enable all of it. I'd estimate such apps account for 20-50% of supercomputer time today. ICC has almost <code>-ffast-math</code> semantics by default and a sizable cohort of apps like it (and sometimes mistakenly conclude it's faster than gcc/clang because they are more conservative without <code>-ffast-math</code>).</p>\n<p>I realize I'm breaking with Kahan (1998) on associativity defaults (or coarse-grained opt-in), though this is informed by what kind of code is written by which sort of developers. Really delicate numerics in the likes of eigensolvers are usually written by competent numerical analysts. Substantial amounts of library and application code supports semantics similar to matrix multiplication (taking advantage of sparsity and other structure). But the bulk of lines and code-churn in applications is implementing physical models such as conservation laws and constitutive models, and much of this is written by domain scientists with little software engineering experience and no input by numerical analysts. They want it to run fast, and every once in a while may encounter numerical issues such as computing a negative pressure from a near-vacuum state (that should produce a tiny positive pressure). This is often when a numerical analyst is brought in to help debug, and would be a place to use types that enforce stricter semantics.</p>\n<p>I never run trapping floating point in production, but it's an extremely useful debugging tool, both whole-project and selectively. It's most convenient to enable project-wide and have functions that intentionally handle denormals push non-trapping mode on entry and pop on return. We do this in PETSc around eigensolvers, for example, while providing a convenient run-time option to enable trapping at initialization.</p>",
        "id": 215268519,
        "sender_full_name": "Jed",
        "timestamp": 1604274035
    }
]