[
    {
        "content": "<p>So I'm using \"core::arch users\" to imply \"intrinsic users, generally\" but also just in general we do have a lot of people who are maintaining multiversioned code right now that they would like to write against std::simd instead because they don't actually care <strong>that</strong> much about what it compiles to enough to pick a specific intrinsic.</p>",
        "id": 212366309,
        "sender_full_name": "Jubilee",
        "timestamp": 1601938473
    },
    {
        "content": "<p>I think if something doesn't compile to a particular instruction that exists, it should be considered a bug</p>",
        "id": 212366367,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1601938549
    },
    {
        "content": "<p>Maybe instruction is too specific</p>",
        "id": 212366374,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1601938555
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"209168\">@Thom Chiovoloni</span> mentioned a desire to be able to find \"what intrinsic maps to what <code>std::simd</code> operation?\" and I mentioned that specific 1:1 mappings are probably not going to be found because that locks the API to mapping to specific intrinsics which we probably don't want, but I don't see anything stopping us from providing elaborations on <em>general patterns</em> that do exist.</p>\n<p><span class=\"user-mention\" data-user-id=\"312331\">@Caleb Zulawski</span> I generally agree but we might have good reasons to not (for instance, we are not going to compile to MMX code any time soon) so we are not going to firm that policy up universally.</p>",
        "id": 212366505,
        "sender_full_name": "Jubilee",
        "timestamp": 1601938652
    },
    {
        "content": "<p>Maybe I should say it should compile to the same as if you used std::arch</p>",
        "id": 212366588,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1601938713
    },
    {
        "content": "<p>The trade-off should only be the reduction of api surface area, I think</p>",
        "id": 212366626,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1601938749
    },
    {
        "content": "<p>Still only a maybe! It's possible for a given op to have two different instructions for it on a given machine.</p>",
        "id": 212366666,
        "sender_full_name": "Jubilee",
        "timestamp": 1601938797
    },
    {
        "content": "<p>we get to pick one.</p>",
        "id": 212366748,
        "sender_full_name": "Jubilee",
        "timestamp": 1601938804
    },
    {
        "content": "<p>yeah, my concern is solely around learnability of a new vector API, and documentation.</p>\n<p>even when i write simd now, i generally don't care which specific instruction actually gets used. nor really can i: IME you don't always get the one you specify anyway — i can use a _mm_shuffle_ps now and on opt-level 3 llvm might use movhlps (which is the right call when viable)</p>",
        "id": 212366791,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1601938850
    },
    {
        "content": "<p>Yeah, it's hard to clarify as a strict policy.  I agree the particular op doesn't generally matter as long as the performance is roughly the same</p>",
        "id": 212366803,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1601938863
    },
    {
        "content": "<p>If the compiler produces something clearly suboptimal we shouldn't shrug it off as a limitation of the API</p>",
        "id": 212366834,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1601938904
    },
    {
        "content": "<p>(though I don't think anyone was going to do that)</p>",
        "id": 212366842,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1601938913
    },
    {
        "content": "<p><strong>that</strong> I can agree with, modulo a discussion about \"clearly\".</p>",
        "id": 212366900,
        "sender_full_name": "Jubilee",
        "timestamp": 1601938945
    },
    {
        "content": "<p>If someone tries to do a fused mul-add on a well-known ISA that implements that and we don't actually generate the faster instructions we better have a good reason.</p>",
        "id": 212366972,
        "sender_full_name": "Jubilee",
        "timestamp": 1601939027
    },
    {
        "content": "<p>fma is tricky since it changes the result :(</p>",
        "id": 212367154,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1601939071
    },
    {
        "content": "<p>\"Lucy, you've got some 'splaining to do.\"</p>",
        "id": 212367180,
        "sender_full_name": "Jubilee",
        "timestamp": 1601939077
    },
    {
        "content": "<p>yeah but <code>mul_add</code> is a function.</p>",
        "id": 212367194,
        "sender_full_name": "Jubilee",
        "timestamp": 1601939098
    },
    {
        "content": "<p>ah, yeah, i see.</p>",
        "id": 212367254,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1601939144
    },
    {
        "content": "<p>So I guess the other question is, what do core::arch users need documentation-wise? <span aria-label=\"eyes\" class=\"emoji emoji-1f440\" role=\"img\" title=\"eyes\">:eyes:</span></p>",
        "id": 212367371,
        "sender_full_name": "Jubilee",
        "timestamp": 1601939234
    },
    {
        "content": "<p>the exact thing we can't give them (an assurance about what old functions they were using that they can switch to in our portable API)</p>",
        "id": 212367397,
        "sender_full_name": "Lokathor",
        "timestamp": 1601939271
    },
    {
        "content": "<p>lol</p>",
        "id": 212367409,
        "sender_full_name": "Jubilee",
        "timestamp": 1601939277
    },
    {
        "content": "<p>I think if we're purposefully not specifying the instruction generated we can't link to vendor instructions</p>",
        "id": 212367454,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1601939288
    },
    {
        "content": "<p>Is there anything else we can give instead tho'?</p>",
        "id": 212367468,
        "sender_full_name": "Jubilee",
        "timestamp": 1601939297
    },
    {
        "content": "<p>As Thom mentioned, LLVM won't necessarily even use an instruction you write in some cases!!! Though maybe not everyone is aware of that detail.</p>",
        "id": 212367492,
        "sender_full_name": "Jubilee",
        "timestamp": 1601939329
    },
    {
        "content": "<p>( indeed I recall people complain about that about LLVM. )</p>",
        "id": 212367507,
        "sender_full_name": "Jubilee",
        "timestamp": 1601939346
    },
    {
        "content": "<p>I think the main thing we can do is make an easy to use API with quality docs, and that'll already be so much better than <code>core::arch</code> that we'll attract people that way.</p>",
        "id": 212367521,
        "sender_full_name": "Lokathor",
        "timestamp": 1601939370
    },
    {
        "content": "<p>like, <code>core::arch</code> is a <em>seriously bad time</em> to be using.</p>",
        "id": 212367536,
        "sender_full_name": "Lokathor",
        "timestamp": 1601939385
    },
    {
        "content": "<p>RIP.</p>",
        "id": 212367602,
        "sender_full_name": "Jubilee",
        "timestamp": 1601939416
    },
    {
        "content": "<p>i think it would be easy to do better than vendor docs, so idk if we need to link to them if we commit to doing a good job. my concern is that (i suspect) at least initially, many/most of the users of this API will be people porting existing core::arch intrinsic code to the new api, and i just want to make sure those users (which selfishly include myself) are able to find the new names for things.</p>",
        "id": 212367871,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1601939643
    },
    {
        "content": "<p>I think using sensible high-level nomenclature will help here. We'll likely use some hybrid of Rust and the various vendor ways of expressing things.</p>",
        "id": 212373997,
        "sender_full_name": "Jubilee",
        "timestamp": 1601945470
    },
    {
        "content": "<p>I want to provide a ~1 page glossary, more like \"quick refresh on what terms we're using\" so that experienced users can read that, find their footing, and be off.</p>",
        "id": 212374033,
        "sender_full_name": "Jubilee",
        "timestamp": 1601945518
    },
    {
        "content": "<p>The two biggest things I'd ask for, personally:</p>\n<ul>\n<li>The ability to interoperate at zero cost, so that it's possible to move part of a program to portable SIMD without moving everything at once or having performance issues.</li>\n<li>A general strategy of \"we provide anything that's fast on at least one platform, even if we have to implement it without hardware support on another platform (but we'll document that)\", rather than \"we'll only provide things that are fast on many platforms\".</li>\n</ul>",
        "id": 212389009,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601965612
    },
    {
        "content": "<p>The latter is critical for being able to switch code from native to portable without losing performance in the process.</p>",
        "id": 212389018,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601965630
    },
    {
        "content": "<p>(WASM takes the \"only what works everywhere\" approach, and that's painful.)</p>",
        "id": 212389024,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601965642
    },
    {
        "content": "<p>(Also, that doesn't mean everything has to exist on day one. But the architecture shouldn't be designed around assumptions like \"128-bit is good enough\" or \"nothing needs to cross 128-bit lanes\", for instance.)</p>",
        "id": 212389242,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1601965875
    },
    {
        "content": "<p>&lt;BillGates&gt;128 bits is enough for anyone.&lt;/BillGates&gt;</p>",
        "id": 212389260,
        "sender_full_name": "Jubilee",
        "timestamp": 1601965914
    },
    {
        "content": "<p>don't need a 4th gimble don't need a 129th bit</p>",
        "id": 212391124,
        "sender_full_name": "Lokathor",
        "timestamp": 1601967874
    },
    {
        "content": "<p>So I generally agree but I do notice one concern (apologies if this is like, sub-intention nitpicking) which is that \"one platform\" is pretty nebulous here. \"x86_64\" is a pretty broad platform covering both AMD and Intel CPUs which have pretty different performance characteristics (AMD loves multithreading and focuses on getting high throughput, Intel is... well, Intel), even moreso when vectors are concerned since AMD just straight-up doesn't implement AVX512.</p>",
        "id": 212391862,
        "sender_full_name": "Jubilee",
        "timestamp": 1601968503
    },
    {
        "content": "<ul>\n<li>As far as interaction with <code>std::arch</code> goes, a <code>transmute</code> should be free at runtime with optimization applied.</li>\n<li>For platform coverage, we should offer all we can with maximum ergonomics.</li>\n<li>Giving specific guidance about what's fast and slow sounds like a whole secondary project on top of the crate, and one that is more of a long term docs issue. possibly best served by ultra-experts in each platform. Like, the Agner Fog tables are a seriously deep investigation that I would hesitate to take on.</li>\n</ul>",
        "id": 212392346,
        "sender_full_name": "Lokathor",
        "timestamp": 1601968859
    },
    {
        "content": "<p>and yeah, a core focus is in fact going to be on allowing essentially-free conversion to <code>arch</code> types because a few people have really wanted a very specific instruction compiled in and my response continues to be \"isn't that what <code>std::arch</code> is for???\" Here, since we have verrrry deep portability concerns blocking overly-specific compilation promises (the goal is \"good\" not \"a very specific assembly output\"), the reality is probably going to be that if someone knows the assembly code they should write the assembly code, or, well, use the <code>std::arch</code> intrinsic more like.</p>",
        "id": 212392903,
        "sender_full_name": "Jubilee",
        "timestamp": 1601969228
    },
    {
        "content": "<p>Ideally if someone uses <del><code>-march=native</code></del> <code>-C target-cpu=native</code> then it does compile how they expect, but anything <em>less</em> specific than that opens up more ambiguity.</p>",
        "id": 212393035,
        "sender_full_name": "Jubilee",
        "timestamp": 1601969331
    },
    {
        "content": "<p><code>cargo install</code> should just have a cli flag to use use -Ctarget-cpu=native during the install build</p>",
        "id": 212394670,
        "sender_full_name": "Lokathor",
        "timestamp": 1601970621
    },
    {
        "content": "<p>or even do it by default</p>",
        "id": 212394689,
        "sender_full_name": "Lokathor",
        "timestamp": 1601970644
    },
    {
        "content": "<p>isn't target-cpu=native sometimes unsound</p>",
        "id": 212397698,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1601972729
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"209168\">@Thom Chiovoloni</span> Do you mean if you then try run that binary on a different CPU?</p>",
        "id": 212416250,
        "sender_full_name": "Ashley Mannix",
        "timestamp": 1601985389
    },
    {
        "content": "<p>i meant <a href=\"https://github.com/rust-lang/rust/issues/64609\">https://github.com/rust-lang/rust/issues/64609</a>.</p>",
        "id": 212416487,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1601985540
    },
    {
        "content": "<blockquote>\n<p>A general strategy of \"we provide anything that's fast on at least one platform, even if we have to implement it without hardware support on another platform (but we'll document that)\", rather than \"we'll only provide things that are fast on many platforms\".</p>\n</blockquote>\n<p>I agree strongly with this, and more or less assumed it would be the case, which is why something like</p>\n<blockquote>\n<p>~1 page glossary, more like \"quick refresh on what terms we're using\" so that experienced users can read that, find their footing, and be off.</p>\n</blockquote>\n<p>sounds insufficient — it would need to be a pretty long glossary. These are big API surfaces. Note that I think you need this <em>more</em> than for the obscure parts of the APIs than the widely used parts.</p>",
        "id": 212416836,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1601985748
    },
    {
        "content": "<blockquote>\n<p>a core focus is in fact going to be on allowing essentially-free conversion to arch types</p>\n</blockquote>\n<p>Yes, it would be very annoying if this were not possible. That said, I had assumed transmute to the equivalent arch type would be guaranteed-sound (and possibly safe depending on the status of safe transmute). I think it would be good if the ergonomics were better than \"use transmute\" but I'm not sure how much worth it's worrying about.</p>",
        "id": 212417334,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1601986020
    },
    {
        "content": "<p>i think it's sound to add features just not take them away, at least that's my understanding from reading the issues</p>",
        "id": 212455025,
        "sender_full_name": "Lokathor",
        "timestamp": 1602003054
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"209168\">@Thom Chiovoloni</span> For the glossary, my goal would be of getting people oriented on a very meta-level sense so that they knew what they were doing when they explored the API (and so could make short work of that), rather than exploring the API and not being sure of what is going on.</p>",
        "id": 212479238,
        "sender_full_name": "Jubilee",
        "timestamp": 1602015195
    },
    {
        "content": "<p>i'm not opposed to a glossary at all (quite the opposite), i just don't think that replaces some method of being able to look up a weird intrinsic and find out what it maps to</p>\n<p>(but perhaps i misunderstood and we're mostly in agreement)</p>",
        "id": 212481156,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1602016032
    },
    {
        "content": "<p>There's certainly an extent to which a person might need to unlearn an intrinsic.</p>",
        "id": 212482151,
        "sender_full_name": "Lokathor",
        "timestamp": 1602016508
    },
    {
        "content": "<p>I was not necessarily responding directly to that desire.</p>",
        "id": 212485311,
        "sender_full_name": "Jubilee",
        "timestamp": 1602018147
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"281757\">Jubilee</span> <a href=\"#narrow/stream/257879-project-portable-simd/topic/What.20core.3A.3Aarch.20users.20need.20from.20std.3A.3Asimd/near/212391862\">said</a>:</p>\n<blockquote>\n<p>So I generally agree but I do notice one concern (apologies if this is like, sub-intention nitpicking) which is that \"one platform\" is pretty nebulous here. \"x86_64\" is a pretty broad platform covering both AMD and Intel CPUs</p>\n</blockquote>\n<p>When I said \"one platform\", I broadly meant \"one vector instruction set\". As in, if something is fast in one vector instruction set, it's worth exposing, even if other instruction sets have to write it out as a series of instructions. That <em>doesn't</em> necessarily mean matching instructions one-for-one, of course.</p>",
        "id": 212493375,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1602022880
    },
    {
        "content": "<p>My hope, generally, is that if someone has an algorithm they wrote using native intrinsics, they can almost always translate that algorithm to portable SIMD and end up with close to the same instructions generated on that target, as well as running elsewhere with varying degrees of performance.</p>",
        "id": 212493519,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1602022969
    },
    {
        "content": "<p>yeah that seems like the ideal. it's a lot of operations... but it also simplifies the decision process about what to support. i don't envy whoever implements the sse4.2 fallback though!</p>",
        "id": 212493658,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1602023071
    },
    {
        "content": "<p>Once the infrastructure for it is in place, perhaps someone would like to record a video of live-coding a few portable intrinsics, starting from instruction-set documentation and native intrinsics. That plus a call for help via @rustlang could produce some contributions.</p>",
        "id": 212494319,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1602023564
    },
    {
        "content": "<p>OK then~ Yeah, I basically think that's relatively ideal.</p>",
        "id": 212494430,
        "sender_full_name": "Jubilee",
        "timestamp": 1602023654
    },
    {
        "content": "<p>I don't think we will in practice be able to support every single niche on every single instruction set because we're going to run into odd cases that don't port well anywhere else... but that's \"the breaks\" and not something magically inherent, not something I intend to accept without a fight. I would like to be able to have as expansive an API as possible, preferably by providing abstractions that are inclined to port well over different SIMD ISAs.</p>\n<p>And if something is supported on any 2 target architectures, I think it's basically a given that we are going to make an effort to include it. At the WASM SIMD level of \"it's basically everywhere\", it definitely will warp our API design more? But I don't think it has to be \"everywhere\" for us to really want to include it, it just has to be in <em>enough</em> places and not completely degenerate in performance elsewhere.</p>",
        "id": 212495422,
        "sender_full_name": "Jubilee",
        "timestamp": 1602024383
    },
    {
        "content": "<p>I think a good criteria would be \"if we implement this without any hardware support, is the result going to be <em>worse</em> than what someone could write themselves\".</p>",
        "id": 212495576,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1602024510
    },
    {
        "content": "<p><em>Yes.</em> That is a very good baseline to catch ourselves on.</p>",
        "id": 212495614,
        "sender_full_name": "Jubilee",
        "timestamp": 1602024547
    },
    {
        "content": "<p>If a platform doesn't have any kind of hardware support for an operation, that operation is going to be slower. But it should still be <em>possible</em>, and it shouldn't be <em>substantially</em> slower than if it was hand-written for that platform.</p>",
        "id": 212495628,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1602024573
    },
    {
        "content": "<p>Now, it's always possible that someone might write a SIMD algorithm expecting certain operations to be fast, and the combination of those operations implements something that would have been faster if done another way.</p>",
        "id": 212495659,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1602024601
    },
    {
        "content": "<p>But that suggests that we could implement a higher-level operation that decomposes differently on different targets.</p>",
        "id": 212495720,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1602024625
    },
    {
        "content": "<p>(Within reason. I don't expect to have a branch-and-do-zlib intrinsic in portable SIMD; flate2 and zlib-ng can worry about that. ;) )</p>",
        "id": 212495755,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1602024664
    },
    {
        "content": "<p>(s390 does actually have hardware instructions for DEFLATE, so I'm not actually joking there.)</p>",
        "id": 212495769,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1602024686
    },
    {
        "content": "<p>amazing.</p>",
        "id": 212495994,
        "sender_full_name": "Jubilee",
        "timestamp": 1602024873
    },
    {
        "content": "<p>I mean, you often have to jump through hoops and do redundant work and such to make a algorithm work for simd, so i think the answer is very very often going to be \"yes if you wrote it by hand you'll get better results than if you use it on targets with no hardware support\". </p>\n<p>i mean i agree about cases like DEFLATE, but i could imagine someone using the AESNI stuff as a similar example, and I suspect we want to support those.</p>\n<p>i guess a good example would be sse4.2, which is probably going to be several orders of magnitude slower without hw support (unless i'm missing a clever way to implement it)</p>",
        "id": 212496017,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1602024898
    },
    {
        "content": "<p>I think... maybe there's a mild disconnect here.</p>\n<blockquote>\n<p>perhaps someone would like to record a video of live-coding a few portable intrinsics, starting from instruction-set documentation and native intrinsics.</p>\n</blockquote>\n<p>This is the exact opposite of the approach being taken so far. We're <em>not</em> looking at specific intrinsics in specific arches and then saying \"how do we make this portable?\". That's a fairly losing battle because it would take forever. Instead we're mostly letting you speak clearly to LLVM about what you want and letting LLVM handle it.</p>\n<p><strong>Example:</strong> Addition.</p>\n<ul>\n<li>The starting point <em>is not</em>  \"okay here's <code>_mm_add_ps</code>, how do we make it portable?\".</li>\n<li>The starting point is that you want addition on floats.</li>\n</ul>\n<p>LLVM knows how to do that, so we just say \"do the addition\", and then at the library level we're done, and LLVM handles it. How <em>specifically</em> that codegens in the end can depend on optimization level and cpu features selected and all that jazz, so we're actively <em>avoiding</em> saying that you get a specific anything. The semantics are just \"the floats in each lane are added and you get a set of lanes of the outputs\". Trying to list out \"Oh this is <code>_mm_add_ps</code> or maybe <code>vaddq_f32</code> or maybe etc etc\" would actually <em>complicate</em> learning the API. The best way to use a <strong>portable</strong> API is to step back from thinking about specific intrinsics and instructions you want, and just communicate the <em>semantics</em> you want to the compiler, and then it will figure out the rest from there.</p>\n<p>The main job we have in front of us is to ensure that we can offer consistent semantics across platforms.</p>",
        "id": 212497559,
        "sender_full_name": "Lokathor",
        "timestamp": 1602026169
    },
    {
        "content": "<blockquote>\n<p>Instead we're mostly letting you speak clearly to LLVM about what you want and letting LLVM handle it.</p>\n</blockquote>\n<p>I suspect this approach has a bit more mileage but will stop working relatively soon. I had assumed we'd eventually implement intrinsic-equivalent functionality — The downsides of only doing what LLVM can support are:</p>\n<ul>\n<li>adding new functionality is much harder as it requires getting code for it into LLVM.</li>\n<li>many existing libraries will end up unable to port to the new API.</li>\n<li>code that does port, or is written whole-cloth using the new API, will have to dip into platform intrinsics for less common operations, or implement the shim themselves.</li>\n</ul>\n<p>among others.</p>\n<blockquote>\n<p>The best way to use a portable API is to step back from thinking about specific intrinsics and instructions you want, and just communicate the semantics you want to the compiler, and then it will figure out the rest from there.</p>\n</blockquote>\n<p>I agree, but I don't think it follows that this is the best way to write an API like this.</p>",
        "id": 212498309,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1602026853
    },
    {
        "content": "<p>I don't mean to say that \"LLVM has sin and cos, but not tan, so we refuse to implement tan because it's not in LLVM\". I don't mean to say that what can be done by direct LLVM intrinsic is the only thing to do.</p>\n<p>But I do mean that the outside API should be, for example <code>f32xN::ceil</code>, and if that happens to be done via hardware or not, you should step back from trying to worry about specific intrinsics</p>",
        "id": 212499600,
        "sender_full_name": "Lokathor",
        "timestamp": 1602027898
    },
    {
        "content": "<p>Oh, I don't think anybody was suggesting <em>exposing</em> an intrinsic-alike API. My point was more: a lot of code will end up using one or two weirder instructions that LLVM doesnt offer a portable equivalent. Say: <code>_mm_movemask_ps</code>¹ (this is not weird as in rarely used at all, it's just semantically a bit odd).</p>\n<p>It's often pretty easy (trivial for movemask) to implement a portable equivalent to this, so we should support it even though it's not like LLVM will automatically implement the portable versions for us.</p>\n<p>I also <em>don't</em> think we should name it the same as the intel intrinsic² as intel intrinsic names range are all either \"bad\" or \"consonant soup\".</p>\n<p>Now, I do think at the end of the day this will mean some of the result APIs will kinda be... a bit intrinsic shaped. E.g. a lot of it you might not include if it weren't for the fact that there's dedicated hardware support for it.</p>\n<p>That's all to say, I think some parts of this will look more like <code>safe_arch</code> than <code>core::num</code>, but hopefully some thoughtful design can bridge the gap there well enough.</p>\n<hr>\n<p>¹: This is perhaps not the best example, as I suspect it's existence informed the design of the mask vectors, but it works well enough.</p>\n<p>²:  That said, as I've made pretty clear, IMO whatever we <em>do</em> name it should be discoverable if you search for the intrinsic in the docs, even if it's not directly mentioned in the docs (or for weird ones, only mentioned in a handwavey \"this is mirrors functionality provided by the &lt;intrinsic&gt; intrinsic from &lt;vendor&gt;\").</p>",
        "id": 212501034,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1602028962
    },
    {
        "content": "<p>Correct.<br>\nI am basically never going to be thinking about specific assembly or even specific instructions, I am going to be thinking at e.g. the level of NE10 API (one of many SIMD-optimized libraries, this one is specifically for ARM) and I am going to be thinking at the level of the overall <em>patterns</em> that are provided by intrinsics.</p>",
        "id": 212501075,
        "sender_full_name": "Jubilee",
        "timestamp": 1602029007
    },
    {
        "content": "<p>realistically my brain has been destroyed by intel mindpoison so i'll probably think in SSE until I die.... but my hope with participating here is to help this to be at a point where i can move past __m128 and think in core::simd</p>",
        "id": 212501401,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1602029245
    },
    {
        "content": "<p>Masks are basically necessary to write something like <code>f32x4.filter().for_each()</code>, and like, if I could provide things as high-level as the Iterator API I would be happy to.</p>",
        "id": 212501487,
        "sender_full_name": "Jubilee",
        "timestamp": 1602029309
    },
    {
        "content": "<p>i kinda think that something like high level simd iterator should be provided by a 3rd party crate, or at least requires a lot of design work first.</p>\n<p>(actually maybe thats what you mean)</p>",
        "id": 212501577,
        "sender_full_name": "Thom Chiovoloni",
        "timestamp": 1602029397
    },
    {
        "content": "<p>Ah, \"as high level as\", not literally the Iterator API. <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span><br>\nBut it would require a lot of design work first, yes!</p>",
        "id": 212501811,
        "sender_full_name": "Jubilee",
        "timestamp": 1602029559
    }
]