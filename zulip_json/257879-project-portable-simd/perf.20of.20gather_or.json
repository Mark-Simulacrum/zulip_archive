[
    {
        "content": "<p>Any ideas how to make the performance of these two functions equivalent?</p>\n<div class=\"codehilite\"><pre><span></span><code>#![feature(portable_simd)]\nuse std::convert::TryInto;\nuse std::simd::*;\n\npub fn take_i32(values: &amp;[i32], indices: &amp;[usize]) -&gt; Vec&lt;i32&gt; {\n    indices.iter().map(|index| values[*index]).collect()\n}\n\npub fn take_i32_simd(values: &amp;[i32], indices: &amp;[usize]) -&gt; Vec&lt;i32&gt; {\n    let chunks = indices.chunks_exact(8);\n\n    let iter = chunks.map(|chunk| {\n        let chunk: [usize; 8] = chunk.try_into().unwrap();\n        let chunk = usizex8::from_array(chunk);\n        Simd::gather_or_default(values, chunk)\n    });\n    let mut values = Vec::with_capacity(indices.len());\n    for chunk in iter {\n        values.extend_from_slice(chunk.as_array())\n    }\n\n    values\n}\n</code></pre></div>\n<p>atm I am getting</p>\n<div class=\"codehilite\"><pre><span></span><code>core_simd_take 2^20 f32 time:   [1.9418 ms 1.9760 ms 2.0199 ms]\nnaive_take 2^20 f32     time:   [904.55 us 921.93 us 943.80 us]\n</code></pre></div>\n<p>which is quite a difference, unfortunately</p>",
        "id": 275137021,
        "sender_full_name": "Jorge Leitao",
        "timestamp": 1647152785
    },
    {
        "content": "<p>Gathers are not going to be performant on most CPUs (but they should at least be the optimal way of performing that SIMD load), but the problem here is those functions aren't really doing anything with SIMD at all</p>",
        "id": 275137101,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1647152971
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"399416\">Jorge Leitao</span> <a href=\"#narrow/stream/257879-project-portable-simd/topic/perf.20of.20gather_or/near/275137021\">said</a>:</p>\n<blockquote>\n<p>Any ideas how to make the performance of these two functions equivalent?</p>\n<p>atm I am getting</p>\n<div class=\"codehilite\"><pre><span></span><code>core_simd_take 2^20 f32 time:   [1.9418 ms 1.9760 ms 2.0199 ms]\nnaive_take 2^20 f32     time:   [904.55 us 921.93 us 943.80 us]\n</code></pre></div>\n<p>which is quite a difference, unfortunately</p>\n</blockquote>\n<p>what architecture are you targeting?</p>",
        "id": 275137161,
        "sender_full_name": "Jubilee",
        "timestamp": 1647153025
    },
    {
        "content": "<p>So in this particular case I don't think SIMD is an optimization except on perhaps a handful of CPUs</p>",
        "id": 275137165,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1647153035
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"281757\">@Jubilee</span> , dammit, that was it - compiling for native (skylake-avx512) fixed it and made it 10% faster:</p>\n<div class=\"codehilite\"><pre><span></span><code>core_simd_take 2^20 f32 time:   [873.38 us 889.33 us 909.57 us]\nnaive_take 2^20 f32     time:   [956.56 us 974.87 us 996.98 us]\n</code></pre></div>",
        "id": 275137478,
        "sender_full_name": "Jorge Leitao",
        "timestamp": 1647153601
    },
    {
        "content": "<p>ha.</p>",
        "id": 275137520,
        "sender_full_name": "Jubilee",
        "timestamp": 1647153625
    },
    {
        "content": "<p>yeah it's not really going to be an optimization tbh on most architectures.</p>",
        "id": 275137529,
        "sender_full_name": "Jubilee",
        "timestamp": 1647153653
    },
    {
        "content": "<p>It's actually disappointing how little of an optimization it is with avx-512</p>",
        "id": 275137532,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1647153655
    },
    {
        "content": "<p>right?</p>",
        "id": 275137536,
        "sender_full_name": "Jubilee",
        "timestamp": 1647153665
    },
    {
        "content": "<p>Though I guess that proves it's doing something</p>",
        "id": 275137541,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1647153682
    },
    {
        "content": "<p>I made it a 10% faster by writing the chunks to the <code>Vec</code> in a different way:</p>\n<div class=\"codehilite\"><pre><span></span><code>core_simd_take 2^20 f32 time:   [763.22 us 772.10 us 782.71 us]\nnaive_take 2^20 f32     time:   [960.43 us 974.74 us 993.29 us]\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code>    let mut values = Vec::with_capacity(indices.len());\n    let mut dst = values.as_mut_ptr() as *mut Simd&lt;i32, 8&gt;;\n    for chunk in iter {\n        unsafe { dst.write_unaligned(chunk) };\n        unsafe { dst = dst.add(1) };\n    }\n    unsafe { values.set_len(indices.len()) }\n</code></pre></div>\n<p>We could consider offer something like this, so that folks do not have to use <code>unsafe</code> to write simds to a vector of values?</p>",
        "id": 275137586,
        "sender_full_name": "Jorge Leitao",
        "timestamp": 1647153753
    },
    {
        "content": "<p>Unfortunately we don't actually know that the capacity is sufficient so it essentially requires <code>unsafe</code> code anyways.</p>",
        "id": 275138118,
        "sender_full_name": "Jubilee",
        "timestamp": 1647154650
    },
    {
        "content": "<p>can't we do it via <a href=\"https://doc.rust-lang.org/std/iter/trait.TrustedLen.html\">TrustedLen</a>?</p>",
        "id": 275138173,
        "sender_full_name": "Jorge Leitao",
        "timestamp": 1647154725
    },
    {
        "content": "<p>Oh I mean the capacity of the vector. This only works because you know your vector cap because you made it.</p>",
        "id": 275138443,
        "sender_full_name": "Jubilee",
        "timestamp": 1647155061
    },
    {
        "content": "<p>something like this (with appropriate generics over lanes and types)</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"cp\">#[inline]</span><span class=\"w\"></span>\n<span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"k\">fn</span> <span class=\"nf\">collect_vec</span><span class=\"o\">&lt;</span><span class=\"n\">I</span>: <span class=\"nc\">TrustedLen</span><span class=\"o\">&lt;</span><span class=\"n\">Item</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">Simd</span><span class=\"o\">&lt;</span><span class=\"kt\">i32</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">16</span><span class=\"o\">&gt;&gt;&gt;</span><span class=\"p\">(</span><span class=\"n\">iter</span>: <span class=\"nc\">I</span><span class=\"p\">)</span><span class=\"w\"> </span>-&gt; <span class=\"nb\">Vec</span><span class=\"o\">&lt;</span><span class=\"kt\">i32</span><span class=\"o\">&gt;</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">const</span><span class=\"w\"> </span><span class=\"n\">LANES</span>: <span class=\"kt\">usize</span> <span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">16</span><span class=\"p\">;</span><span class=\"w\"></span>\n\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">len</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">iter</span><span class=\"p\">.</span><span class=\"n\">size_hint</span><span class=\"p\">().</span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">LANES</span><span class=\"p\">;</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"n\">values</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nb\">Vec</span>::<span class=\"n\">with_capacity</span><span class=\"p\">(</span><span class=\"n\">len</span><span class=\"p\">);</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"n\">dst</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">values</span><span class=\"p\">.</span><span class=\"n\">as_mut_ptr</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"n\">Simd</span><span class=\"o\">&lt;</span><span class=\"kt\">i32</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">LANES</span><span class=\"o\">&gt;</span><span class=\"p\">;</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"n\">chunk</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"n\">iter</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">        </span><span class=\"k\">unsafe</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"n\">dst</span><span class=\"p\">.</span><span class=\"n\">write_unaligned</span><span class=\"p\">(</span><span class=\"n\">chunk</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">};</span><span class=\"w\"></span>\n<span class=\"w\">        </span><span class=\"k\">unsafe</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"n\">dst</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">dst</span><span class=\"p\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">};</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"p\">}</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">unsafe</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"n\">values</span><span class=\"p\">.</span><span class=\"n\">set_len</span><span class=\"p\">(</span><span class=\"n\">len</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">};</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"n\">values</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>",
        "id": 275138608,
        "sender_full_name": "Jorge Leitao",
        "timestamp": 1647155371
    },
    {
        "content": "<p>tbh, I'd expect <code>.flat_map(|x| x.to_array()).collect()</code> to be pretty good for that, now, since <code>flat_map</code> is specialized for arrays to have a perfect <code>size_hint</code>.</p>",
        "id": 275140799,
        "sender_full_name": "scottmcm",
        "timestamp": 1647159055
    },
    {
        "content": "<p>But what I really want is a <code>Vec&lt;[T; N]&gt; -&gt; Vec&lt;T&gt;</code> API, since I think that can be done in-place no problem, and it's infallible, unlike the other way.</p>\n<p>Then you could <code>.map(|x| x.to_array()).collect_vec().flatten()</code>.</p>\n<p>(Well, infallible for non-ZSTs)</p>",
        "id": 275140860,
        "sender_full_name": "scottmcm",
        "timestamp": 1647159145
    },
    {
        "content": "<p>Thanks for the tip <span class=\"user-mention\" data-user-id=\"125270\">@scottmcm</span> - I confirm - <code>flat_map</code> is equivalent to the above performance-wise</p>",
        "id": 275141008,
        "sender_full_name": "Jorge Leitao",
        "timestamp": 1647159430
    }
]