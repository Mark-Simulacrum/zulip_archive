[
    {
        "content": "<p>Hi, I've got a question. I have a Simd&lt;i8, 32&gt;  and want to <code>horizontal_sum</code> it into an i32. Seems like <code>horizontal_sum</code> only allows to add into the same type, i.e. an <code>i8</code> right now. Is this possible somehow/planned?</p>",
        "id": 264424528,
        "sender_full_name": "Jakub Koralewski",
        "timestamp": 1639130311
    },
    {
        "content": "<p>All the horizontal instructions in LLVM are same-type, it looks like: <a href=\"https://llvm.org/docs/LangRef.html#vector-reduction-intrinsics\">https://llvm.org/docs/LangRef.html#vector-reduction-intrinsics</a></p>\n<p>All the intel <code>reduce_add</code> and <code>hadd</code> instructions are also homogeneous, from what I can find: <a href=\"https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=reduce\">https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=reduce</a></p>\n<p>So I would be somewhat surprised for mixed-type addition to be offered directly.</p>",
        "id": 264425934,
        "sender_full_name": "scottmcm",
        "timestamp": 1639131109
    },
    {
        "content": "<p>That said, with only 32 items, you only need i16 to avoid overflow.  So you can probably find a way to expand the <code>Simd&lt;i8, 32&gt;</code> to a <code>Simd&lt;i16, 32&gt;</code>, then horizontally-sum that (then extend it to <code>i32</code> if you really need <code>i32</code>).</p>",
        "id": 264426246,
        "sender_full_name": "scottmcm",
        "timestamp": 1639131282
    },
    {
        "content": "<p>I see, thanks!</p>",
        "id": 264426454,
        "sender_full_name": "Jakub Koralewski",
        "timestamp": 1639131418
    },
    {
        "content": "<p>(Also, I'm a simd noob, so it's totally possible that someone will show up and point out something I missed.)</p>",
        "id": 264426707,
        "sender_full_name": "scottmcm",
        "timestamp": 1639131583
    },
    {
        "content": "<p>Hmm, I spelunked through the intrinsics guide some more, and I think my guess is plausible.</p>\n<p>It looks like <code>vpmovsxbw</code> does the <code>i8x16</code> -&gt; <code>i16x16</code> and <code>i8x32</code> -&gt; <code>i16x32</code> kinds of conversions (<a href=\"https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=cvtepi8_epi16&amp;ig_expand=1823\">https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=cvtepi8_epi16&amp;ig_expand=1823</a>)</p>\n<p>That said, the <code>reduce_add</code> things in the intrinsics guide aren't actually instructions -- they're listed as \"Sequence\".  So the \"best\" way might not even involve using <code>horizontal_sum</code> at all, in the end.  (Or if it does, it might only be because that makes it obvious what you're trying to do to the LLVM optimizer, so it can emit it in a way that looks nothing like a horizontal sum.)</p>",
        "id": 264428101,
        "sender_full_name": "scottmcm",
        "timestamp": 1639132426
    },
    {
        "content": "<p>if you want this you might want something like</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">simd_val</span><span class=\"p\">.</span><span class=\"n\">to_array</span><span class=\"p\">().</span><span class=\"n\">iter</span><span class=\"p\">().</span><span class=\"n\">copied</span><span class=\"p\">().</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"o\">|</span><span class=\"n\">i</span><span class=\"o\">|</span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"kt\">i32</span><span class=\"p\">).</span><span class=\"n\">sum</span><span class=\"p\">()</span><span class=\"w\"></span>\n</code></pre></div>\n<p>which is not at all simd.</p>\n<p>I don't know if the portable-simd api does the width changing conversions at the moment. If so they're maybe a good bet, they can at least speed up some of the process, if not all.</p>\n<p>if not, there's always core::arch i guess</p>",
        "id": 264471257,
        "sender_full_name": "Lokathor",
        "timestamp": 1639153511
    },
    {
        "content": "<p>I don't believe we have those conversions yet, we've been discussing them though.  Integer conversions are easy, the floats have held us back a bit</p>",
        "id": 264472397,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1639154043
    },
    {
        "content": "<p>Actually it's the other way around.<br>\nFloat conversions are easy,<br>\nintegers are hard,<br>\nbecause our current intrinsics don't support isize/usize conversions.</p>",
        "id": 264506242,
        "sender_full_name": "Jubilee",
        "timestamp": 1639169505
    },
    {
        "content": "<p>I was thinking more how we don't have any safe int-to-float intrinsic, either</p>",
        "id": 264507049,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1639169870
    },
    {
        "content": "<p>Maybe conversions are the next thing we should focus on...</p>",
        "id": 264507132,
        "sender_full_name": "Caleb Zulawski",
        "timestamp": 1639169900
    },
    {
        "content": "<p>Why would isize / usize matter for the widening and narrowing ops on the fixed sized types?</p>",
        "id": 264513646,
        "sender_full_name": "Lokathor",
        "timestamp": 1639173168
    },
    {
        "content": "<p>shrug, they all pipe through the same intrinsic at the moment.<br>\nand atm the intrinsic just breaks on those.</p>",
        "id": 264516770,
        "sender_full_name": "Jubilee",
        "timestamp": 1639174669
    },
    {
        "content": "<p>ah, so it might work for only fixed sized integers?</p>",
        "id": 264520403,
        "sender_full_name": "Lokathor",
        "timestamp": 1639177042
    },
    {
        "content": "<p>yes.</p>",
        "id": 264520621,
        "sender_full_name": "Jubilee",
        "timestamp": 1639177185
    }
]