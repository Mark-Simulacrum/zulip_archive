[
    {
        "content": "<p>Hi all! </p>\n<p>Just posting a little thread to introduce myself, as I'll probably be around a fair bit over the coming few days!  </p>\n<p>I'm a compiler dev, based in Scotland, working for a hardware/chip company. One of my projects in the near future is to try and get Rust working with our architecture, which means lots of compiler hacking on rustc, and probably some library hacking too. We already have a fairly mature C compiler based on llvm, so I'm hoping to be able to sort-of slot our llvm branch into the rust compiler and run from there.</p>\n<p>Apologies if I ask questions in the wrong places while I get my bearings.</p>\n<p>Cheers!</p>",
        "id": 256110538,
        "sender_full_name": "AdamH",
        "timestamp": 1633372938
    },
    {
        "content": "<p>As an aside, while I'm primarily going to be concerned with the compiler side, I'm also interested in talking with people about how designing APIs that support single(ish)-source rust running on a host+coprocessor. At present our C API takes a hard-coded string that points to a compiled kernel, which is rather delicate. One of our strategic hopes with Rust is to be able to provide a nicer API, and hopefully provide well-typed APIs between our host and kernel code.</p>",
        "id": 256111091,
        "sender_full_name": "AdamH",
        "timestamp": 1633373141
    },
    {
        "content": "<p>I would love to talk more about that! I'm quite interested in common codebases that span multiple systems communicating via shared memory.</p>",
        "id": 256148075,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1633388955
    },
    {
        "content": "<p>You may be interested in the artifact dependencies RFC.</p>",
        "id": 256148083,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1633388966
    },
    {
        "content": "<p>Which chip company?</p>",
        "id": 256148096,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1633388978
    },
    {
        "content": "<p>And what is the status of getting your LLVM changes upstream?</p>",
        "id": 256148109,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1633388992
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/122652-new-members/topic/AdamH/near/256148083\">said</a>:</p>\n<blockquote>\n<p>You may be interested in the artifact dependencies RFC.</p>\n</blockquote>\n<p>I shall look into it! Would you be able to send me a link to it?</p>",
        "id": 256190613,
        "sender_full_name": "AdamH",
        "timestamp": 1633419196
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/122652-new-members/topic/AdamH/near/256148096\">said</a>:</p>\n<blockquote>\n<p>Which chip company?</p>\n</blockquote>\n<p>I work for \"UPMEM\" - we're quite a small-ish startup, working on in-memory computing. We're not planning on upstreaming any of our LLVM changes in the near future, as they're almost entirely in our target, and we have some dependencies on closed-source/in house libraries that would be difficult to make public.</p>",
        "id": 256190816,
        "sender_full_name": "AdamH",
        "timestamp": 1633419306
    },
    {
        "content": "<p>Have you seen <a href=\"https://github.com/geobacter-rs/geobacter\">https://github.com/geobacter-rs/geobacter</a>? It does single source for gpu compute. It does need some changes to rustc itself though to achieve this.</p>",
        "id": 256203839,
        "sender_full_name": "bjorn3",
        "timestamp": 1633426434
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"209804\">@AdamH</span> Note that that means you won't be able to get your target upstream, either.</p>",
        "id": 256249245,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1633442131
    },
    {
        "content": "<p>From what I understood, we should be able to get in as a tier 3 target, as we don't mind failing in upstream, but would like to avoid major code churn. That said, I'm not at the stage where I'm thinking about upstreaming any of our code as a) it doesn't work yet, and b) we need to work out our upstreaming policy for LLVM first. Once that's all in place though, I'm going to be talking to <span class=\"user-mention\" data-user-id=\"116107\">@davidtwco</span> about what the steps for upstreaming are, what we'll specifically need to do, and where we will be able to slot in. Thanks for your concern though!</p>",
        "id": 256255954,
        "sender_full_name": "AdamH",
        "timestamp": 1633444414
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"133247\">bjorn3</span> <a href=\"#narrow/stream/122652-new-members/topic/AdamH/near/256203839\">said</a>:</p>\n<blockquote>\n<p>Have you seen <a href=\"https://github.com/geobacter-rs/geobacter\">https://github.com/geobacter-rs/geobacter</a>? It does single source for gpu compute. It does need some changes to rustc itself though to achieve this.</p>\n</blockquote>\n<p>I have not, which I'm somewhat surprised by as I used to live in the single-source-gpu-computing world. I'll check it out, thanks!</p>",
        "id": 256256047,
        "sender_full_name": "AdamH",
        "timestamp": 1633444443
    },
    {
        "content": "<p><span aria-label=\"wave\" class=\"emoji emoji-1f44b\" role=\"img\" title=\"wave\">:wave:</span> was super confused for a moment until I recognized the name</p>",
        "id": 256256507,
        "sender_full_name": "davidtwco",
        "timestamp": 1633444609
    },
    {
        "content": "<p>The day of reckoning that we spoke of is almost upon us!</p>",
        "id": 256256634,
        "sender_full_name": "AdamH",
        "timestamp": 1633444655
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"209804\">@AdamH</span> A proprietary codegen backend would be an issue even for a tier 3 target.</p>",
        "id": 256260261,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1633445726
    },
    {
        "content": "<p>(I wrote the policy in question, and that was very much the intent.)</p>",
        "id": 256260316,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1633445743
    },
    {
        "content": "<p>Ah, my apologies, it does appear that I misunderstood!</p>",
        "id": 256260341,
        "sender_full_name": "AdamH",
        "timestamp": 1633445756
    },
    {
        "content": "<p>But if you're interested in working towards an open source codegen backend, I have a lot of experience helping codebases get in shape to become open, and I would be happy to help with that.</p>",
        "id": 256260463,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1633445784
    },
    {
        "content": "<p>I would note, however, that our LLVM fork is \"open\" (<a href=\"https://github.com/upmem/llvm-project/\">https://github.com/upmem/llvm-project/</a>). The issue is that compiling it (at present) relies on some runtime libraries that are currently closed source.</p>",
        "id": 256260539,
        "sender_full_name": "AdamH",
        "timestamp": 1633445811
    },
    {
        "content": "<p>That's effectively equivalent, from a policy perspective.</p>",
        "id": 256260665,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1633445851
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"239881\">Josh Triplett</span> <a href=\"#narrow/stream/122652-new-members/topic/AdamH/near/256260463\">said</a>:</p>\n<blockquote>\n<p>But if you're interested in working towards an open source codegen backend, I have a lot of experience helping codebases get in shape to become open, and I would be happy to help with that.</p>\n</blockquote>\n<p>That's a very kind offer, thank you! At present the issue is mostly political (working out how we can open-source our proprietary code, and also <em>whether</em> we can), rather than code based, so it may be a while until we can open that.</p>",
        "id": 256260745,
        "sender_full_name": "AdamH",
        "timestamp": 1633445879
    },
    {
        "content": "<p>FWIW, \"how\" and \"whether\" are things I can help with, professionally; I've done that exact work for a company of 100k people. :)</p>",
        "id": 256262463,
        "sender_full_name": "Josh Triplett",
        "timestamp": 1633446451
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"209804\">@Adam Brouwers-Harries</span> Interesting, If you are used to using CUDA, i am currently working on a codegen backend which targets the libnvvm CUDA library, currently (in my limited tests) it works and builds all the kernels ive thrown at it, with speed matching cuda c/c++ (because libnvvm is the same thing nvcc uses). So if you are interested in using rust for everything this may be an option for you once i release it. Obviously it only works on nvidia gpus, but it has a really nice API for launching and building kernels. And most of CUDA's features are available in it, like unified memory, graphs, etc.</p>",
        "id": 256301379,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1633460876
    },
    {
        "content": "<p>If you'd like to see an example of the API, there is an example here: <a href=\"https://gist.github.com/RDambrosio016/40676d9414c4e8092eb406c020dc410c\">https://gist.github.com/RDambrosio016/40676d9414c4e8092eb406c020dc410c</a></p>",
        "id": 256301613,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1633460975
    },
    {
        "content": "<p>You <em>can</em> use a single crate because cuda_std builds on non-nvptx and kernel cfg-gates the function for nvptx, but its usually easier to use a separate gpu-kernels crate. Since the codegen will compile the crate and then emit a ptx file which can then be loaded and ran with <code>cust</code></p>",
        "id": 256302793,
        "sender_full_name": "Riccardo D'Ambrosio",
        "timestamp": 1633461432
    }
]