[
    {
        "content": "<p>Hi all, <br>\nI'm an undergrad student from Germany. I'm mostly interested in Neural Networks. <br>\nRecently I ran into <a href=\"https://enzyme.mit.edu/\">https://enzyme.mit.edu/</a> which calculates gradients based on LLVM-IR.<br>\nThere is a test repository where it is accessed from Rust, but by manually replacing part of the rustc output with the relevant information. ( e.g. here <a href=\"https://github.com/tiberiusferreira/oxide-enzyme/blob/master/src/main.rs\">https://github.com/tiberiusferreira/oxide-enzyme/blob/master/src/main.rs</a>)<br>\nThey discussed adding a custom backend which is able to parse an enzyme call (and probably leaving everything else unchanged).</p>\n<p>I am entirely new to the compiler topic and just know a few basics from university, like AST and k-lookahead.<br>\nThis summer I will write my final undergrad thesis and I'm wondering if improving the enzyme integration might be a suitable topic.<br>\nThe time schedule would be around 300h, so for now I'm trying to find out how complicated that will be and whether there is a good chance<br>\nof ending up with a good result.<br>\nI would like to know if creating such an mvp would require writing a few hundred or multiple thousand LoC and where to look up how such a work<br>\nwould be carried out.</p>",
        "id": 225939273,
        "sender_full_name": "Manuel Drehwald",
        "timestamp": 1613008242
    },
    {
        "content": "<p>I don't understand anything on that homepage <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span> what is automatic diffentiation? What does it have to do with LLVM IR?</p>",
        "id": 225941498,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1613010411
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232545\">Joshua Nelson</span> </p>\n<p>No worries, this is also new for me. Let me try to explain (hopefully none of the original devs ever has to read that)<br>\nBasically you are trying to get the derivative of a function. So when having x^2 as input, you want to get 2x back.<br>\nIn general it seems like most libraries who support this work by overloading the operators or having some kind of macro-substitution and just storing a lot of information. Then they calculate the derivative by some magic (our 2x) and place it at the location where you tell them.</p>\n<p>Regarding enzyme the authors added the ability to llvm to trace a function / code-interval and calculate the derivative based on the llvm-ir.<br>\nSo that in a higher language you can just say something like:</p>\n<p>let x = 5;<br>\n_enzyme_start(x);<br>\nx *= 2;<br>\nx *= 3;<br>\n_enzyme_end(x);</p>\n<p>let g = _enzyme_gradient(x); // g = 2*3 = 6;</p>\n<p>By now, rustc would just complain that _enzyme_x is an undefined function. <br>\nA solution would be to tell him to embed the calls into LLVM-IR, since enzyme knows how to handle it.</p>\n<p>Please take it with a grain of salt, I'm neither an expert in AD, nor do I know much about enzyme. <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span></p>",
        "id": 225942304,
        "sender_full_name": "Manuel Drehwald",
        "timestamp": 1613011293
    },
    {
        "content": "<p>I've been interested in this subject from the PL side since there are a number of DSLs based around these <br>\ntechniques, my research was in the Stan probabilistic programming language (PPL) which is a way of expressing<br>\nneeded math and having it compile to C++</p>",
        "id": 225942437,
        "sender_full_name": "oliver",
        "timestamp": 1613011445
    },
    {
        "content": "<p>what is the point though? Why are you trying to find the derivative?</p>",
        "id": 225942644,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1613011686
    },
    {
        "content": "<p>the derivative is incredibly important for a number of reasons, most generally in min/max identification</p>",
        "id": 225942672,
        "sender_full_name": "oliver",
        "timestamp": 1613011739
    },
    {
        "content": "<p>autodiff is just one way the alternative is to define the gradients analytically which may or may not be humanly possible</p>",
        "id": 225942914,
        "sender_full_name": "oliver",
        "timestamp": 1613012017
    },
    {
        "content": "<p>analytically is to say how one does their Calc homework in college</p>",
        "id": 225942968,
        "sender_full_name": "oliver",
        "timestamp": 1613012069
    },
    {
        "content": "<p>What <span class=\"user-mention\" data-user-id=\"281739\">@oliver</span>  says + at least regarding NeuralNetworks people tend to frequently play around with new functions to see which one solves a given task best.<br>\nCalculating the derivative for each of those manually can become complex for more complex functions, especially when considering corner cases (e.g. division by zero) and you also need to have it implemented in an efficient way (since you will spend hundreds / thousand of cpu/gpu hours on that).<br>\nSo the idea is to just have to define the function and let a library take over the rest.<br>\nEnzyme claims to be more efficient than other implementations since it is integrated into llvm, allowing more intelligent optimizations.</p>",
        "id": 225943002,
        "sender_full_name": "Manuel Drehwald",
        "timestamp": 1613012144
    },
    {
        "content": "<p>Here are some other examples. <br>\n<a href=\"https://pytorch.org/docs/stable/autograd.html\">https://pytorch.org/docs/stable/autograd.html</a><br>\n<a href=\"https://www.tensorflow.org/api_docs/python/tf/autodiff\">https://www.tensorflow.org/api_docs/python/tf/autodiff</a><br>\nBoth as part of Neural Network Frameworks, but the concept is not limited to that, as pointed out above.</p>",
        "id": 225943182,
        "sender_full_name": "Manuel Drehwald",
        "timestamp": 1613012386
    },
    {
        "content": "<p>C++ had a proposal to add autodiff but I can't find it now</p>",
        "id": 225943273,
        "sender_full_name": "oliver",
        "timestamp": 1613012440
    },
    {
        "content": "<p>oh: <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p2072r0.pdf\">http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p2072r0.pdf</a></p>",
        "id": 225943286,
        "sender_full_name": "oliver",
        "timestamp": 1613012456
    },
    {
        "content": "<p>C++ has _everything_ <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span></p>",
        "id": 225943305,
        "sender_full_name": "oliver",
        "timestamp": 1613012475
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232545\">Joshua Nelson</span> <a href=\"#narrow/stream/122652-new-members/topic/Manuel.20from.20Germany/near/225941498\">said</a>:</p>\n<blockquote>\n<p>I don't understand anything on that homepage <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span> what is automatic diffentiation? What does it have to do with LLVM IR?</p>\n</blockquote>\n<p>The <a href=\"https://enzyme.mit.edu/getting_started/CallingConvention/\">API</a> for user code isn't particularly complex, but the functionality is implemented as an LLVM opt pass.<br>\nIdeally it's fed with optimized IR (but without vectorization), followed by another round of optimization (this time with vectorization).</p>\n<p>At least in C, it's just hijacking all functions that contain <code>__enzyme_autodiff</code> in their (presumably mangled) name, who's calls take a function pointer, inputs, outputs for the computed gradients, and optional parameter type hints where you just pass one of a few magic globals as an argument directly in front of the argument it should apply to. Quite similar to cli args for e.g. <code>git</code> (where the heuristics sometimes interpret a branch name as a file name or vice-versa).</p>\n<p>The <em>Getting Started</em> category I linked to makes no mention of code regions.<br>\nFunctions seem to work, though.</p>\n<p>And the undefined function complains are to be treated like any other C FFI, e.g. with declaring the symbols before use and applying <code>#[no_mangle]</code>.</p>",
        "id": 225955087,
        "sender_full_name": "namibj",
        "timestamp": 1613027455
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232545\">Joshua Nelson</span> <a href=\"#narrow/stream/122652-new-members/topic/Manuel.20from.20Germany/near/225942644\">said</a>:</p>\n<blockquote>\n<p>what is the point though? Why are you trying to find the derivative?</p>\n</blockquote>\n<p>AFAIK the predominant use case for such autodiff tools is to run a variant of gradient descent to optimize a non-trivial function expressed as code.<br>\nI.e., numerical optimization.</p>",
        "id": 225955297,
        "sender_full_name": "namibj",
        "timestamp": 1613027759
    },
    {
        "content": "<p>you might have seen this: <a href=\"https://github.com/tensorflow/swift#swift-for-tensorflow-in-archive-mode\">https://github.com/tensorflow/swift#swift-for-tensorflow-in-archive-mode</a></p>",
        "id": 226273458,
        "sender_full_name": "oliver",
        "timestamp": 1613256823
    },
    {
        "content": "<p>why is the Rust bindings to TF so much bigger than for Swift and TF its 3MB v 40MB?</p>",
        "id": 226274506,
        "sender_full_name": "oliver",
        "timestamp": 1613258291
    },
    {
        "content": "<p>I thought they were putting lots of work into Swift Tensorflow yet it's a fraction of Rust <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span></p>",
        "id": 226274621,
        "sender_full_name": "oliver",
        "timestamp": 1613258508
    },
    {
        "content": "<p>They were putting LOTS of work into TF in Swift last I heard, to the point of maintaining an alternate fork of the compiler IIRC</p>",
        "id": 226276893,
        "sender_full_name": "Poliorcetics",
        "timestamp": 1613262025
    },
    {
        "content": "<p>even some of the contributors didn't seem to know it was being cancelled</p>",
        "id": 226320325,
        "sender_full_name": "oliver",
        "timestamp": 1613330587
    },
    {
        "content": "<p><a href=\"https://forums.swift.org/t/differentiable-programming-for-gradient-based-machine-learning/42147/101\">https://forums.swift.org/t/differentiable-programming-for-gradient-based-machine-learning/42147/101</a></p>",
        "id": 226320383,
        "sender_full_name": "oliver",
        "timestamp": 1613330688
    },
    {
        "content": "<p><a href=\"https://twitter.com/TroyAaronHarvey/status/1360718493734215685\">https://twitter.com/TroyAaronHarvey/status/1360718493734215685</a></p>\n<div class=\"inline-preview-twitter\"><div class=\"twitter-tweet\"><a href=\"https://twitter.com/TroyAaronHarvey/status/1360718493734215685\"><img class=\"twitter-avatar\" src=\"https://pbs.twimg.com/profile_images/1238536029239304192/pwiLiPPi_normal.jpg\"></a><p>We are indebted to the #S4TF team at Google for all their effort... Swift became the 1st language with 1st class differentiable compiler support. We at PassiveLogic continue to invest deeply Swift auto-diff... as we forge new technologies beyond \"naive\" deep learning.</p><span>- Troy Harvey (@TroyAaronHarvey)</span></div></div>",
        "id": 226320399,
        "sender_full_name": "oliver",
        "timestamp": 1613330734
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"281739\">oliver</span> <a href=\"#narrow/stream/122652-new-members/topic/Manuel.20from.20Germany/near/226274621\">said</a>:</p>\n<blockquote>\n<p>I thought they were putting lots of work into Swift Tensorflow yet it's a fraction of Rust <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span></p>\n</blockquote>\n<p>The main interest in differentiable functions for Rust would be reinforcement learning, where more robust programming models are need to combine with DL gradients.   S4TF definitely pushed the envelope.  Too bad it got caught in the middle of two tech giants.</p>",
        "id": 226873000,
        "sender_full_name": "Chris Black",
        "timestamp": 1613679258
    },
    {
        "content": "<p>(sorry - inadvertently posted my intro as a reply)</p>",
        "id": 227043274,
        "sender_full_name": "Irene Knapp",
        "timestamp": 1613775710
    },
    {
        "content": "<p><a href=\"https://arxiv.org/abs/2102.13243\">https://arxiv.org/abs/2102.13243</a></p>",
        "id": 228661714,
        "sender_full_name": "oliver",
        "timestamp": 1614803091
    },
    {
        "content": "<p>\"To our knowledge, this combination of (1) requesting differ-<br>\nentiation within the language, (2) supporting user-defined<br>\ntypes with arbitrary tangent vector types, and (3) performing<br>\nthe AD code transformation at AOT-compile time was first<br>\nexplored in Swift for TensorFlow.\"</p>",
        "id": 228662753,
        "sender_full_name": "oliver",
        "timestamp": 1614803475
    }
]