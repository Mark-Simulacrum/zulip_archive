[
    {
        "content": "<p>So I spent yesterday looking at cgu sizes for <code>rustc_mir</code> on the theory that if we can get a more even partitioning, we can probably improve peak memory usage and hopefully that would be true of other large crates as well. What's interesting to me was that I found we have a huge disparity between the largest CGUs and the smallest CGUs after merging. </p>\n<p>To give an idea:</p>\n<ul>\n<li>1st largest CGU: <code>178769</code></li>\n<li>2nd &amp; 3rd largest CGUs: <code>~82000</code></li>\n<li>Next 8 CGUs: <code>40000 - 60000</code></li>\n<li>Next 8 CGUs: <code>20000 - 34000</code></li>\n<li>Next 18 CGUs: <code>10000 - 20000</code></li>\n<li>Remaining 219 CGUs: <code>1100 - 9800</code></li>\n</ul>\n<p>What's even more interesting is that the largest CGU is not the result of the merging algorithm adding some large CGUs together, it's actually that way before the merging step even runs! So the question in mind was then \"What's in that cgu?\" and the answer appears to be <code>hashbrown</code> monomorphizations for the various <code>HashMaps</code> we use in <code>rustc_mir</code>. (<a href=\"https://gist.github.com/wesleywiser/cc2bab8da28971f8e91df0638ba46d09\">gist with details</a>)</p>",
        "id": 228807884,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614876520
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span> am I right to assume that in theory all of those functions are independent - since they are all generic with different arguments - and so there's pretty much no chance they'd get inlined into each other? the only win might be llvm merging them (i.e. seeing they're identical) which seems not high priority to me?</p>",
        "id": 228808803,
        "sender_full_name": "simulacrum",
        "timestamp": 1614876823
    },
    {
        "content": "<p>As for what to do with this, I see two potential avenues worth exploring:</p>\n<ol>\n<li>\n<p>Implement CGU splitting. When we have a \"large\" CGU (for some definition of large) before merging, we should break it up into smaller CGUs so the merge step can do a better job of generating more evenly sized CGUs. One complication here is that after splitting a CGU, we need to make sure it still has all of the necessary inline functions inside it (basically we need to re-run the inlining CGU partitioning step). </p>\n</li>\n<li>\n<p>Change the initial partitioning so we don't put all this code in the same CGU to begin with. Perhaps for things like <code>Vec&lt;T&gt;</code> instead of putting every <code>Vec&lt;_&gt;</code> instantiation together, we can split them across multiple CGUs in some way? For example we could do something similar to what we do for local items and group all the <code>Vec&lt;T&gt;</code> instances where <code>T</code> is in the same module together.</p>\n</li>\n</ol>",
        "id": 228809463,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614877075
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116122\">@simulacrum</span> No, I'm pretty sure a lot of these are calling each other and going to be inlined. Just as an example, <code>Vec&lt;Foo&gt;::get(usize)</code> calls <code>Vec&lt;Foo&gt;::len()</code> and we want that to be inlined.</p>",
        "id": 228809708,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614877158
    },
    {
        "content": "<p>right, but Vec&lt;bar&gt; and vec&lt;foo&gt; are pretty much independent, since we don't have polymorphization</p>",
        "id": 228809763,
        "sender_full_name": "simulacrum",
        "timestamp": 1614877183
    },
    {
        "content": "<p>Yes, that's true.</p>",
        "id": 228809792,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614877192
    },
    {
        "content": "<p>i.e., point 2 in what you said</p>",
        "id": 228809874,
        "sender_full_name": "simulacrum",
        "timestamp": 1614877206
    },
    {
        "content": "<p>Which is why I'm thinking option 2 might be the first thing to try here.</p>",
        "id": 228809879,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614877207
    },
    {
        "content": "<p>it's definitely my instinct</p>",
        "id": 228809991,
        "sender_full_name": "simulacrum",
        "timestamp": 1614877235
    },
    {
        "content": "<p>I'm a bit surprised we group them</p>",
        "id": 228810023,
        "sender_full_name": "simulacrum",
        "timestamp": 1614877242
    },
    {
        "content": "<p>seems odd to group by the self type w/o generics? or maybe we're grouping by the module of definition</p>",
        "id": 228810089,
        "sender_full_name": "simulacrum",
        "timestamp": 1614877261
    },
    {
        "content": "<p>I think it'll be annoying to get inlining still nice, but it might not be too bad</p>",
        "id": 228810179,
        "sender_full_name": "simulacrum",
        "timestamp": 1614877297
    },
    {
        "content": "<p>I'm pretty sure it's a result of using <a href=\"https://github.com/rust-lang/rust/blob/8fe989dd768f5dfdb0fc90933f3f74fa4579fefd/compiler/rustc_mir/src/monomorphize/partitioning/default.rs#L266\"><code>characteristic_def_id_of_mono_item</code></a> to decide what CGU to initially put the item in.</p>",
        "id": 228810534,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614877411
    },
    {
        "content": "<p>Based on the big comment at the top of<code>rustc_mir/src/monomorphize/partitioning/mod.rs</code>, it looks like reducing CGU invalidation in incremental was a primary motivator for the current partitioning scheme.</p>\n<p>IIUC, grouping instantiations of a generic together has the advantage that only one CGU will be invalidated if the generic definition changes.</p>\n<p>So, when generic instantiations all being in one CGU isn't actually a problem, then splitting them into different CGUs might be all downside.</p>",
        "id": 228851668,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614892550
    },
    {
        "content": "<p>Though I don't have a full understanding of all that goes into partitioning, so I could be off</p>",
        "id": 228851835,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614892614
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"306073\">Tyson Nottingham</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/cgu.20sizes/near/228851668\">said</a>:</p>\n<blockquote>\n<p>IIUC, grouping instantiations of a generic together has the advantage that only one CGU will be invalidated if the generic definition changes.</p>\n<p>So, when generic instantiations all being in one CGU isn't actually a problem, then splitting them into different CGUs might be all downside.</p>\n</blockquote>\n<p>Yeah, that's a great point! I think the tweak I want to pursue is doing something differently when the type is not local to the current crate.</p>",
        "id": 228852701,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614892966
    },
    {
        "content": "<p>If the smaller cgus are faster to process and less long-tail, then invalidating more cgus seems fine to me. Presuming that the new scheme doesn't result in too much overhead (functions we \"add\" to more than one cgu for inlining), it may still be worthwhile. And in non-incr is just a win.</p>",
        "id": 228854806,
        "sender_full_name": "simulacrum",
        "timestamp": 1614893835
    },
    {
        "content": "<p>When you modify a dependency crate, are all of the work products for downstream crates thrown out (or invalidated)?</p>",
        "id": 228855339,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614894055
    },
    {
        "content": "<p>I'm ... not entirely sure.</p>",
        "id": 228855901,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614894302
    },
    {
        "content": "<p>If they are, then it seems like there might be little downside to separating the generic instantiations into separate CGUs for generics from upstream crates. Since any modification to the upstream crate would invalidate all the downstream CGUs anyway... if that's how it works. :)</p>",
        "id": 228856465,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614894514
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116122\">simulacrum</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/cgu.20sizes/near/228854806\">said</a>:</p>\n<blockquote>\n<p>If the smaller cgus are faster to process and less long-tail, then invalidating more cgus seems fine to me. Presuming that the new scheme doesn't result in too much overhead (functions we \"add\" to more than one cgu for inlining), it may still be worthwhile. And in non-incr is just a win.</p>\n</blockquote>\n<p>True, it might not be a problem in practice.</p>",
        "id": 228856592,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614894571
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"306073\">Tyson Nottingham</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/cgu.20sizes/near/228855339\">said</a>:</p>\n<blockquote>\n<p>When you modify a dependency crate, are all of the work products for downstream crates thrown out (or invalidated)?</p>\n</blockquote>\n<p>I don't know, but I'd guess the answer is no. It may depend on the modification, though.</p>",
        "id": 228857244,
        "sender_full_name": "simulacrum",
        "timestamp": 1614894839
    },
    {
        "content": "<p>I can also imagine this creating a bunch of smaller CGUs that end up having to get merged into larger CGUs, which might be a bad thing. A modification to the generic would invalidate every CGU that the instantiations got merged into, which might be a lot if there were many instantiations. But this is all speculation...</p>",
        "id": 228858179,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614895216
    },
    {
        "content": "<p>Anyway, not trying to naysay. Just thinking we could factor some of these considerations into the design. :)</p>",
        "id": 228858624,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614895383
    },
    {
        "content": "<p>Yeah, I'd love to get a collection of traces or something like that so we could write quick simulators to evaluate the cache invalidation and expected runtime of various strategies. Not sure how to do that though.</p>",
        "id": 228859846,
        "sender_full_name": "simulacrum",
        "timestamp": 1614895812
    },
    {
        "content": "<p>Just to corroborate Wesley's findings, rustc_middle's and rustc_query_impl's largest CGUs are also just instantiations from <code>hashbrown::raw</code>.</p>\n<p>Estimated sizes and contents of some significant CGUs in rustc_query_impl (built with debug assertions) at each stage of partitioning:</p>\n<div class=\"codehilite\"><pre><span></span><code>Initial partitioning:\n\n584685: hashbrown::raw generics\n412743: rustc_query_system::query generics\n324035: Vec generics\n214190: RawVec generics\n61992:  rustc_query_system::query::caches generics\n\nPost merging:\n\nSame.\n\nPost inlining:\n\n744205: hashbrown::raw generics + inlines\n684480: rustc_query_system::query generics + inlines\n529637: Vec generics + inlines\n314849: RawVec generics + inlines\n513496: rustc_query_system::query::caches + inlines\n</code></pre></div>\n<p>Interesting how much <code>rustc_query_system::query::caches</code> ballooned from inlining. While it didn't matter here, it shows how it could be important to consider the effects of inlining before merging. If a couple CGUs like that had been merged, it might have created a monster CGU after inlining.</p>\n<p>FWIW, a large number of the inlined items in that CGU were related to iterators.</p>",
        "id": 228887606,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614911700
    },
    {
        "content": "<p>One other question I've had is the extent to which these sizes are \"real\", i.e. do we have an estimate for how accurate our size counts are? I think they're based on MIR statements, it'd be interesting to compare to pre and post opt llvm IR for that cgu</p>",
        "id": 228889464,
        "sender_full_name": "simulacrum",
        "timestamp": 1614913032
    },
    {
        "content": "<p>Relevant -- <a href=\"https://github.com/rust-lang/rust/issues/69382\">#69382</a></p>",
        "id": 228889779,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614913256
    },
    {
        "content": "<p>Seeing if I can reduce the explosion of that <code>rustc_query_system::query::caches</code> CGU. Looks like it might be overly generic.</p>",
        "id": 228911285,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614931403
    },
    {
        "content": "<p>Well, I tried the \"change the initial partitioning\" idea and it didn't make much of a difference. Most of the hashmap instantiations in rustc_mir are for types defined in rustc_middle.</p>",
        "id": 229027053,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614979403
    },
    {
        "content": "<p>However, I do have a patch set that implements the cgu splitting idea. It does a much better job of creating evenly sized cgus and <em>I think</em> we're seeing lower peak memory usage during bootstrap.</p>",
        "id": 229027207,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614979469
    },
    {
        "content": "<p>I need to profile more.</p>",
        "id": 229027227,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614979480
    },
    {
        "content": "<p>What was the new idea behind the initial partitioning?</p>",
        "id": 229027544,
        "sender_full_name": "tm",
        "timestamp": 1614979617
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"125250\">Wesley Wiser</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/cgu.20sizes/near/228809463\">said</a>:</p>\n<blockquote>\n<p>As for what to do with this, I see two potential avenues worth exploring:</p>\n<ol>\n<li>\n<p>Implement CGU splitting. When we have a \"large\" CGU (for some definition of large) before merging, we should break it up into smaller CGUs so the merge step can do a better job of generating more evenly sized CGUs. One complication here is that after splitting a CGU, we need to make sure it still has all of the necessary inline functions inside it (basically we need to re-run the inlining CGU partitioning step). </p>\n</li>\n<li>\n<p>Change the initial partitioning so we don't put all this code in the same CGU to begin with. Perhaps for things like <code>Vec&lt;T&gt;</code> instead of putting every <code>Vec&lt;_&gt;</code> instantiation together, we can split them across multiple CGUs in some way? For example we could do something similar to what we do for local items and group all the <code>Vec&lt;T&gt;</code> instances where <code>T</code> is in the same module together.</p>\n</li>\n</ol>\n</blockquote>",
        "id": 229027598,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614979641
    },
    {
        "content": "<ol start=\"2\">\n<li>in that list</li>\n</ol>",
        "id": 229027610,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614979649
    },
    {
        "content": "<p>and then</p>",
        "id": 229027619,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614979655
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"125250\">Wesley Wiser</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/cgu.20sizes/near/228852701\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"306073\">Tyson Nottingham</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/cgu.20sizes/near/228851668\">said</a>:</p>\n<blockquote>\n<p>IIUC, grouping instantiations of a generic together has the advantage that only one CGU will be invalidated if the generic definition changes.</p>\n<p>So, when generic instantiations all being in one CGU isn't actually a problem, then splitting them into different CGUs might be all downside.</p>\n</blockquote>\n<p>Yeah, that's a great point! I think the tweak I want to pursue is doing something differently when the type is not local to the current crate.</p>\n</blockquote>",
        "id": 229027637,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614979666
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"306073\">Tyson Nottingham</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/cgu.20sizes/near/228911285\">said</a>:</p>\n<blockquote>\n<p>Seeing if I can reduce the explosion of that <code>rustc_query_system::query::caches</code> CGU. Looks like it might be overly generic.</p>\n</blockquote>\n<p>Indeed, <code>QueryCache::lookup</code> is instantiated 3 times for each query. That's... a lot.</p>",
        "id": 229027831,
        "sender_full_name": "cjgillot",
        "timestamp": 1614979758
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"125250\">Wesley Wiser</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/cgu.20sizes/near/229027637\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"125250\">Wesley Wiser</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/cgu.20sizes/near/228852701\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"306073\">Tyson Nottingham</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/cgu.20sizes/near/228851668\">said</a>:</p>\n<blockquote>\n<p>IIUC, grouping instantiations of a generic together has the advantage that only one CGU will be invalidated if the generic definition changes.</p>\n<p>So, when generic instantiations all being in one CGU isn't actually a problem, then splitting them into different CGUs might be all downside.</p>\n</blockquote>\n<p>Yeah, that's a great point! I think the tweak I want to pursue is doing something differently when the type is not local to the current crate.<br>\n</p>\n</blockquote>\n</blockquote>\n<p>Can we consider that a function from non-local types is either: inlined at MIR level, or optimized through LTO? This would avoid having to include it in another function's CGU.</p>",
        "id": 229028588,
        "sender_full_name": "cjgillot",
        "timestamp": 1614980082
    },
    {
        "content": "<p>Actually, all instantiation of a same function with non-local types should pretty much be independent. Could we be dumber and put them all in a different CGU?</p>",
        "id": 229029982,
        "sender_full_name": "cjgillot",
        "timestamp": 1614980813
    },
    {
        "content": "<p>So I have data about the split large CGUs idea. The idea is basically that after the initial partitioning, we check if any CGUs are large and if they are, we split them in half and then repeat the check again on each part recursively. A large CGU is defined as a CGU that is 10x larger than the median CGU's size and is at least 50,000 estimation units in size. </p>\n<p>The net effect on bootstrap that it don't change memory used much at all and it regresses compilation time by about 2%. (Note: an initial version did not have the \"must be at least 50,000 estimation units in size\" restriction and the regression was much more significant there). The issue is that without more intelligent CGU splitting  and merging  algorithms, we end up generating many more copies of inlined functions than we do previously which causes the regressions (the extra copies increased the total estimated size by ~30%). </p>\n<p>While that's unfortunate, I think the good news here is that if we can be more intelligent to avoid having to duplicate the inlined functions so much, we could probably see some significant wins here. </p>\n<p>The data I collected:</p>\n<table>\n<thead>\n<tr>\n<th>Stat</th>\n<th>baseline</th>\n<th>with splitting</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Largest CGU</td>\n<td>253,976</td>\n<td>37,598</td>\n</tr>\n<tr>\n<td>Smallest CGU</td>\n<td>1,209</td>\n<td>8,706</td>\n</tr>\n<tr>\n<td>Delta from largest &amp; smallest</td>\n<td>252,767</td>\n<td>28,892</td>\n</tr>\n<tr>\n<td>Count of CGUs less than avg size</td>\n<td>202</td>\n<td>148 (out of 256 CGUs total)</td>\n</tr>\n<tr>\n<td>Stddev</td>\n<td>26,705.86214</td>\n<td>6,312.718697</td>\n</tr>\n<tr>\n<td>Total size of all CGUs</td>\n<td>3,502,838.00</td>\n<td>4,333,445.00</td>\n</tr>\n</tbody>\n</table>",
        "id": 229569484,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1615329703
    },
    {
        "content": "<p>is the 2% longer build time accounting for the time taken to do this partitioning, or purely downstream from that (I assume it's included, and that's it the overall bootstrap time) ? I should know this, so I'm sorry in advance if this is wrong/stupid: I assume the sizes are still the \"instance def size estimate\"s but do we have an idea whether in this test there were many MIR statements that are not really used in codegen (say, fake reads) or heavily skew the numbers w.r.t to \"total estimated size\" increasing by 30% (maybe we filter those out before partitioning so ignore this if that's the case) (that is, was the 30% cgu size increase also seen in the codegen) ? \"if we can be more intelligent\" do you think we could have a way to test this theory, à la \"we analyze which are the costly duplicated inlined functions, manually figure out what a good partitioning would look like in this case, hardcode that ideal results for this test and measure again\" to see if there are indeed the probable significant wins we hope for with this technique ? (maybe we can then work backwards from those results to infer the tweaks that strategy needs, and of course see how well it generalizes to many different tests)</p>",
        "id": 229645587,
        "sender_full_name": "lqd",
        "timestamp": 1615378157
    },
    {
        "content": "<blockquote>\n<p>is the 2% longer build time accounting for the time taken to do this partitioning, or purely downstream from that (I assume it's included, and that's it the overall bootstrap time) ?</p>\n</blockquote>\n<p>The 2% time regression is in terms of the overall bootstrap process. </p>\n<blockquote>\n<p>I assume the sizes are still the \"instance def size estimate\"s</p>\n</blockquote>\n<p>Yeah, it's still that same code. </p>\n<blockquote>\n<p>do we have an idea whether in this test there were many MIR statements that are not really used in codegen (say, fake reads) or heavily skew the numbers w.r.t to \"total estimated size\" increasing by 30% (maybe we filter those out before partitioning so ignore this if that's the case) (that is, was the 30% cgu size increase also seen in the codegen) ? </p>\n</blockquote>\n<p>I'm not sure. I thought we had a pass that removed those statements at the beginning of the MIR opt pipeline. In terms of your overall question, yeah it's very possible the actual work is not 30% larger, it's just our bad estimates that are making it look that way. </p>\n<blockquote>\n<p>do you think we could have a way to test this theory, à la \"we analyze which are the costly duplicated inlined functions, manually figure out what a good partitioning would look like in this case, hardcode that ideal results for this test and measure again\" to see if there are indeed the probable significant wins we hope for with this technique ? </p>\n</blockquote>\n<p>That's an interesting approach! I think we could do that if we use a much smaller crate. There's ~70,000 instances that get codegen'd from rustc_mir so doing any kind of manual analysis on it felt intractable to me <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 230586122,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1615926712
    }
]