[
    {
        "content": "<p>Hi everyone in <span class=\"user-group-mention\" data-user-group-id=\"897\">@T-compiler/meeting</span> ! Its the first day of the sprint. I really wish I had started this topic at midnight last night. :)</p>",
        "id": 228224427,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614603152
    },
    {
        "content": "<p>/me totally forgot the sprint was this week and thought it was next week.</p>",
        "id": 228224510,
        "sender_full_name": "rylev",
        "timestamp": 1614603193
    },
    {
        "content": "<p>/me furiously tries to clear schedule in a last minute ditch effort.</p>",
        "id": 228224542,
        "sender_full_name": "rylev",
        "timestamp": 1614603213
    },
    {
        "content": "<p>One of my hopes for the sprint is to encourage some extra socialization in a semi-casual manner. Its something we haven't really been able to do so well: Its one of the few weaknesses I'll admit of Zulip over Discord. (and of course it also doesn't help that we cannot meet in person right now.)</p>",
        "id": 228224564,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614603230
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116083\">@pnkfelix</span> shall we do a standup style progress check every day in the following style:</p>\n<ul>\n<li>Yesterday: I accomplished ???</li>\n<li>Today: I will try to accomplish ???</li>\n<li>Help: I'd like help in ???</li>\n</ul>",
        "id": 228224835,
        "sender_full_name": "rylev",
        "timestamp": 1614603343
    },
    {
        "content": "<p>Yeah that's exactly the sort of thing I'd like to see. I'm not sure if its best off in this topic or in a different one. I guess having it here is good for now.</p>",
        "id": 228224958,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614603382
    },
    {
        "content": "<p>Maybe a dedicated \"standup\" topic?</p>",
        "id": 228224986,
        "sender_full_name": "rylev",
        "timestamp": 1614603404
    },
    {
        "content": "<p>That is another option I was considering.</p>",
        "id": 228224998,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614603410
    },
    {
        "content": "<p>or checkins</p>",
        "id": 228225022,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614603420
    },
    {
        "content": "<p>in parallel, I've created the <a class=\"stream-topic\" data-stream-id=\"131828\" href=\"/#narrow/stream/131828-t-compiler/topic/shrinkmem.20sprint.20highlights\">#t-compiler &gt; shrinkmem sprint highlights</a> topic. My hope is to post the most important bits of info there, so that people who are just watching from the sidelines in an infrequent manner can still see what's up.</p>",
        "id": 228225367,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614603571
    },
    {
        "content": "<p>By the way, in our \"investigations\" so far, <span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span> and I have discovered the not so surprising fact that LLVM dominates memory consumption (at least when compiling small crates like regex)</p>",
        "id": 228227221,
        "sender_full_name": "rylev",
        "timestamp": 1614604370
    },
    {
        "content": "<p>yep, that's true</p>",
        "id": 228227402,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614604456
    },
    {
        "content": "<p>I'd like to know if we can correlate the size of the codegen units we make with LLVM's memory usage</p>",
        "id": 228227428,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614604480
    },
    {
        "content": "<p>When using cg_clif, the memory consumption of Cranelift should be tiny given that only a single function is compiled at a time and once that function is done, only the compiled bytes and relocation info is stored until the object file corresponding to the codegen unit is written.</p>",
        "id": 228227453,
        "sender_full_name": "bjorn3",
        "timestamp": 1614604500
    },
    {
        "content": "<p>(I.e. if there's a predictable multiplicative factor there.)</p>",
        "id": 228227454,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614604500
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/sprint.20day.201.20chat/near/228227428\">said</a>:</p>\n<blockquote>\n<p>I'd like to know if we can correlate the size of the codegen units we make with LLVM's memory usage</p>\n</blockquote>\n<p>This would be pretty easy to do (as far as I can tell) on Windows by adding ETW annotations when creating codegen units. But obviously we don't want to come up with a solution that is Windows only.</p>",
        "id": 228227543,
        "sender_full_name": "rylev",
        "timestamp": 1614604554
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"133247\">@bjorn3</span> How is cg_clif doing with respect to other aspects of performance?</p>",
        "id": 228227560,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614604562
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"224872\">@rylev</span> do you or <span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span> know if there is some equivalent to <code>getrusage</code> you can call from within a Windows process?</p>",
        "id": 228227651,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614604590
    },
    {
        "content": "<p>In particular, I'd like to know if we could generalize my PR <a href=\"https://github.com/rust-lang/rust/issues/82532\">#82532</a> to work on Windows as well as Linux.</p>",
        "id": 228227680,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614604612
    },
    {
        "content": "<p>(in terms of making the process report the max-rss among its child processes.)</p>",
        "id": 228227740,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614604640
    },
    {
        "content": "<p>The new Cranelift backend regressed performance of when value debuginfo is used. (I should probably disable it for now as no DWARF value debuginfo is emitted anyway) But otherwise ~30% perf improvement. That is with only a single thread doing compilation for cg_clif and a normal amount for cg_llvm.</p>",
        "id": 228227875,
        "sender_full_name": "bjorn3",
        "timestamp": 1614604696
    },
    {
        "content": "<p>I assume that's ~30% perf improvement in terms of compile-times. What is codegen quality at that point, compared to say llvm --opt-level=0 ?</p>",
        "id": 228228018,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614604774
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/sprint.20day.201.20chat/near/228227651\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"224872\">rylev</span> do you or <span class=\"user-mention silent\" data-user-id=\"125250\">Wesley Wiser</span> know if there is some equivalent to <code>getrusage</code> you can call from within a Windows process?</p>\n</blockquote>\n<p>I don't believe there is an exact equivalent (though I'm not a win32 expert), but we should be able to find the same info. For example <a href=\"https://docs.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-getprocesstimes\">GetProcessTimes</a> offers some of the same info albeit without maxrss info</p>",
        "id": 228228026,
        "sender_full_name": "rylev",
        "timestamp": 1614604781
    },
    {
        "content": "<p>Codegen quality is roughly on par with cg_llvm + <code>-Copt-level=0</code>.</p>",
        "id": 228228127,
        "sender_full_name": "bjorn3",
        "timestamp": 1614604815
    },
    {
        "content": "<p>I think <code>GetProcessMemoryInfo</code> can help with maxrss</p>",
        "id": 228228199,
        "sender_full_name": "rylev",
        "timestamp": 1614604845
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"125250\">@Wesley Wiser</span> should we coordinate on seeing if we can get <a href=\"https://github.com/rust-lang/rust/issues/82532\">https://github.com/rust-lang/rust/issues/82532</a> working on Windows?</p>",
        "id": 228228330,
        "sender_full_name": "rylev",
        "timestamp": 1614604908
    },
    {
        "content": "<p>Cool; I'd just want to double-check whether <code>GetProcessMemoryInfo</code> handles only oneself, or if it will handle child proceses that have terminated (and your process waited for, analogous to Linux).</p>",
        "id": 228228401,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614604931
    },
    {
        "content": "<p>So <code>GetProcessMemoryInfo</code> doesn't enumerate children like <code>getrusage</code> with <code>RUSAGE_CHILDREN</code>does. Perhaps we can enumerate the child processes ourselves and get that info, but I've been looking at other people's translation of <code>getrusage</code> for Windows and most don't support <code>RUSAGE_CHILDREN</code> which is not a good sign.</p>",
        "id": 228230028,
        "sender_full_name": "rylev",
        "timestamp": 1614605685
    },
    {
        "content": "<p>I added some ideas to the brainstorm hackmd about perf and mem tests that can be done on mir optimizations</p>",
        "id": 228230279,
        "sender_full_name": "oli",
        "timestamp": 1614605806
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224872\">rylev</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/sprint.20day.201.20chat/near/228230028\">said</a>:</p>\n<blockquote>\n<p>So <code>GetProcessMemoryInfo</code> doesn't enumerate children like <code>getrusage</code> with <code>RUSAGE_CHILDREN</code>does. Perhaps we can enumerate the child processes ourselves and get that info, but I've been looking at other people's translation of <code>getrusage</code> for Windows and most don't support <code>RUSAGE_CHILDREN</code> which is not a good sign.</p>\n</blockquote>\n<p>Part of the goal is to get the peak memory usage over the entirety of each processes' lifetime, so that's why its part of the interface that you get the info for RUSAGE_CHILDREN only on terminated processes (and its accumulated via the <code>wait</code>). Still, I agree that we might be able to hack something in ourselves; e.g., feed the peak memory value back up ourselves manually as info passed to the parent process. (But then again, maybe we would be just well off focusing on reporting RUSAGE_SELF in some consistent way across the platforms.)</p>",
        "id": 228237130,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614608346
    },
    {
        "content": "<p>Should we be profiling with incremental compilation enabled or without? I guess more generally, what is the \"scenario\" we want to optimize for?</p>",
        "id": 228246606,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614611475
    },
    {
        "content": "<p>personally the scenario I care about most is <code>incr-full</code>, since I have incremental turned on for rust-lang/rust</p>",
        "id": 228247002,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1614611618
    },
    {
        "content": "<p>though it's a bit worrisome that you care about that scenario, as it sort of implies you care about \"incremental, but on a clean build\" which is odd :)</p>",
        "id": 228247698,
        "sender_full_name": "simulacrum",
        "timestamp": 1614611860
    },
    {
        "content": "<p>but we might not have anything better.</p>",
        "id": 228247711,
        "sender_full_name": "simulacrum",
        "timestamp": 1614611866
    },
    {
        "content": "<p>I use it a lot when rebasing, which means libstd has been modified and the whole compiler needs to be rebuilt</p>",
        "id": 228248928,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1614612291
    },
    {
        "content": "<p>Also I use incremental a ton for patched builds, but it works well there :) so I don't think we need to focus on improving that</p>",
        "id": 228249021,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1614612325
    },
    {
        "content": "<p>I guess this isn't terribly surprising but a clean incremental compilation has a significantly higher peak memory use on my machine than a clean non-incremental compilation. </p>\n<table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Peak Memory Usage (mb)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>build</code></td>\n<td>3,161</td>\n</tr>\n<tr>\n<td><code>build -i</code></td>\n<td>4,213</td>\n</tr>\n</tbody>\n</table>",
        "id": 228249283,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614612413
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"125250\">Wesley Wiser</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/sprint.20day.201.20chat/near/228249283\">said</a>:</p>\n<blockquote>\n<p>I guess this isn't terribly surprising but a clean incremental compilation has a significantly higher peak memory use on my machine than a clean non-incremental compilation. </p>\n<table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Peak Memory Usage (mb)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>build</code></td>\n<td>3,161</td>\n</tr>\n<tr>\n<td><code>build -i</code></td>\n<td>4,213</td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n<p>to me that is surprising because it means incremental uses significantly more memory than LLVM</p>",
        "id": 228249571,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1614612499
    },
    {
        "content": "<p>which seems ... concerning</p>",
        "id": 228249578,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1614612503
    },
    {
        "content": "<blockquote>\n<p>incremental uses significantly more memory than LLVM</p>\n</blockquote>\n<p>Wouldn't that mean peak would be 2x higher?</p>",
        "id": 228249675,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614612546
    },
    {
        "content": "<p>Incremental uses a lot but I don't think it's more than LLVM.</p>",
        "id": 228249710,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614612557
    },
    {
        "content": "<p>I don't follow? <code>-i</code> doesn't affect memory usage for LLVM, only for incremental</p>",
        "id": 228249727,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1614612570
    },
    {
        "content": "<p>Right but it's only 25% more memory</p>",
        "id": 228249759,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614612582
    },
    {
        "content": "<p>If incremental used more than LLVM, wouldn't we see memory usage spike by at least 2x?</p>",
        "id": 228249806,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614612599
    },
    {
        "content": "<p>maybe my assumptions are wrong - is the incremental graph held in memory at the same time LLVM is running?</p>",
        "id": 228249848,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1614612607
    },
    {
        "content": "<p>I think it has to be</p>",
        "id": 228249934,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614612628
    },
    {
        "content": "<p>ok right because CGUs are cached</p>",
        "id": 228250005,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1614612654
    },
    {
        "content": "<p>At least part of it, sure</p>",
        "id": 228250010,
        "sender_full_name": "simulacrum",
        "timestamp": 1614612655
    },
    {
        "content": "<p>ignore me then</p>",
        "id": 228250013,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1614612656
    },
    {
        "content": "<p>Well at least until all the CGUs are ready to be optimized</p>",
        "id": 228250035,
        "sender_full_name": "Wesley Wiser",
        "timestamp": 1614612664
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"232545\">Joshua Nelson</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/sprint.20day.201.20chat/near/228249727\">said</a>:</p>\n<blockquote>\n<p>I don't follow? <code>-i</code> doesn't affect memory usage for LLVM, only for incremental</p>\n</blockquote>\n<p>You might have meant something else, but it does affect it in that it changes the number of codegen units. That means smaller units in memory simultaneously during codegen and pre-LTO optimization, but more serialized units in memory during thin local LTO.</p>",
        "id": 228289236,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614626059
    },
    {
        "content": "<p>oh good point <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span> I was wrong in many ways then</p>",
        "id": 228289875,
        "sender_full_name": "Joshua Nelson",
        "timestamp": 1614626328
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116083\">pnkfelix</span> <a href=\"#narrow/stream/276895-t-compiler.2Fshrinkmem-sprint/topic/sprint.20day.201.20chat/near/228227428\">said</a>:</p>\n<blockquote>\n<p>I'd like to know if we can correlate the size of the codegen units we make with LLVM's memory usage</p>\n</blockquote>\n<p>By the way, I have a PR out for adding time-passes events for each CGU codegen -- <a href=\"https://github.com/rust-lang/rust/issues/81538\">#81538</a>. The output includes CGU size estimates (number of MIR statements IIRC):</p>\n<div class=\"codehilite\"><pre><span></span><code>        time:   2.682; rss: 1926MB -&gt; 2060MB ( +134MB)  codegen_module(3k1mmmip9jmhdnm3, 740742)\n        time:   3.829; rss: 2061MB -&gt; 2176MB ( +115MB)  codegen_module(35g2r0huhfuoe5v3, 715671)\n        time:   4.005; rss: 2279MB -&gt; 2335MB (  +57MB)  codegen_module(3shjsbojvk8p48io, 513640)\n</code></pre></div>",
        "id": 228291768,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614627039
    },
    {
        "content": "<p>(Though you have to be careful of the RSS readings during codegen, because things are happening in parallel unless you control for it with <code>-j1</code> or <code>-Z no-parallel-llvm</code>.)</p>",
        "id": 228292412,
        "sender_full_name": "Tyson Nottingham",
        "timestamp": 1614627267
    },
    {
        "content": "<p>There is <a href=\"https://github.com/rust-lang/rust/issues/65281\">#65281</a> (unmerged) which included inlined items in CGU size estimates before merging them. This reduced the size dispersion of CGUs quite a bit (using no-opt.bc file sizes as true value) when I was testing it.</p>",
        "id": 228298846,
        "sender_full_name": "tm",
        "timestamp": 1614629882
    },
    {
        "content": "<p>there Is discussion at the tail end of <a href=\"https://github.com/rust-lang/rust/issues/65281\">#65281</a> that maybe we should retry landing it. That may be a relatively simple task for someone to look into.</p>",
        "id": 228299469,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614630130
    },
    {
        "content": "<p>As an aside, It is sort of a sad observation about how much the current perf results “build on luck”, as adnjo403 put it...</p>",
        "id": 228299571,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614630170
    },
    {
        "content": "<p>Or maybe there are strictly smaller ways we could improve the estimation outlined in <a href=\"https://github.com/rust-lang/rust/issues/69382\">#69382</a> ...</p>",
        "id": 228299710,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614630231
    },
    {
        "content": "<p>Should we try to define a smarter partition based on the full MIR control-flow graph?</p>",
        "id": 228299796,
        "sender_full_name": "cjgillot",
        "timestamp": 1614630256
    },
    {
        "content": "<p>The query system can be extended to recover on cycles. With that modification, the full crate CFG can be computed efficiently using queries. Is this a relevant information to build codegen units?</p>",
        "id": 228300103,
        "sender_full_name": "cjgillot",
        "timestamp": 1614630369
    },
    {
        "content": "<p>I was thinking more something smaller. E.g. trying to estimate the cost of drop terminators. The last comment on <a href=\"https://github.com/rust-lang/rust/issues/69382\">#69382</a> says that a drop shim receives cost estimate of zero today...</p>",
        "id": 228300107,
        "sender_full_name": "pnkfelix",
        "timestamp": 1614630372
    },
    {
        "content": "<p>I looked into improving the estimate itself, but without inclusion of inlined items into estimate, in terms of CGU size balance, the results were rather inconclusive, so I would suggest doing something like <a href=\"https://github.com/rust-lang/rust/issues/65281\">#65281</a> first. Though, size estimates are also used for scheduling (and at that point they do include inlined items) so maybe it would be worth it from that perspective.</p>",
        "id": 228300601,
        "sender_full_name": "tm",
        "timestamp": 1614630566
    }
]